"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"SBDIBIM8","preprint","2021","Northcutt, Curtis G.; Athalye, Anish; Mueller, Jonas","Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks","","","","","http://arxiv.org/abs/2103.14749","We identify label errors in the test sets of 10 of the most commonly-used computer vision, natural language, and audio datasets, and subsequently study the potential for these label errors to affect benchmark results. Errors in test sets are numerous and widespread: we estimate an average of at least 3.3% errors across the 10 datasets, where for example label errors comprise at least 6% of the ImageNet validation set. Putative label errors are identified using confident learning algorithms and then human-validated via crowdsourcing (51% of the algorithmically-flagged candidates are indeed erroneously labeled, on average across the datasets). Traditionally, machine learning practitioners choose which model to deploy based on test accuracy - our findings advise caution here, proposing that judging models over correctly labeled test sets may be more useful, especially for noisy real-world datasets. Surprisingly, we find that lower capacity models may be practically more useful than higher capacity models in real-world datasets with high proportions of erroneously labeled data. For example, on ImageNet with corrected labels: ResNet-18 outperforms ResNet-50 if the prevalence of originally mislabeled test examples increases by just 6%. On CIFAR-10 with corrected labels: VGG-11 outperforms VGG-19 if the prevalence of originally mislabeled test examples increases by just 5%. Test set errors across the 10 datasets can be viewed at https://labelerrors.com and all label errors can be reproduced by https://github.com/cleanlab/label-errors.","2021-11-07","2024-08-29 02:48:11","2024-08-29 02:48:11","2023-01-10 19:35:54","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2103.14749 arXiv:2103.14749 [cs, stat]","","C:\Users\frank\Zotero\storage\26L8L2WX\Northcutt et al. - 2021 - Pervasive Label Errors in Test Sets Destabilize Ma.pdf; C:\Users\frank\Zotero\storage\ZXLR2N4K\2103.html","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2103.14749","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z5ZULGPQ","journalArticle","2018","Zhou, Zhi-Hua","A brief introduction to weakly supervised learning","National Science Review","","2095-5138, 2053-714X","10.1093/nsr/nwx106","https://academic.oup.com/nsr/article/5/1/44/4093912","Supervised learning techniques construct predictive models by learning from a large number of training examples, where each training example has a label indicating its ground-truth output. Though current techniques have achieved great success, it is noteworthy that in many tasks it is difficult to get strong supervision information like fully ground-truth labels due to the high cost of the data-labeling process. Thus, it is desirable for machine-learning techniques to work with weak supervision. This article reviews some research progress of weakly supervised learning, focusing on three typical types of weak supervision: incomplete supervision, where only a subset of training data is given with labels; inexact supervision, where the training data are given with only coarse-grained labels; and inaccurate supervision, where the given labels are not always ground-truth.","2018-01-01","2024-08-29 02:48:14","2024-08-29 02:48:14","2022-12-19 18:17:38","44-53","","1","5","","","","","","","","","","en","","","","","DOI.org (Crossref)","","Number: 1","","C:\Users\frank\Zotero\storage\RKP4W4ND\nwx106.pdf; C:\Users\frank\Zotero\storage\9SNI3P6I\4093912.html; C:\Users\frank\Zotero\storage\SCLVHS4T\Zhou - 2018 - A brief introduction to weakly supervised learning.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RVFTYNRU","preprint","2022","Zhang, Rongzhi; Yu, Yue; Shetty, Pranav; Song, Le; Zhang, Chao","PRBoost: Prompt-Based Rule Discovery and Boosting for Interactive Weakly-Supervised Learning","","","","","http://arxiv.org/abs/2203.09735","Weakly-supervised learning (WSL) has shown promising results in addressing label scarcity on many NLP tasks, but manually designing a comprehensive, high-quality labeling rule set is tedious and difficult. We study interactive weakly-supervised learning -- the problem of iteratively and automatically discovering novel labeling rules from data to improve the WSL model. Our proposed model, named PRBoost, achieves this goal via iterative prompt-based rule discovery and model boosting. It uses boosting to identify large-error instances and then discovers candidate rules from them by prompting pre-trained LMs with rule templates. The candidate rules are judged by human experts, and the accepted rules are used to generate complementary weak labels and strengthen the current model. Experiments on four tasks show PRBoost outperforms state-of-the-art WSL baselines up to 7.1% and bridges the gaps with fully supervised models. Our Implementation is available at \url{https://github.com/rz-zhang/PRBoost}.","2022-03-18","2024-08-29 02:48:17","2024-08-29 02:48:17","2023-01-01 20:44:19","","","","","","","PRBoost","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2203.09735 arXiv:2203.09735 [cs]","","C:\Users\frank\Zotero\storage\8G8TJBFK\Zhang et al. - 2022 - PRBoost Prompt-Based Rule Discovery and Boosting .pdf; C:\Users\frank\Zotero\storage\KGNPYN6J\2203.html","","notion","Computer Science - Machine Learning; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2203.09735","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RHDZNG78","preprint","2019","Varma, Paroma; Sala, Frederic; He, Ann; Ratner, Alexander; Ré, Christopher","Learning Dependency Structures for Weak Supervision Models","","","","","http://arxiv.org/abs/1903.05844","Labeling training data is a key bottleneck in the modern machine learning pipeline. Recent weak supervision approaches combine labels from multiple noisy sources by estimating their accuracies without access to ground truth labels; however, estimating the dependencies among these sources is a critical challenge. We focus on a robust PCA-based algorithm for learning these dependency structures, establish improved theoretical recovery rates, and outperform existing methods on various real-world tasks. Under certain conditions, we show that the amount of unlabeled data needed can scale sublinearly or even logarithmically with the number of sources $m$, improving over previous efforts that ignore the sparsity pattern in the dependency structure and scale linearly in $m$. We provide an information-theoretic lower bound on the minimum sample complexity of the weak supervision setting. Our method outperforms weak supervision approaches that assume conditionally-independent sources by up to 4.64 F1 points and previous structure learning approaches by up to 4.41 F1 points on real-world relation extraction and image classification tasks.","2019-03-14","2024-08-29 02:48:19","2024-08-29 02:48:19","2022-12-31 20:50:09","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:1903.05844 arXiv:1903.05844 [cs, stat]","","C:\Users\frank\Zotero\storage\N2F9ZJLV\Varma et al. - 2019 - Learning Dependency Structures for Weak Supervisio.pdf; C:\Users\frank\Zotero\storage\Y5M82EYJ\1903.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1903.05844","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VXNEW7WV","preprint","2022","Hsieh, Cheng-Yu; Zhang, Jieyu; Ratner, Alexander","Nemo: Guiding and Contextualizing Weak Supervision for Interactive Data Programming","","","","","http://arxiv.org/abs/2203.01382","Weak Supervision (WS) techniques allow users to efficiently create large training datasets by programmatically labeling data with heuristic sources of supervision. While the success of WS relies heavily on the provided labeling heuristics, the process of how these heuristics are created in practice has remained under-explored. In this work, we formalize the development process of labeling heuristics as an interactive procedure, built around the existing workflow where users draw ideas from a selected set of development data for designing the heuristic sources. With the formalism, we study two core problems of how to strategically select the development data to guide users in efficiently creating informative heuristics, and how to exploit the information within the development process to contextualize and better learn from the resultant heuristics. Building upon two novel methodologies that effectively tackle the respective problems considered, we present Nemo, an end-to-end interactive system that improves the overall productivity of WS learning pipeline by an average 20% (and up to 47% in one task) compared to the prevailing WS approach.","2022-10-23","2024-08-29 02:48:21","2024-08-29 02:48:21","2022-12-31 20:47:33","","","","","","","Nemo","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2203.01382 arXiv:2203.01382 [cs, stat]","","C:\Users\frank\Zotero\storage\HIBI2YC2\Hsieh et al. - 2022 - Nemo Guiding and Contextualizing Weak Supervision.pdf; C:\Users\frank\Zotero\storage\XU49D6MZ\2203.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2203.01382","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R4ZYBDQP","preprint","2021","Zhang, Jieyu; Yu, Yue; Li, Yinghao; Wang, Yujing; Yang, Yaming; Yang, Mao; Ratner, Alexander","WRENCH: A Comprehensive Benchmark for Weak Supervision","","","","","http://arxiv.org/abs/2109.11377","Recent Weak Supervision (WS) approaches have had widespread success in easing the bottleneck of labeling training data for machine learning by synthesizing labels from multiple potentially noisy supervision sources. However, proper measurement and analysis of these approaches remain a challenge. First, datasets used in existing works are often private and/or custom, limiting standardization. Second, WS datasets with the same name and base data often vary in terms of the labels and weak supervision sources used, a significant ""hidden"" source of evaluation variance. Finally, WS studies often diverge in terms of the evaluation protocol and ablations used. To address these problems, we introduce a benchmark platform, WRENCH, for thorough and standardized evaluation of WS approaches. It consists of 22 varied real-world datasets for classification and sequence tagging; a range of real, synthetic, and procedurally-generated weak supervision sources; and a modular, extensible framework for WS evaluation, including implementations for popular WS methods. We use WRENCH to conduct extensive comparisons over more than 120 method variants to demonstrate its efficacy as a benchmark platform. The code is available at https://github.com/JieyuZ2/wrench.","2021-10-11","2024-08-29 02:48:23","2024-08-29 02:48:23","2022-12-31 20:46:27","","","","","","","WRENCH","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2109.11377 arXiv:2109.11377 [cs, stat]","","C:\Users\frank\Zotero\storage\AW3MGFQA\Zhang et al. - 2021 - WRENCH A Comprehensive Benchmark for Weak Supervi.pdf; C:\Users\frank\Zotero\storage\XF354KEB\2109.html","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2109.11377","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2NUITXIK","journalArticle","2019","Li, Yu-Feng; Guo, Lan-Zhe; Zhou, Zhi-Hua","Towards Safe Weakly Supervised Learning","IEEE Transactions on Pattern Analysis and Machine Intelligence","","0162-8828, 2160-9292, 1939-3539","10.1109/TPAMI.2019.2922396","https://ieeexplore.ieee.org/document/8735810/","","2019","2024-08-29 02:48:25","2024-08-29 02:48:25","2022-12-31 20:26:25","1-1","","","","","IEEE Trans. Pattern Anal. Mach. Intell.","","","","","","","","","","","","","DOI.org (Crossref)","","","","","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TA8H6Q56","preprint","2022","Zhang, Jieyu; Hsieh, Cheng-Yu; Yu, Yue; Zhang, Chao; Ratner, Alexander","A Survey on Programmatic Weak Supervision","","","","","http://arxiv.org/abs/2202.05433","Labeling training data has become one of the major roadblocks to using machine learning. Among various weak supervision paradigms, programmatic weak supervision (PWS) has achieved remarkable success in easing the manual labeling bottleneck by programmatically synthesizing training labels from multiple potentially noisy supervision sources. This paper presents a comprehensive survey of recent advances in PWS. In particular, we give a brief introduction of the PWS learning paradigm, and review representative approaches for each component within PWS's learning workflow. In addition, we discuss complementary learning paradigms for tackling limited labeled data scenarios and how these related approaches can be used in conjunction with PWS. Finally, we identify several critical challenges that remain under-explored in the area to hopefully inspire future research directions in the field.","2022-02-14","2024-08-29 02:48:25","2024-08-29 02:48:25","2022-12-31 20:05:20","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2202.05433 arXiv:2202.05433 [cs, stat]","","C:\Users\frank\Zotero\storage\MKCXI8VW\Zhang et al. - 2022 - A Survey on Programmatic Weak Supervision.pdf; C:\Users\frank\Zotero\storage\WXABEGF6\2202.html","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Applications","","","","","","","","","","","","","","","","","","","arXiv:2202.05433","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2A54V3FF","conferencePaper","2021","Nodet, Pierre; Lemaire, Vincent; Bondu, Alexis; Cornuejols, Antoine; Ouorou, Adam","From Weakly Supervised Learning to Biquality Learning: an Introduction","2021 International Joint Conference on Neural Networks (IJCNN)","978-1-66543-900-8","","10.1109/IJCNN52387.2021.9533353","https://ieeexplore.ieee.org/document/9533353/","","2021-07-18","2024-08-29 02:48:26","2024-08-29 02:48:26","2022-12-31 17:06:53","1-10","","","","","","From Weakly Supervised Learning to Biquality Learning","","","","","IEEE","Shenzhen, China","","","","","","DOI.org (Crossref)","","","","C:\Users\frank\Zotero\storage\UCNT828M\2012.html; C:\Users\frank\Zotero\storage\7BG4ASWW\Nodet et al. - 2021 - From Weakly Supervised Learning to Biquality Learn.pdf","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2021 International Joint Conference on Neural Networks (IJCNN)","","","","","","","","","","","","","","",""
"Z6ZN7CNB","conferencePaper","2009","Mintz, Mike; Bills, Steven; Snow, Rion; Jurafsky, Daniel","Distant supervision for relation extraction without labeled data","Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP","","","","https://aclanthology.org/P09-1113","","2009-08","2024-08-29 02:48:28","2024-08-29 02:48:28","2022-12-30 10:48:53","1003–1011","","","","","","","","","","","Association for Computational Linguistics","Suntec, Singapore","","","","","","ACLWeb","","","","C:\Users\frank\Zotero\storage\GMF884HJ\Mintz et al. - 2009 - Distant supervision for relation extraction withou.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ACL-IJCNLP 2009","","","","","","","","","","","","","","",""
"CH6BF2S8","conferencePaper","2013","Roth, Benjamin; Barth, Tassilo; Wiegand, Michael; Klakow, Dietrich","A survey of noise reduction methods for distant supervision","Proceedings of the 2013 workshop on Automated knowledge base construction - AKBC '13","978-1-4503-2411-3","","10.1145/2509558.2509571","http://dl.acm.org/citation.cfm?doid=2509558.2509571","We survey recent approaches to noise reduction in distant supervision learning for relation extraction. We group them according to the principles they are based on: at-least-one constraints, topic-based models, or pattern correlations. Besides describing them, we illustrate the fundamental diﬀerences and attempt to give an outlook to potentially fruitful further research. In addition, we identify related work in sentiment analysis which could proﬁt from approaches to noise reduction.","2013","2024-08-29 02:48:29","2024-08-29 02:48:29","2022-12-30 10:48:31","73-78","","","","","","","","","","","ACM Press","San Francisco, California, USA","en","","","","","DOI.org (Crossref)","","","","C:\Users\frank\Zotero\storage\3RRJXNR6\Roth et al. - 2013 - A survey of noise reduction methods for distant su.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","the 2013 workshop","","","","","","","","","","","","","","",""
"KUE85DEC","journalArticle","2019","Smirnova, Alisa; Cudré-Mauroux, Philippe","Relation Extraction Using Distant Supervision: A Survey","ACM Computing Surveys","","0360-0300, 1557-7341","10.1145/3241741","https://dl.acm.org/doi/10.1145/3241741","Relation extraction is a subtask of information extraction where               semantic relationships               are extracted from natural language text and then classified. In essence, it allows us to acquire structured knowledge from unstructured text. In this article, we present a survey of relation extraction methods that leverage pre-existing structured or semi-structured data to guide the extraction process. We introduce a taxonomy of existing methods and describe distant supervision approaches in detail. We describe, in addition, the evaluation methodologies and the datasets commonly used for quality assessment. Finally, we give a high-level outlook on the field, highlighting open problems as well as the most promising research directions.","2019-09-30","2024-08-29 02:48:30","2024-08-29 02:48:30","2022-12-30 10:48:00","1-35","","5","51","","ACM Comput. Surv.","Relation Extraction Using Distant Supervision","","","","","","","en","","","","","DOI.org (Crossref)","","Number: 5","","C:\Users\frank\Zotero\storage\ZXL9DC4Q\Smirnova e Cudré-Mauroux - 2019 - Relation Extraction Using Distant Supervision A S.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AG56K5QX","conferencePaper","2019","Shi, Yong; Xiao, Yang; Niu, Lingfeng","A Brief Survey of Relation Extraction Based on Distant Supervision","Computational Science – ICCS 2019","978-3-030-22744-9","","10.1007/978-3-030-22744-9_23","","As a core task and important part of Information ExtractionEntity Relation Extraction can realize the identification of the semantic relation between entity pairs. And it plays an important role in semantic understanding of sentences and the construction of entity knowledge base. It has the potential of employing distant supervision method, end-to-end model and other deep learning model with the creation of large datasets. In this review, we compare the contributions and defect of the various models that have been used for the task, to help guide the path ahead.","2019","2024-08-29 02:48:32","2024-08-29 02:48:32","","293-303","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","C:\Users\frank\Zotero\storage\Q2X7ITQF\Shi et al. - 2019 - A Brief Survey of Relation Extraction Based on Dis.pdf","","notion","Deep learning; Distant supervision; Relation extraction","Rodrigues, João M. F.; Cardoso, Pedro J. S.; Monteiro, Jânio; Lam, Roberto; Krzhizhanovskaya, Valeria V.; Lees, Michael H.; Dongarra, Jack J.; Sloot, Peter M.A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2ISMF36G","conferencePaper","2021","Hedderich, Michael A.; Lange, Lukas; Adel, Heike; Strötgen, Jannik; Klakow, Dietrich","A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios","Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","","","10.18653/v1/2021.naacl-main.201","https://aclanthology.org/2021.naacl-main.201","Deep neural networks and huge language models are becoming omnipresent in natural language applications. As they are known for requiring large amounts of training data, there is a growing body of work to improve the performance in low-resource settings. Motivated by the recent fundamental changes towards neural models and the popular pre-train and fine-tune paradigm, we survey promising approaches for low-resource natural language processing. After a discussion about the different dimensions of data availability, we give a structured overview of methods that enable learning when training data is sparse. This includes mechanisms to create additional labeled data like data augmentation and distant supervision as well as transfer learning settings that reduce the need for target supervision. A goal of our survey is to explain how these methods differ in their requirements as understanding them is essential for choosing a technique suited for a specific low-resource setting. Further key aspects of this work are to highlight open issues and to outline promising directions for future research.","2021-06","2024-08-29 02:48:33","2024-08-29 02:48:33","2022-12-30 10:46:13","2545–2568","","","","","","","","","","","Association for Computational Linguistics","Online","","","","","","ACLWeb","","","","C:\Users\frank\Zotero\storage\X4CAZ9UH\Hedderich et al. - 2021 - A Survey on Recent Approaches for Natural Language.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NAACL-HLT 2021","","","","","","","","","","","","","","",""
"YYDYU24X","journalArticle","2021","Algan, Görkem; Ulusoy, Ilkay","Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey","Knowledge-Based Systems","","09507051","10.1016/j.knosys.2021.106771","http://arxiv.org/abs/1912.05170","Image classification systems recently made a giant leap with the advancement of deep neural networks. However, these systems require an excessive amount of labeled data to be adequately trained. Gathering a correctly annotated dataset is not always feasible due to several factors, such as the expensiveness of the labeling process or difficulty of correctly classifying data, even for the experts. Because of these practical challenges, label noise is a common problem in real-world datasets, and numerous methods to train deep neural networks with label noise are proposed in the literature. Although deep neural networks are known to be relatively robust to label noise, their tendency to overfit data makes them vulnerable to memorizing even random noise. Therefore, it is crucial to consider the existence of label noise and develop counter algorithms to fade away its adverse effects to train deep neural networks efficiently. Even though an extensive survey of machine learning techniques under label noise exists, the literature lacks a comprehensive survey of methodologies centered explicitly around deep learning in the presence of noisy labels. This paper aims to present these algorithms while categorizing them into one of the two subgroups: noise model based and noise model free methods. Algorithms in the first group aim to estimate the noise structure and use this information to avoid the adverse effects of noisy labels. Differently, methods in the second group try to come up with inherently noise robust algorithms by using approaches like robust losses, regularizers or other learning paradigms.","2021-03","2024-08-29 02:48:35","2024-08-29 02:48:35","2022-12-30 10:45:57","106771","","","215","","Knowledge-Based Systems","Image Classification with Deep Learning in the Presence of Noisy Labels","","","","","","","","","","","","arXiv.org","","arXiv:1912.05170 [cs, stat]","","C:\Users\frank\Zotero\storage\GD3ULLGH\Algan e Ulusoy - 2021 - Image Classification with Deep Learning in the Pre.pdf; C:\Users\frank\Zotero\storage\QPDHH5BR\1912.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YES2Q2X2","webpage","2021","Hedderich, Michael A.","A Visual Guide to Low-Resource NLP","Medium","","","","https://towardsdatascience.com/a-visual-guide-to-low-resource-nlp-d7b4c7b1a4bc","An overview of recent approaches that help you train NLP models if you only have limited amounts of labeled data.","2021-09-07","2024-08-29 02:48:37","2024-08-29 02:48:37","2022-12-30 10:44:29","","","","","","","","","","","","","","en","","","","","","","","","C:\Users\frank\Zotero\storage\7USE32YC\a-visual-guide-to-low-resource-nlp-d7b4c7b1a4bc.html","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KWY3Z769","conferencePaper","2021","Sedova, Anastasiia; Stephan, Andreas; Speranskaya, Marina; Roth, Benjamin","Knodle: Modular Weakly Supervised Learning with PyTorch","Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)","","","10.18653/v1/2021.repl4nlp-1.12","http://arxiv.org/abs/2104.11557","Strategies for improving the training and prediction quality of weakly supervised machine learning models vary in how much they are tailored to a specific task or integrated with a specific model architecture. In this work, we introduce Knodle, a software framework that treats weak data annotations, deep learning models, and methods for improving weakly supervised training as separate, modular components. This modularization gives the training process access to fine-grained information such as data set characteristics, matches of heuristic rules, or elements of the deep learning model ultimately used for prediction. Hence, our framework can encompass a wide range of training methods for improving weak supervision, ranging from methods that only look at correlations of rules and output classes (independently of the machine learning model trained with the resulting labels), to those that harness the interplay of neural networks and weakly labeled data. We illustrate the benchmarking potential of the framework with a performance comparison of several reference implementations on a selection of datasets that are already available in Knodle. The framework is published as an open-source Python package knodle and available at https://github.com/knodle/knodle.","2021","2024-08-29 02:48:37","2024-08-29 02:48:37","2022-12-30 10:05:47","100-111","","","","","","Knodle","","","","","","","","","","","","arXiv.org","","arXiv:2104.11557 [cs]","","C:\Users\frank\Zotero\storage\V794N3SP\Sedova et al. - 2021 - Knodle Modular Weakly Supervised Learning with Py.pdf; C:\Users\frank\Zotero\storage\FT4R44SQ\2104.html","","notion","Computer Science - Machine Learning; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3W8FBNWW","preprint","2020","Chen, Mayee F.; Fu, Daniel Y.; Sala, Frederic; Wu, Sen; Mullapudi, Ravi Teja; Poms, Fait; Fatahalian, Kayvon; Ré, Christopher","Train and You'll Miss It: Interactive Model Iteration with Weak Supervision and Pre-Trained Embeddings","","","","10.48550/arXiv.2006.15168","http://arxiv.org/abs/2006.15168","Our goal is to enable machine learning systems to be trained interactively. This requires models that perform well and train quickly, without large amounts of hand-labeled data. We take a step forward in this direction by borrowing from weak supervision (WS), wherein models can be trained with noisy sources of signal instead of hand-labeled data. But WS relies on training downstream deep networks to extrapolate to unseen data points, which can take hours or days. Pre-trained embeddings can remove this requirement. We do not use the embeddings as features as in transfer learning (TL), which requires fine-tuning for high performance, but instead use them to define a distance function on the data and extend WS source votes to nearby points. Theoretically, we provide a series of results studying how performance scales with changes in source coverage, source accuracy, and the Lipschitzness of label distributions in the embedding space, and compare this rate to standard WS without extension and TL without fine-tuning. On six benchmark NLP and video tasks, our method outperforms WS without extension by 4.1 points, TL without fine-tuning by 12.8 points, and traditionally-supervised deep networks by 13.1 points, and comes within 0.7 points of state-of-the-art weakly-supervised deep networks-all while training in less than half a second.","2020-06-26","2024-08-29 02:48:39","2024-08-29 02:48:39","2022-12-29 19:40:11","","","","","","","Train and You'll Miss It","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2006.15168 arXiv:2006.15168 [cs, stat]","","C:\Users\frank\Zotero\storage\IAN44RYT\Chen et al. - 2020 - Train and You'll Miss It Interactive Model Iterat.pdf; C:\Users\frank\Zotero\storage\89R67DRL\2006.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2006.15168","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QXNN7LMJ","journalArticle","","Ré, Chris","Weak Supervision Overview","","","","","","","","2024-08-29 02:48:42","2024-08-29 02:48:42","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\frank\Zotero\storage\KN644HDU\Ré - Weak Supervision Overview.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YHJ3IUSM","preprint","2021","Cachay, Salva Rühling; Boecking, Benedikt; Dubrawski, Artur","End-to-End Weak Supervision","","","","10.48550/arXiv.2107.02233","http://arxiv.org/abs/2107.02233","Aggregating multiple sources of weak supervision (WS) can ease the data-labeling bottleneck prevalent in many machine learning applications, by replacing the tedious manual collection of ground truth labels. Current state of the art approaches that do not use any labeled training data, however, require two separate modeling steps: Learning a probabilistic latent variable model based on the WS sources -- making assumptions that rarely hold in practice -- followed by downstream model training. Importantly, the first step of modeling does not consider the performance of the downstream model. To address these caveats we propose an end-to-end approach for directly learning the downstream model by maximizing its agreement with probabilistic labels generated by reparameterizing previous probabilistic posteriors with a neural network. Our results show improved performance over prior work in terms of end model performance on downstream test sets, as well as in terms of improved robustness to dependencies among weak supervision sources.","2021-11-30","2024-08-29 02:48:43","2024-08-29 02:48:43","2022-12-29 17:54:09","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2107.02233 arXiv:2107.02233 [cs, stat]","","C:\Users\frank\Zotero\storage\9IH8QIVN\Cachay et al. - 2021 - End-to-End Weak Supervision.pdf; C:\Users\frank\Zotero\storage\USB3X6LM\2107.html","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2107.02233","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K2KN98S7","preprint","2021","Biegel, Samantha; El-Khatib, Rafah; Oliveira, Luiz Otavio Vilas Boas; Baak, Max; Aben, Nanne","Active WeaSuL: Improving Weak Supervision with Active Learning","","","","10.48550/arXiv.2104.14847","http://arxiv.org/abs/2104.14847","The availability of labelled data is one of the main limitations in machine learning. We can alleviate this using weak supervision: a framework that uses expert-defined rules $\boldsymbol{\lambda}$ to estimate probabilistic labels $p(y|\boldsymbol{\lambda})$ for the entire data set. These rules, however, are dependent on what experts know about the problem, and hence may be inaccurate or may fail to capture important parts of the problem-space. To mitigate this, we propose Active WeaSuL: an approach that incorporates active learning into weak supervision. In Active WeaSuL, experts do not only define rules, but they also iteratively provide the true label for a small set of points where the weak supervision model is most likely to be mistaken, which are then used to better estimate the probabilistic labels. In this way, the weak labels provide a warm start, which active learning then improves upon. We make two contributions: 1) a modification of the weak supervision loss function, such that the expert-labelled data inform and improve the combination of weak labels; and 2) the maxKL divergence sampling strategy, which determines for which data points expert labelling is most beneficial. Our experiments show that when the budget for labelling data is limited (e.g. $\leq 60$ data points), Active WeaSuL outperforms weak supervision, active learning, and competing strategies, with only a handful of labelled data points. This makes Active WeaSuL ideal for situations where obtaining labelled data is difficult.","2021-04-30","2024-08-29 02:48:45","2024-08-29 02:48:45","2022-12-29 17:10:26","","","","","","","Active WeaSuL","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2104.14847 arXiv:2104.14847 [cs, stat]","","C:\Users\frank\Zotero\storage\PNWF7IDL\Biegel et al. - 2021 - Active WeaSuL Improving Weak Supervision with Act.pdf; C:\Users\frank\Zotero\storage\YXL6PRM8\2104.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2104.14847","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YXCNAVLP","preprint","2021","Boecking, Benedikt; Neiswanger, Willie; Xing, Eric; Dubrawski, Artur","Interactive Weak Supervision: Learning Useful Heuristics for Data Labeling","","","","10.48550/arXiv.2012.06046","http://arxiv.org/abs/2012.06046","Obtaining large annotated datasets is critical for training successful machine learning models and it is often a bottleneck in practice. Weak supervision offers a promising alternative for producing labeled datasets without ground truth annotations by generating probabilistic labels using multiple noisy heuristics. This process can scale to large datasets and has demonstrated state of the art performance in diverse domains such as healthcare and e-commerce. One practical issue with learning from user-generated heuristics is that their creation requires creativity, foresight, and domain expertise from those who hand-craft them, a process which can be tedious and subjective. We develop the first framework for interactive weak supervision in which a method proposes heuristics and learns from user feedback given on each proposed heuristic. Our experiments demonstrate that only a small number of feedback iterations are needed to train models that achieve highly competitive test set performance without access to ground truth training labels. We conduct user studies, which show that users are able to effectively provide feedback on heuristics and that test set results track the performance of simulated oracles.","2021-01-25","2024-08-29 02:48:46","2024-08-29 02:48:46","2022-12-21 20:33:08","","","","","","","Interactive Weak Supervision","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2012.06046 arXiv:2012.06046 [cs, stat]","","C:\Users\frank\Zotero\storage\6IJ5WJCR\Boecking et al. - 2021 - Interactive Weak Supervision Learning Useful Heur.pdf; C:\Users\frank\Zotero\storage\QK5V4AV8\2012.html","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2012.06046","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KMK93KHY","preprint","2017","Ratner, Alexander; De Sa, Christopher; Wu, Sen; Selsam, Daniel; Ré, Christopher","Data Programming: Creating Large Training Sets, Quickly","","","","10.48550/arXiv.1605.07723","http://arxiv.org/abs/1605.07723","Large labeled training sets are the critical building blocks of supervised learning methods and are key enablers of deep learning techniques. For some applications, creating labeled training sets is the most time-consuming and expensive part of applying machine learning. We therefore propose a paradigm for the programmatic creation of training sets called data programming in which users express weak supervision strategies or domain heuristics as labeling functions, which are programs that label subsets of the data, but that are noisy and may conflict. We show that by explicitly representing this training set labeling process as a generative model, we can ""denoise"" the generated training set, and establish theoretically that we can recover the parameters of these generative models in a handful of settings. We then show how to modify a discriminative loss function to make it noise-aware, and demonstrate our method over a range of discriminative models including logistic regression and LSTMs. Experimentally, on the 2014 TAC-KBP Slot Filling challenge, we show that data programming would have led to a new winning score, and also show that applying data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points over a state-of-the-art LSTM baseline (and into second place in the competition). Additionally, in initial user studies we observed that data programming may be an easier way for non-experts to create machine learning models when training data is limited or unavailable.","2017-01-08","2024-08-29 02:48:48","2024-08-29 02:48:48","2022-12-17 20:21:35","","","","","","","Data Programming","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:1605.07723 arXiv:1605.07723 [cs, stat] version: 3","","C:\Users\frank\Zotero\storage\P5V9I8R2\Ratner et al. - 2017 - Data Programming Creating Large Training Sets, Qu.pdf; C:\Users\frank\Zotero\storage\9K8FCU99\1605.html","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1605.07723","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TRCMXGEA","conferencePaper","","","Interactive Programmatic Labeling for Weak Supervision","","","","","","","","2024-08-29 02:48:50","2024-08-29 02:48:50","","","","","","","","","","","","","","","en","","","","","","","Benjamin Cohen-Wang, Stephen Mussmann, Alex Ratner, and Chris Ré. 2019. Interactive Programmatic Labeling for Weak Supervision. In Data Collection, Curation, and Labeling for Mining and Learning Workshop at KDD ’19, August 04–08, 2019, Anchorage, AK. ACM, New York, NY, U","","C:\Users\frank\Zotero\storage\JPCGIH84\Hu e Liu - 2004 - Mining and summarizing customer reviews.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FGS6HVUQ","preprint","2019","Fonseca, Eduardo; Font, Frederic; Serra, Xavier","Model-agnostic Approaches to Handling Noisy Labels When Training Sound Event Classifiers","","","","","http://arxiv.org/abs/1910.12004","Label noise is emerging as a pressing issue in sound event classification. This arises as we move towards larger datasets that are difficult to annotate manually, but it is even more severe if datasets are collected automatically from online repositories, where labels are inferred through automated heuristics applied to the audio content or metadata. While learning from noisy labels has been an active area of research in computer vision, it has received little attention in sound event classification. Most recent computer vision approaches against label noise are relatively complex, requiring complex networks or extra data resources. In this work, we evaluate simple and efficient model-agnostic approaches to handling noisy labels when training sound event classifiers, namely label smoothing regularization, mixup and noise-robust loss functions. The main advantage of these methods is that they can be easily incorporated to existing deep learning pipelines without need for network modifications or extra resources. We report results from experiments conducted with the FSDnoisy18k dataset. We show that these simple methods can be effective in mitigating the effect of label noise, providing up to 2.5\% of accuracy boost when incorporated to two different CNNs, while requiring minimal intervention and computational overhead.","2019-10-26","2024-08-29 02:48:51","2024-08-29 02:48:51","2022-11-08 17:16:10","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:1910.12004 arXiv:1910.12004 [cs, eess, stat]","","C:\Users\frank\Zotero\storage\4NQ7LUYF\Fonseca et al. - 2019 - Model-agnostic Approaches to Handling Noisy Labels.pdf; C:\Users\frank\Zotero\storage\AJITJFI4\1910.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Sound; Electrical Engineering and Systems Science - Audio and Speech Processing","","","","","","","","","","","","","","","","","","","arXiv:1910.12004","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"URS3S7X8","conferencePaper","2020","Lukasik, Michal; Bhojanapalli, Srinadh; Menon, Aditya; Kumar, Sanjiv","Does label smoothing mitigate label noise?","Proceedings of the 37th International Conference on Machine Learning","","","","https://proceedings.mlr.press/v119/lukasik20a.html","Label smoothing is commonly used in training deep learning models, wherein one-hot training labels are mixed with uniform label vectors. Empirically, smoothing has been shown to improve both predictive performance and model calibration. In this paper, we study whether label smoothing is also effective as a means of coping with label noise. While label smoothing apparently amplifies this problem — being equivalent to injecting symmetric noise to the labels — we show how it relates to a general family of loss-correction techniques from the label noise literature. Building on this connection, we show that label smoothing is competitive with loss-correction under label noise. Further, we show that when distilling models from noisy data, label smoothing of the teacher is beneficial; this is in contrast to recent findings for noise-free problems, and sheds further light on settings where label smoothing is beneficial.","2020-11-21","2024-08-29 02:48:52","2024-08-29 02:48:52","2022-11-08 17:14:40","6448-6458","","","","","","","","","","","PMLR","","en","","","","","proceedings.mlr.press","","ISSN: 2640-3498","","C:\Users\frank\Zotero\storage\PQSTYA2V\Lukasik et al. - 2020 - Does label smoothing mitigate label noise.pdf; C:\Users\frank\Zotero\storage\G9JBKS9P\Lukasik et al. - 2020 - Does label smoothing mitigate label noise.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Machine Learning","","","","","","","","","","","","","","",""
"TZKW4WXL","preprint","2022","Wei, Jiaheng; Liu, Hangyu; Liu, Tongliang; Niu, Gang; Sugiyama, Masashi; Liu, Yang","To Smooth or Not? When Label Smoothing Meets Noisy Labels","","","","10.48550/arXiv.2106.04149","http://arxiv.org/abs/2106.04149","Label smoothing (LS) is an arising learning paradigm that uses the positively weighted average of both the hard training labels and uniformly distributed soft labels. It was shown that LS serves as a regularizer for training data with hard labels and therefore improves the generalization of the model. Later it was reported LS even helps with improving robustness when learning with noisy labels. However, we observed that the advantage of LS vanishes when we operate in a high label noise regime. Intuitively speaking, this is due to the increased entropy of $\mathbb{P}(\text{noisy label}|X)$ when the noise rate is high, in which case, further applying LS tends to ""over-smooth"" the estimated posterior. We proceeded to discover that several learning-with-noisy-labels solutions in the literature instead relate more closely to negative/not label smoothing (NLS), which acts counter to LS and defines as using a negative weight to combine the hard and soft labels! We provide understandings for the properties of LS and NLS when learning with noisy labels. Among other established properties, we theoretically show NLS is considered more beneficial when the label noise rates are high. We provide extensive experimental results on multiple benchmarks to support our findings too. Code is publicly available at https://github.com/UCSC-REAL/negative-label-smoothing.","2022-06-24","2024-08-29 02:48:53","2024-08-29 02:48:53","2022-11-08 17:03:25","","","","","","","To Smooth or Not?","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2106.04149 arXiv:2106.04149 [cs]","","C:\Users\frank\Zotero\storage\N5ULZK68\Wei et al. - 2022 - To Smooth or Not When Label Smoothing Meets Noisy.pdf; C:\Users\frank\Zotero\storage\LYDEA3P3\2106.html","","notion","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2106.04149","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MK7N9PJC","conferencePaper","2021","Lison, Pierre; Barnes, Jeremy; Hubin, Aliaksandr","skweak: Weak Supervision Made Easy for NLP","Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations","","","10.18653/v1/2021.acl-demo.40","http://arxiv.org/abs/2104.09683","We present skweak, a versatile, Python-based software toolkit enabling NLP developers to apply weak supervision to a wide range of NLP tasks. Weak supervision is an emerging machine learning paradigm based on a simple idea: instead of labelling data points by hand, we use labelling functions derived from domain knowledge to automatically obtain annotations for a given dataset. The resulting labels are then aggregated with a generative model that estimates the accuracy (and possible confusions) of each labelling function. The skweak toolkit makes it easy to implement a large spectrum of labelling functions (such as heuristics, gazetteers, neural models or linguistic constraints) on text data, apply them on a corpus, and aggregate their results in a fully unsupervised fashion. skweak is especially designed to facilitate the use of weak supervision for NLP tasks such as text classification and sequence labelling. We illustrate the use of skweak for NER and sentiment analysis. skweak is released under an open-source license and is available at: https://github.com/NorskRegnesentral/skweak","2021","2024-08-29 02:48:55","2024-08-29 02:48:55","2022-11-08 16:44:56","337-346","","","","","","skweak","","","","","","","","","","","","arXiv.org","","arXiv:2104.09683 [cs]","","C:\Users\frank\Zotero\storage\SU29VTM4\Lison et al. - 2021 - skweak Weak Supervision Made Easy for NLP.pdf; C:\Users\frank\Zotero\storage\8CAF7H9A\2104.html","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L6PXDS3Z","preprint","2022","Tunstall, Lewis; Reimers, Nils; Jo, Unso Eun Seo; Bates, Luke; Korat, Daniel; Wasserblat, Moshe; Pereg, Oren","Efficient Few-Shot Learning Without Prompts","","","","10.48550/arXiv.2209.11055","http://arxiv.org/abs/2209.11055","Recent few-shot methods, such as parameter-efficient fine-tuning (PEFT) and pattern exploiting training (PET), have achieved impressive results in label-scarce settings. However, they are difficult to employ since they are subject to high variability from manually crafted prompts, and typically require billion-parameter language models to achieve high accuracy. To address these shortcomings, we propose SetFit (Sentence Transformer Fine-tuning), an efficient and prompt-free framework for few-shot fine-tuning of Sentence Transformers (ST). SetFit works by first fine-tuning a pretrained ST on a small number of text pairs, in a contrastive Siamese manner. The resulting model is then used to generate rich text embeddings, which are used to train a classification head. This simple framework requires no prompts or verbalizers, and achieves high accuracy with orders of magnitude less parameters than existing techniques. Our experiments show that SetFit obtains comparable results with PEFT and PET techniques, while being an order of magnitude faster to train. We also show that SetFit can be applied in multilingual settings by simply switching the ST body. Our code is available at https://github.com/huggingface/setfit and our datasets at https://huggingface.co/setfit .","2022-09-22","2024-08-29 02:48:56","2024-08-29 02:48:56","2022-10-29 11:16:44","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2209.11055 arXiv:2209.11055 [cs]","","C:\Users\frank\Zotero\storage\AV5IZA4T\Tunstall et al. - 2022 - Efficient Few-Shot Learning Without Prompts.pdf; C:\Users\frank\Zotero\storage\N8YXIZ4L\2209.html","","notion","Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2209.11055","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YZ875CN6","preprint","2022","Northcutt, Curtis G.; Jiang, Lu; Chuang, Isaac L.","Confident Learning: Estimating Uncertainty in Dataset Labels","","","","10.48550/arXiv.1911.00068","http://arxiv.org/abs/1911.00068","Learning exists in the context of data, yet notions of confidence typically focus on model predictions, not label quality. Confident learning (CL) is an alternative approach which focuses instead on label quality by characterizing and identifying label errors in datasets, based on the principles of pruning noisy data, counting with probabilistic thresholds to estimate noise, and ranking examples to train with confidence. Whereas numerous studies have developed these principles independently, here, we combine them, building on the assumption of a class-conditional noise process to directly estimate the joint distribution between noisy (given) labels and uncorrupted (unknown) labels. This results in a generalized CL which is provably consistent and experimentally performant. We present sufficient conditions where CL exactly finds label errors, and show CL performance exceeding seven recent competitive approaches for learning with noisy labels on the CIFAR dataset. Uniquely, the CL framework is not coupled to a specific data modality or model (e.g., we use CL to find several label errors in the presumed error-free MNIST dataset and improve sentiment classification on text data in Amazon Reviews). We also employ CL on ImageNet to quantify ontological class overlap (e.g., estimating 645 ""missile"" images are mislabeled as their parent class ""projectile""), and moderately increase model accuracy (e.g., for ResNet) by cleaning data prior to training. These results are replicable using the open-source cleanlab release.","2022-08-21","2024-08-29 02:48:57","2024-08-29 02:48:57","2022-10-14 21:11:11","","","","","","","Confident Learning","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:1911.00068 arXiv:1911.00068 [cs, stat]","","C:\Users\frank\Zotero\storage\5HBJKZPZ\Northcutt et al. - 2022 - Confident Learning Estimating Uncertainty in Data.pdf; C:\Users\frank\Zotero\storage\2J7NVKC6\1911.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1911.00068","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JLZ8IBAG","preprint","2020","Mukherjee, Subhabrata; Awadallah, Ahmed Hassan","Uncertainty-aware Self-training for Text Classification with Few Labels","","","","10.48550/arXiv.2006.15315","http://arxiv.org/abs/2006.15315","Recent success of large-scale pre-trained language models crucially hinge on fine-tuning them on large amounts of labeled data for the downstream task, that are typically expensive to acquire. In this work, we study self-training as one of the earliest semi-supervised learning approaches to reduce the annotation bottleneck by making use of large-scale unlabeled data for the target task. Standard self-training mechanism randomly samples instances from the unlabeled pool to pseudo-label and augment labeled data. In this work, we propose an approach to improve self-training by incorporating uncertainty estimates of the underlying neural network leveraging recent advances in Bayesian deep learning. Specifically, we propose (i) acquisition functions to select instances from the unlabeled pool leveraging Monte Carlo (MC) Dropout, and (ii) learning mechanism leveraging model confidence for self-training. As an application, we focus on text classification on five benchmark datasets. We show our methods leveraging only 20-30 labeled samples per class for each task for training and for validation can perform within 3% of fully supervised pre-trained language models fine-tuned on thousands of labeled instances with an aggregate accuracy of 91% and improving by upto 12% over baselines.","2020-06-27","2024-08-29 02:49:00","2024-08-29 02:49:00","2022-08-18 19:08:49","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2006.15315 arXiv:2006.15315 [cs]","","C:\Users\frank\Zotero\storage\NNNJFALQ\Mukherjee e Awadallah - 2020 - Uncertainty-aware Self-training for Text Classific.pdf; C:\Users\frank\Zotero\storage\FR4XR6F8\2006.html","","notion","Computer Science - Machine Learning; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2006.15315","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5XVEYIA2","journalArticle","2022","Chen, Li-Ming; Xiu, Bao-Xin; Ding, Zhao-Yun","Multiple weak supervision for short text classification","Applied Intelligence","","0924-669X","10.1007/s10489-021-02958-3","https://doi.org/10.1007/s10489-021-02958-3","For short text classification, insufficient labeled data, data sparsity, and imbalanced classification have become three major challenges. For this, we proposed multiple weak supervision, which can label unlabeled data automatically. Different from prior work, the proposed method can generate probabilistic labels through conditional independent model. What’s more, experiments were conducted to verify the effectiveness of multiple weak supervision. According to experimental results on public dadasets, real datasets and synthetic datasets, unlabeled imbalanced short text classification problem can be solved effectively by multiple weak supervision. Notably, without reducing precision, recall, and F1-score can be improved by adding distant supervision clustering, which can be used to meet different application needs.","2022-06-01","2024-08-29 02:49:02","2024-08-29 02:49:02","2023-01-20 10:58:53","9101–9116","","8","52","","Applied Intelligence","","","","","","","","","","","","","Jun 2022","","Number: 8","","C:\Users\frank\Zotero\storage\S9DMNSSK\Chen et al. - 2022 - Multiple weak supervision for short text classific.pdf","","notion","Distant supervision clustering; Imbalanced classification; Multiple weak supervision; Probabilistic labels; Short text classification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LWZEPDAT","conferencePaper","2020","Mekala, Dheeraj; Shang, Jingbo","Contextualized Weak Supervision for Text Classification","Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics","","","10.18653/v1/2020.acl-main.30","https://aclanthology.org/2020.acl-main.30","Weakly supervised text classification based on a few user-provided seed words has recently attracted much attention from researchers. Existing methods mainly generate pseudo-labels in a context-free manner (e.g., string matching), therefore, the ambiguous, context-dependent nature of human language has been long overlooked. In this paper, we propose a novel framework ConWea, providing contextualized weak supervision for text classification. Specifically, we leverage contextualized representations of word occurrences and seed word information to automatically differentiate multiple interpretations of the same word, and thus create a contextualized corpus. This contextualized corpus is further utilized to train the classifier and expand seed words in an iterative manner. This process not only adds new contextualized, highly label-indicative keywords but also disambiguates initial seed words, making our weak supervision fully contextualized. Extensive experiments and case studies on real-world datasets demonstrate the necessity and significant advantages of using contextualized weak supervision, especially when the class labels are fine-grained.","2020-07","2024-08-29 02:49:03","2024-08-29 02:49:03","2023-01-20 11:02:25","323–333","","","","","","","","","","","Association for Computational Linguistics","Online","","","","","","ACLWeb","","","","C:\Users\frank\Zotero\storage\XXSFJ3YG\Mekala e Shang - 2020 - Contextualized Weak Supervision for Text Classific.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ACL 2020","","","","","","","","","","","","","","",""
"B7N6IEUK","preprint","2022","Metzenthin, Emanuel; Bartz, Christian; Meinel, Christoph","Weakly Supervised Scene Text Detection using Deep Reinforcement Learning","","","","10.48550/arXiv.2201.04866","http://arxiv.org/abs/2201.04866","The challenging field of scene text detection requires complex data annotation, which is time-consuming and expensive. Techniques, such as weak supervision, can reduce the amount of data needed. In this paper we propose a weak supervision method for scene text detection, which makes use of reinforcement learning (RL). The reward received by the RL agent is estimated by a neural network, instead of being inferred from ground-truth labels. First, we enhance an existing supervised RL approach to text detection with several training optimizations, allowing us to close the performance gap to regression-based algorithms. We then use our proposed system in a weakly- and semi-supervised training on real-world data. Our results show that training in a weakly supervised setting is feasible. However, we find that using our model in a semi-supervised setting , e.g. when combining labeled synthetic data with unannotated real-world data, produces the best results.","2022-01-13","2024-08-29 02:49:04","2024-08-29 02:49:04","2023-01-20 11:05:59","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2201.04866 arXiv:2201.04866 [cs]","","C:\Users\frank\Zotero\storage\X9TPEKBK\Metzenthin et al. - 2022 - Weakly Supervised Scene Text Detection using Deep .pdf; C:\Users\frank\Zotero\storage\BVUDH7B2\2201.html","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2201.04866","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZLNIWP78","preprint","2020","Robinson, Joshua; Jegelka, Stefanie; Sra, Suvrit","Strength from Weakness: Fast Learning Using Weak Supervision","","","","","http://arxiv.org/abs/2002.08483","We study generalization properties of weakly supervised learning. That is, learning where only a few ""strong"" labels (the actual target of our prediction) are present but many more ""weak"" labels are available. In particular, we show that having access to weak labels can significantly accelerate the learning rate for the strong task to the fast rate of $\mathcal{O}(\nicefrac1n)$, where $n$ denotes the number of strongly labeled data points. This acceleration can happen even if by itself the strongly labeled data admits only the slower $\mathcal{O}(\nicefrac{1}{\sqrt{n}})$ rate. The actual acceleration depends continuously on the number of weak labels available, and on the relation between the two tasks. Our theoretical results are reflected empirically across a range of tasks and illustrate how weak labels speed up learning on the strong task.","2020-02-19","2024-08-29 02:49:06","2024-08-29 02:49:06","2023-01-20 11:11:16","","","","","","","Strength from Weakness","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2002.08483 arXiv:2002.08483 [cs, stat]","","C:\Users\frank\Zotero\storage\4E8FS77B\Robinson et al. - 2020 - Strength from Weakness Fast Learning Using Weak S.pdf; C:\Users\frank\Zotero\storage\5XUL6P8P\2002.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2002.08483","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8IMMUIHM","book","2022","Tok, Wee-Hyong; Bahree, Amit; Filipi, Senja","Practical weak supervision: doing more with less data","","978-1-4920-7706-0","","","","","2022","2024-08-29 02:49:07","2024-08-29 02:49:07","","","169","","","","","Practical weak supervision","","","","","O'Reilly","Cambridge","","","","","","Library of Congress ISBN","Q325.75 .T65 2022","OCLC: on1272887561","","","","notion","Apprentissage supervisé (Intelligence artificielle); Computer vision; Natural Language Processing; Natural language processing (Computer science); Supervised learning (Machine learning); Traitement automatique des langues naturelles; Vision par ordinateur","","","","","","","","","","","","","","","","","","","","First edition","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DD3V83B2","bookSection","2020","Zhang, Minqing; Gao, Jiantao; Lyu, Zhen; Zhao, Weibing; Wang, Qin; Ding, Weizhen; Wang, Sheng; Li, Zhen; Cui, Shuguang","Characterizing Label Errors: Confident Learning for Noisy-Labeled Image Segmentation","Medical Image Computing and Computer Assisted Intervention – MICCAI 2020","978-3-030-59709-2 978-3-030-59710-8","","","https://link.springer.com/10.1007/978-3-030-59710-8_70","","2020","2024-08-29 02:49:07","2024-08-29 02:49:07","2023-01-20 11:17:05","721-730","","","12261","","","Characterizing Label Errors","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-59710-8_70","","C:\Users\frank\Zotero\storage\WAUPIJG7\Zhang et al. - 2020 - Characterizing Label Errors Confident Learning fo.pdf","","notion","","Martel, Anne L.; Abolmaesumi, Purang; Stoyanov, Danail; Mateus, Diana; Zuluaga, Maria A.; Zhou, S. Kevin; Racoceanu, Daniel; Joskowicz, Leo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z4MB2WAC","journalArticle","2022","Zhang, Jing","Knowledge Learning With Crowdsourcing: A Brief Review and Systematic Perspective","IEEE/CAA Journal of Automatica Sinica","","2329-9274","10.1109/JAS.2022.105434","","Big data have the characteristics of enormous volume, high velocity, diversity, value-sparsity, and uncertainty, which lead the knowledge learning from them full of challenges. With the emergence of crowdsourcing, versatile information can be obtained on-demand so that the wisdom of crowds is easily involved to facilitate the knowledge learning process. During the past thirteen years, researchers in the AI community made great efforts to remove the obstacles in the field of learning from crowds. This concentrated survey paper comprehensively reviews the technical progress in crowdsourcing learning from a systematic perspective that includes three dimensions of data, models, and learning processes. In addition to reviewing existing important work, the paper places a particular emphasis on providing some promising blueprints on each dimension as well as discussing the lessons learned from our past research work, which will light up the way for new researchers and encourage them to pursue new contributions.","2022-05","2024-08-29 02:49:12","2024-08-29 02:49:12","","749-762","","5","9","","","Knowledge Learning With Crowdsourcing","","","","","","","","","","","","IEEE Xplore","","Number: 5 Conference Name: IEEE/CAA Journal of Automatica Sinica","","C:\Users\frank\Zotero\storage\GTQTK6KT\9763477.html; C:\Users\frank\Zotero\storage\DP4W653I\Zhang - 2022 - Knowledge Learning With Crowdsourcing A Brief Rev.pdf","","notion","Costs; Crowdsourcing; data fusion; Data models; Lead; learning from crowds; learning paradigms; learning with uncertainty; Systematics; Training; Uncertainty","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3IZM6KAA","bookSection","2021","Zhou, Zhi-Hua","Semi-Supervised Learning","Machine Learning","9789811519666 9789811519673","","","https://link.springer.com/10.1007/978-981-15-1967-3_13","","2021","2024-08-29 02:49:15","2024-08-29 02:49:15","2023-01-20 11:26:39","315-341","","","","","","","","","","","Springer Singapore","Singapore","en","","","","","DOI.org (Crossref)","","Book Authors: _:n464 DOI: 10.1007/978-981-15-1967-3_13","","","","notion","","","","","","","Zhou, Zhi-Hua","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QZJYKJ8F","conferencePaper","2021","Yu, Yue; Zuo, Simiao; Jiang, Haoming; Ren, Wendi; Zhao, Tuo; Zhang, Chao","Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach","Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","","","10.18653/v1/2021.naacl-main.84","https://aclanthology.org/2021.naacl-main.84","Fine-tuned pre-trained language models (LMs) have achieved enormous success in many natural language processing (NLP) tasks, but they still require excessive labeled data in the fine-tuning stage. We study the problem of fine-tuning pre-trained LMs using only weak supervision, without any labeled data. This problem is challenging because the high capacity of LMs makes them prone to overfitting the noisy labels generated by weak supervision. To address this problem, we develop a contrastive self-training framework, COSINE, to enable fine-tuning LMs with weak supervision. Underpinned by contrastive regularization and confidence-based reweighting, our framework gradually improves model fitting while effectively suppressing error propagation. Experiments on sequence, token, and sentence pair classification tasks show that our model outperforms the strongest baseline by large margins and achieves competitive performance with fully-supervised fine-tuning methods. Our implementation is available on https://github.com/yueyu1030/COSINE.","2021-06","2024-08-29 02:49:15","2024-08-29 02:49:15","2023-01-22 13:18:49","1063–1077","","","","","","Fine-Tuning Pre-trained Language Model with Weak Supervision","","","","","Association for Computational Linguistics","Online","","","","","","ACLWeb","","","","C:\Users\frank\Zotero\storage\JK2DBSB4\Yu et al. - 2021 - Fine-Tuning Pre-trained Language Model with Weak S.pdf; C:\Users\frank\Zotero\storage\UPG3UJRC\2010.html; C:\Users\frank\Zotero\storage\DFLBCKRY\Yu et al. - 2021 - Fine-Tuning Pre-trained Language Model with Weak S.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NAACL-HLT 2021","","","","","","","","","","","","","","",""
"EJH2XBH3","preprint","2022","Wei, Jiaheng; Liu, Hangyu; Liu, Tongliang; Niu, Gang; Sugiyama, Masashi; Liu, Yang","To Smooth or Not? When Label Smoothing Meets Noisy Labels","","","","10.48550/arXiv.2106.04149","http://arxiv.org/abs/2106.04149","Label smoothing (LS) is an arising learning paradigm that uses the positively weighted average of both the hard training labels and uniformly distributed soft labels. It was shown that LS serves as a regularizer for training data with hard labels and therefore improves the generalization of the model. Later it was reported LS even helps with improving robustness when learning with noisy labels. However, we observed that the advantage of LS vanishes when we operate in a high label noise regime. Intuitively speaking, this is due to the increased entropy of $\mathbb{P}(\text{noisy label}|X)$ when the noise rate is high, in which case, further applying LS tends to ""over-smooth"" the estimated posterior. We proceeded to discover that several learning-with-noisy-labels solutions in the literature instead relate more closely to negative/not label smoothing (NLS), which acts counter to LS and defines as using a negative weight to combine the hard and soft labels! We provide understandings for the properties of LS and NLS when learning with noisy labels. Among other established properties, we theoretically show NLS is considered more beneficial when the label noise rates are high. We provide extensive experimental results on multiple benchmarks to support our findings too. Code is publicly available at https://github.com/UCSC-REAL/negative-label-smoothing.","2022-06-24","2024-08-29 02:49:18","2024-08-29 02:49:18","2023-01-22 13:39:51","","","","","","","To Smooth or Not?","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2106.04149 arXiv:2106.04149 [cs]","","C:\Users\frank\Zotero\storage\5X2XXVR8\Wei et al. - 2022 - To Smooth or Not When Label Smoothing Meets Noisy.pdf; C:\Users\frank\Zotero\storage\MVB4TFCQ\2106.html","","notion","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2106.04149","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IQQVHU9V","journalArticle","2017","Ratner, Alexander; Bach, Stephen H.; Ehrenberg, Henry; Fries, Jason; Wu, Sen; Ré, Christopher","Snorkel: Rapid Training Data Creation with Weak Supervision","Proceedings of the VLDB Endowment","","2150-8097","10.14778/3157794.3157797","http://arxiv.org/abs/1711.10160","Labeling training data is increasingly the largest bottleneck in deploying machine learning systems. We present Snorkel, a first-of-its-kind system that enables users to train state-of-the-art models without hand labeling any training data. Instead, users write labeling functions that express arbitrary heuristics, which can have unknown accuracies and correlations. Snorkel denoises their outputs without access to ground truth by incorporating the first end-to-end implementation of our recently proposed machine learning paradigm, data programming. We present a flexible interface layer for writing labeling functions based on our experience over the past year collaborating with companies, agencies, and research labs. In a user study, subject matter experts build models 2.8x faster and increase predictive performance an average 45.5% versus seven hours of hand labeling. We study the modeling tradeoffs in this new setting and propose an optimizer for automating tradeoff decisions that gives up to 1.8x speedup per pipeline execution. In two collaborations, with the U.S. Department of Veterans Affairs and the U.S. Food and Drug Administration, and on four open-source text and image data sets representative of other deployments, Snorkel provides 132% average improvements to predictive performance over prior heuristic approaches and comes within an average 3.60% of the predictive performance of large hand-curated training sets.","2017-11","2024-08-29 02:49:21","2024-08-29 02:49:21","2023-01-22 18:21:39","269-282","","3","11","","Proc. VLDB Endow.","Snorkel","","","","","","","","","","","","arXiv.org","","Number: 3 arXiv:1711.10160 [cs, stat]","","C:\Users\frank\Zotero\storage\GE9ELL6X\Ratner et al. - 2017 - Snorkel Rapid Training Data Creation with Weak Su.pdf; C:\Users\frank\Zotero\storage\EV9F69KH\1711.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2GXT7DQV","preprint","2020","Ma, Xingjun; Huang, Hanxun; Wang, Yisen; Romano, Simone; Erfani, Sarah; Bailey, James","Normalized Loss Functions for Deep Learning with Noisy Labels","","","","10.48550/arXiv.2006.13554","http://arxiv.org/abs/2006.13554","Robust loss functions are essential for training accurate deep neural networks (DNNs) in the presence of noisy (incorrect) labels. It has been shown that the commonly used Cross Entropy (CE) loss is not robust to noisy labels. Whilst new loss functions have been designed, they are only partially robust. In this paper, we theoretically show by applying a simple normalization that: any loss can be made robust to noisy labels. However, in practice, simply being robust is not sufficient for a loss function to train accurate DNNs. By investigating several robust loss functions, we find that they suffer from a problem of underfitting. To address this, we propose a framework to build robust loss functions called Active Passive Loss (APL). APL combines two robust loss functions that mutually boost each other. Experiments on benchmark datasets demonstrate that the family of new loss functions created by our APL framework can consistently outperform state-of-the-art methods by large margins, especially under large noise rates such as 60% or 80% incorrect labels.","2020-06-24","2024-08-29 02:49:23","2024-08-29 02:49:23","2023-01-23 13:03:01","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2006.13554 arXiv:2006.13554 [cs, stat]","","C:\Users\frank\Zotero\storage\JKSL9X92\Ma et al. - 2020 - Normalized Loss Functions for Deep Learning with N.pdf; C:\Users\frank\Zotero\storage\L4WAVR63\2006.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2006.13554","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RBBRBHCP","preprint","2019","Wang, Yisen; Ma, Xingjun; Chen, Zaiyi; Luo, Yuan; Yi, Jinfeng; Bailey, James","Symmetric Cross Entropy for Robust Learning with Noisy Labels","","","","10.48550/arXiv.1908.06112","http://arxiv.org/abs/1908.06112","Training accurate deep neural networks (DNNs) in the presence of noisy labels is an important and challenging task. Though a number of approaches have been proposed for learning with noisy labels, many open issues remain. In this paper, we show that DNN learning with Cross Entropy (CE) exhibits overfitting to noisy labels on some classes (""easy"" classes), but more surprisingly, it also suffers from significant under learning on some other classes (""hard"" classes). Intuitively, CE requires an extra term to facilitate learning of hard classes, and more importantly, this term should be noise tolerant, so as to avoid overfitting to noisy labels. Inspired by the symmetric KL-divergence, we propose the approach of \textbf{Symmetric cross entropy Learning} (SL), boosting CE symmetrically with a noise robust counterpart Reverse Cross Entropy (RCE). Our proposed SL approach simultaneously addresses both the under learning and overfitting problem of CE in the presence of noisy labels. We provide a theoretical analysis of SL and also empirically show, on a range of benchmark and real-world datasets, that SL outperforms state-of-the-art methods. We also show that SL can be easily incorporated into existing methods in order to further enhance their performance.","2019-08-16","2024-08-29 02:49:25","2024-08-29 02:49:25","2023-01-23 13:04:49","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:1908.06112 arXiv:1908.06112 [cs, stat]","","C:\Users\frank\Zotero\storage\XTPKJQUJ\Wang et al. - 2019 - Symmetric Cross Entropy for Robust Learning with N.pdf; C:\Users\frank\Zotero\storage\DTAMVWNW\1908.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:1908.06112","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EIZG72RT","conferencePaper","2021","Karamanolakis, Giannis; Mukherjee, Subhabrata (Subho); Zheng, Guoqing; Awadallah, Ahmed H.","Self-training with Weak Supervision","NAACL 2021","","","","https://www.microsoft.com/en-us/research/publication/self-training-weak-supervision-astra/","State-of-the-art deep neural networks require large-scale labeled training data that is often expensive to obtain or not available for many tasks. Weak supervision in the form of domainspecific rules has been shown to be useful in such settings to automatically generate weakly labeled training data. However, learning with weak rules is challenging due to their inherent heuristic and noisy nature. An additional challenge is rule coverage and overlap, where prior work on weak supervision only considers instances that are covered by weak rules, thus leaving valuable unlabeled data behind. In this work, we develop a weak supervision framework (ASTRA) that leverages all the available data for a given task. To this end, we leverage task-specific unlabeled data through self-training with a model (student) that considers contextualized representations and predicts pseudo-labels for instances that may not be covered by weak rules. We further develop a rule attention network (teacher) that learns how to aggregate student pseudo-labels with weak rule labels, conditioned on their fidelity and the underlying context of an instance. Finally, we construct a semi-supervised learning objective for end-to-end training with unlabeled data, domain-specific rules, and a small amount of labeled data. Extensive experiments on six benchmark datasets for text classification demonstrate the effectiveness of our approach with significant improvements over state-of-the-art baselines.","2021-05","2024-08-29 02:49:27","2024-08-29 02:49:27","","","","","","","","","","","","","NAACL 2021","","","","","","","","","","","C:\Users\frank\Zotero\storage\H2758H46\Karamanolakis et al. - 2021 - Self-training with Weak Supervision.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MNSZCVY8","thesis","","Northcutt, Curtis George","Confident Learning for Machines and Humans","","","","","","The coupling of machine intelligence and human intelligence has the potential to empower humans with augmented capabilities (e.g., improving rhyme-density while writing song lyrics, enhancing empathy via emotion detection, and personalizing learning in online courses). Unfortunately, humans operate in an uncertain world – where the performance of even the most sophisticated model-centric artificially intelligent system often depends on its data-centric ability to deal with the uncertainty in the labels upon which it is trained.","","2024-08-29 02:49:29","2024-08-29 02:49:29","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\frank\Zotero\storage\SQWWLWZC\Northcutt - Confident Learning for Machines and Humans.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NTU9DKG8","preprint","2022","Goh, Hui Wen; Tkachenko, Ulyana; Mueller, Jonas","Utilizing supervised models to infer consensus labels and their quality from data with multiple annotators","","","","10.48550/arXiv.2210.06812","http://arxiv.org/abs/2210.06812","Real-world data for classification is often labeled by multiple annotators. For analyzing such data, we introduce CROWDLAB, a straightforward approach to estimate: (1) A consensus label for each example that aggregates the individual annotations (more accurately than aggregation via majority-vote or other algorithms used in crowdsourcing); (2) A confidence score for how likely each consensus label is correct (via well-calibrated estimates that account for the number of annotations for each example and their agreement, prediction-confidence from a trained classifier, and trustworthiness of each annotator vs. the classifier); (3) A rating for each annotator quantifying the overall correctness of their labels. While many algorithms have been proposed to estimate related quantities in crowdsourcing, these often rely on sophisticated generative models with iterative inference schemes, whereas CROWDLAB is based on simple weighted ensembling. Many algorithms also rely solely on annotator statistics, ignoring the features of the examples from which the annotations derive. CROWDLAB in contrast utilizes any classifier model trained on these features, which can generalize between examples with similar features. In evaluations on real-world multi-annotator image data, our proposed method provides superior estimates for (1)-(3) than many alternative algorithms.","2022-10-13","2024-08-29 02:49:34","2024-08-29 02:49:34","2023-01-25 17:53:14","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2210.06812 arXiv:2210.06812 [cs, stat]","","C:\Users\frank\Zotero\storage\FCPSHBME\Goh et al. - 2022 - Utilizing supervised models to infer consensus lab.pdf; C:\Users\frank\Zotero\storage\3RFPI3A2\2210.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Human-Computer Interaction","","","","","","","","","","","","","","","","","","","arXiv:2210.06812","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WW2TDG6J","webpage","2023","","Class Imbalance, Outliers, and Distribution Shift","Introduction to Data-Centric AI","","","","https://dcai.csail.mit.edu/lectures/imbalance-outliers-shift/","","2023-01-26","2024-08-29 02:49:36","2024-08-29 02:49:36","2023-01-26 18:17:44","","","","","","","","","","","","","","en","","","","","","","","","","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ED3PAS9Y","preprint","2021","Coleman, Cody; Chou, Edward; Katz-Samuels, Julian; Culatana, Sean; Bailis, Peter; Berg, Alexander C.; Nowak, Robert; Sumbaly, Roshan; Zaharia, Matei; Yalniz, I. Zeki","Similarity Search for Efficient Active Learning and Search of Rare Concepts","","","","10.48550/arXiv.2007.00077","http://arxiv.org/abs/2007.00077","Many active learning and search approaches are intractable for large-scale industrial settings with billions of unlabeled examples. Existing approaches search globally for the optimal examples to label, scaling linearly or even quadratically with the unlabeled data. In this paper, we improve the computational efficiency of active learning and search methods by restricting the candidate pool for labeling to the nearest neighbors of the currently labeled set instead of scanning over all of the unlabeled data. We evaluate several selection strategies in this setting on three large-scale computer vision datasets: ImageNet, OpenImages, and a de-identified and aggregated dataset of 10 billion images provided by a large internet company. Our approach achieved similar mean average precision and recall as the traditional global approach while reducing the computational cost of selection by up to three orders of magnitude, thus enabling web-scale active learning.","2021-07-22","2024-08-29 02:49:36","2024-08-29 02:49:36","2023-01-26 19:37:54","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2007.00077 arXiv:2007.00077 [cs, stat]","","C:\Users\frank\Zotero\storage\6CSB9WPI\Coleman et al. - 2021 - Similarity Search for Efficient Active Learning an.pdf; C:\Users\frank\Zotero\storage\3VGRE2MF\2007.html","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2007.00077","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2UYE8P4G","preprint","2021","Renggli, Cedric; Rimanic, Luka; Gürel, Nezihe Merve; Karlaš, Bojan; Wu, Wentao; Zhang, Ce","A Data Quality-Driven View of MLOps","","","","10.48550/arXiv.2102.07750","http://arxiv.org/abs/2102.07750","Developing machine learning models can be seen as a process similar to the one established for traditional software development. A key difference between the two lies in the strong dependency between the quality of a machine learning model and the quality of the data used to train or perform evaluations. In this work, we demonstrate how different aspects of data quality propagate through various stages of machine learning development. By performing a joint analysis of the impact of well-known data quality dimensions and the downstream machine learning process, we show that different components of a typical MLOps pipeline can be efficiently designed, providing both a technical and theoretical perspective.","2021-02-15","2024-08-29 02:49:39","2024-08-29 02:49:39","2023-01-26 20:02:44","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2102.07750 arXiv:2102.07750 [cs]","","C:\Users\frank\Zotero\storage\AI25MN77\Renggli et al. - 2021 - A Data Quality-Driven View of MLOps.pdf; C:\Users\frank\Zotero\storage\TEF8UVJQ\2102.html","","notion","Computer Science - Machine Learning; Computer Science - Databases","","","","","","","","","","","","","","","","","","","arXiv:2102.07750","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WGNVBPDV","conferencePaper","2022","Patel, Hima; Guttula, Shanmukha; Mittal, Ruhi Sharma; Manwani, Naresh; Berti-Equille, Laure; Manatkar, Abhijit","Advances in Exploratory Data Analysis, Visualisation and Quality for Data Centric AI Systems","Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining","978-1-4503-9385-0","","10.1145/3534678.3542604","https://doi.org/10.1145/3534678.3542604","It is widely accepted that data preparation is one of the most time-consuming steps of the machine learning (ML) lifecycle. It is also one of the most important steps, as the quality of data directly influences the quality of a model. In this tutorial, we will discuss the importance and the role of exploratory data analysis (EDA) and data visualisation techniques to find data quality issues and for data preparation, relevant to building ML pipelines. We will also discuss the latest advances in these fields and bring out areas that need innovation. To make the tutorial actionable for practitioners, we will also discuss the most popular open-source packages that one can get started with along with their strengths and weaknesses. Finally, we will discuss on the challenges posed by industry workloads and the gaps to be addressed to make data-centric AI real in industry settings.","2022-08-14","2024-08-29 02:49:40","2024-08-29 02:49:40","2023-01-26","4814–4815","","","","","","","KDD '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","","","notion","data-centric ai; exploratory data analysis; large scale analysis; machine learning; visualization techniques","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TMVYZ67W","book","","Monarch, Robert; Manning, Christopher D.","Human-in-the-Loop Machine Learning: Active Learning and Annotation for Human-Centered AI","","978-1-61729-674-1","","","","","","2024-08-29 02:49:40","2024-08-29 02:49:40","","","398","","","","","Human-in-the-Loop Machine Learning","","","","","","Sherlter Island, NY","en","","","","","K10plus ISBN","","","","C:\Users\frank\Zotero\storage\HSM3325K\Monarch e Manning - Human-in-the-Loop Machine Learning Active Learnin.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2PS3BWL8","journalArticle","","Chen, Mayee; Sala, Frederic; Re, Chris","Lecture Notes on Weak Supervision","","","","","","","","2024-08-29 02:49:45","2024-08-29 02:49:45","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\frank\Zotero\storage\4UVSE89K\Chen et al. - Lecture Notes on Weak Supervision.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H486FBBX","preprint","2022","Narayan, Avanika; Chami, Ines; Orr, Laurel; Arora, Simran; Ré, Christopher","Can Foundation Models Wrangle Your Data?","","","","","http://arxiv.org/abs/2205.09911","Foundation Models (FMs) are models trained on large corpora of data that, at very large scale, can generalize to new tasks without any task-specific finetuning. As these models continue to grow in size, innovations continue to push the boundaries of what these models can do on language and image tasks. This paper aims to understand an underexplored area of FMs: classical data tasks like cleaning and integration. As a proof-of-concept, we cast five data cleaning and integration tasks as prompting tasks and evaluate the performance of FMs on these tasks. We find that large FMs generalize and achieve SoTA performance on data cleaning and integration tasks, even though they are not trained for these data tasks. We identify specific research challenges and opportunities that these models present, including challenges with private and domain specific data, and opportunities to make data management systems more accessible to non-experts. We make our code and experiments publicly available at: https://github.com/HazyResearch/fm_data_tasks.","2022-12-24","2024-08-29 02:49:47","2024-08-29 02:49:47","2023-01-29 18:32:08","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2205.09911 arXiv:2205.09911 [cs]","","C:\Users\frank\Zotero\storage\QFZBFBS7\Narayan et al. - 2022 - Can Foundation Models Wrangle Your Data.pdf; C:\Users\frank\Zotero\storage\3U2D5JCD\2205.html","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Databases","","","","","","","","","","","","","","","","","","","arXiv:2205.09911","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LPFICFMS","preprint","2022","Lang, Hunter; Vijayaraghavan, Aravindan; Sontag, David","Training Subset Selection for Weak Supervision","","","","","http://arxiv.org/abs/2206.02914","Existing weak supervision approaches use all the data covered by weak signals to train a classifier. We show both theoretically and empirically that this is not always optimal. Intuitively, there is a tradeoff between the amount of weakly-labeled data and the precision of the weak labels. We explore this tradeoff by combining pretrained data representations with the cut statistic (Muhlenbach et al., 2004) to select (hopefully) high-quality subsets of the weakly-labeled training data. Subset selection applies to any label model and classifier and is very simple to plug in to existing weak supervision pipelines, requiring just a few lines of code. We show our subset selection method improves the performance of weak supervision for a wide range of label models, classifiers, and datasets. Using less weakly-labeled data improves the accuracy of weak supervision pipelines by up to 19% (absolute) on benchmark tasks.","2022-06-06","2024-08-29 02:49:49","2024-08-29 02:49:49","2023-01-30 18:09:03","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2206.02914 arXiv:2206.02914 [cs, stat]","","C:\Users\frank\Zotero\storage\CTUA94UI\Lang et al. - 2022 - Training Subset Selection for Weak Supervision.pdf; C:\Users\frank\Zotero\storage\AZP5XESA\2206.html","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2206.02914","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WBE9X79K","preprint","2022","Vishwakarma, Harit; Roberts, Nicholas; Sala, Frederic","Lifting Weak Supervision To Structured Prediction","","","","","http://arxiv.org/abs/2211.13375","Weak supervision (WS) is a rich set of techniques that produce pseudolabels by aggregating easily obtained but potentially noisy label estimates from a variety of sources. WS is theoretically well understood for binary classification, where simple approaches enable consistent estimation of pseudolabel noise rates. Using this result, it has been shown that downstream models trained on the pseudolabels have generalization guarantees nearly identical to those trained on clean labels. While this is exciting, users often wish to use WS for structured prediction, where the output space consists of more than a binary or multi-class label set: e.g. rankings, graphs, manifolds, and more. Do the favorable theoretical properties of WS for binary classification lift to this setting? We answer this question in the affirmative for a wide range of scenarios. For labels taking values in a finite metric space, we introduce techniques new to weak supervision based on pseudo-Euclidean embeddings and tensor decompositions, providing a nearly-consistent noise rate estimator. For labels in constant-curvature Riemannian manifolds, we introduce new invariants that also yield consistent noise rate estimation. In both cases, when using the resulting pseudolabels in concert with a flexible downstream model, we obtain generalization guarantees nearly identical to those for models trained on clean data. Several of our results, which can be viewed as robustness guarantees in structured prediction with noisy labels, may be of independent interest. Empirical evaluation validates our claims and shows the merits of the proposed method.","2022-11-23","2024-08-29 02:49:53","2024-08-29 02:49:53","2023-02-09 14:35:49","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2211.13375 arXiv:2211.13375 [cs, stat]","","C:\Users\frank\Zotero\storage\WJ57XK8C\Vishwakarma et al. - 2022 - Lifting Weak Supervision To Structured Prediction.pdf; C:\Users\frank\Zotero\storage\USABJPUW\2211.html","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2211.13375","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CU7B75YS","preprint","2022","Shin, Changho; Li, Winfred; Vishwakarma, Harit; Roberts, Nicholas; Sala, Frederic","Universalizing Weak Supervision","","","","","http://arxiv.org/abs/2112.03865","Weak supervision (WS) frameworks are a popular way to bypass hand-labeling large datasets for training data-hungry models. These approaches synthesize multiple noisy but cheaply-acquired estimates of labels into a set of high-quality pseudolabels for downstream training. However, the synthesis technique is specific to a particular kind of label, such as binary labels or sequences, and each new label type requires manually designing a new synthesis algorithm. Instead, we propose a universal technique that enables weak supervision over any label type while still offering desirable properties, including practical flexibility, computational efficiency, and theoretical guarantees. We apply this technique to important problems previously not tackled by WS frameworks including learning to rank, regression, and learning in hyperbolic space. Theoretically, our synthesis approach produces a consistent estimators for learning some challenging but important generalizations of the exponential family model. Experimentally, we validate our framework and show improvement over baselines in diverse settings including real-world learning-to-rank and regression problems along with learning on hyperbolic manifolds.","2022-03-17","2024-08-29 02:49:55","2024-08-29 02:49:55","2023-02-09 14:34:42","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2112.03865 arXiv:2112.03865 [cs]","","C:\Users\frank\Zotero\storage\E7XNKMND\Shin et al. - 2022 - Universalizing Weak Supervision.pdf; C:\Users\frank\Zotero\storage\5V75A9DJ\2112.html","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2112.03865","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N2UAW9BW","preprint","2022","Vishwakarma, Harit; Lin, Heguang; Sala, Frederic; Vinayak, Ramya Korlakai","Good Data from Bad Models : Foundations of Threshold-based Auto-labeling","","","","","http://arxiv.org/abs/2211.12620","Creating large-scale high-quality labeled datasets is a major bottleneck in supervised machine learning workflows. Auto-labeling systems are a promising way to reduce reliance on manual labeling for dataset construction. Threshold-based auto-labeling, where validation data obtained from humans is used to find a threshold for confidence above which the data is machine-labeled, is emerging as a popular solution used widely in practice. Given the long shelf-life and diverse usage of the resulting datasets, understanding when the data obtained by such auto-labeling systems can be relied on is crucial. In this work, we analyze threshold-based auto-labeling systems and derive sample complexity bounds on the amount of human-labeled validation data required for guaranteeing the quality of machine-labeled data. Our results provide two insights. First, reasonable chunks of the unlabeled data can be automatically and accurately labeled by seemingly bad models. Second, a hidden downside of threshold-based auto-labeling systems is potentially prohibitive validation data usage. Together, these insights describe the promise and pitfalls of using such systems. We validate our theoretical guarantees with simulations and study the efficacy of threshold-based auto-labeling on real datasets.","2022-11-22","2024-08-29 02:49:59","2024-08-29 02:49:59","2023-02-09 14:33:43","","","","","","","Good Data from Bad Models","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2211.12620 arXiv:2211.12620 [cs, stat]","","C:\Users\frank\Zotero\storage\E56NGGEV\Vishwakarma et al. - 2022 - Good Data from Bad Models  Foundations of Thresho.pdf; C:\Users\frank\Zotero\storage\QHEUUMDY\2211.html","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2211.12620","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P247PN5H","conferencePaper","2022","Kwon, Yongchan; Zou, James","Beta Shapley: a Unified and Noise-reduced Data Valuation Framework for Machine Learning","Proceedings of The 25th International Conference on Artificial Intelligence and Statistics","","","","https://proceedings.mlr.press/v151/kwon22a.html","Data Shapley has recently been proposed as a principled framework to quantify the contribution of individual datum in machine learning. It can effectively identify helpful or harmful data points for a learning algorithm. In this paper, we propose Beta Shapley, which is a substantial generalization of Data Shapley. Beta Shapley arises naturally by relaxing the efficiency axiom of the Shapley value, which is not critical for machine learning settings. Beta Shapley unifies several popular data valuation methods and includes data Shapley as a special case. Moreover, we prove that Beta Shapley has several desirable statistical properties and propose efficient algorithms to estimate it. We demonstrate that Beta Shapley outperforms state-of-the-art data valuation methods on several downstream ML tasks such as: 1) detecting mislabeled training data; 2) learning with subsamples; and 3) identifying points whose addition or removal have the largest positive or negative impact on the model.","2022-05-03","2024-08-29 02:50:01","2024-08-29 02:50:01","2023-02-12 13:12:00","8780-8802","","","","","","Beta Shapley","","","","","PMLR","","en","","","","","proceedings.mlr.press","","ISSN: 2640-3498","","C:\Users\frank\Zotero\storage\9BITXPCR\Kwon e Zou - 2022 - Beta Shapley a Unified and Noise-reduced Data Val.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Artificial Intelligence and Statistics","","","","","","","","","","","","","","",""
"YCCPZ8WW","preprint","2022","Klie, Jan-Christoph; Webber, Bonnie; Gurevych, Iryna","Annotation Error Detection: Analyzing the Past and Present for a More Coherent Future","","","","10.48550/arXiv.2206.02280","http://arxiv.org/abs/2206.02280","Annotated data is an essential ingredient in natural language processing for training and evaluating machine learning models. It is therefore very desirable for the annotations to be of high quality. Recent work, however, has shown that several popular datasets contain a surprising amount of annotation errors or inconsistencies. To alleviate this issue, many methods for annotation error detection have been devised over the years. While researchers show that their approaches work well on their newly introduced datasets, they rarely compare their methods to previous work or on the same datasets. This raises strong concerns on methods' general performance and makes it difficult to asses their strengths and weaknesses. We therefore reimplement 18 methods for detecting potential annotation errors and evaluate them on 9 English datasets for text classification as well as token and span labeling. In addition, we define a uniform evaluation setup including a new formalization of the annotation error detection task, evaluation protocol and general best practices. To facilitate future research and reproducibility, we release our datasets and implementations in an easy-to-use and open source software package.","2022-09-25","2024-08-29 02:50:03","2024-08-29 02:50:03","2023-02-12 13:21:45","","","","","","","Annotation Error Detection","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2206.02280 arXiv:2206.02280 [cs]","","C:\Users\frank\Zotero\storage\VPZPUDY6\Klie et al. - 2022 - Annotation Error Detection Analyzing the Past and.pdf; C:\Users\frank\Zotero\storage\ZTCTQ2AE\2206.html","","notion","Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2206.02280","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BIASX3JC","journalArticle","1988","Angluin, Dana; Laird, Philip","Learning from noisy examples","Machine Learning","","0885-6125, 1573-0565","10.1007/BF00116829","http://link.springer.com/10.1007/BF00116829","The basic question addressed in this paper is: how can a learning algorithm cope with incorrect training examples? Specifically, how can algorithms that produce an ""approximately correct"" identification with ""high probability"" for reliable data be adapted to handle noisy data? We show that when the teacher may make independent random errors in classifying the example data, the strategy of selecting the most consistent rule for the sample is sufficient, and usually requires a feasibly small number of examples, provided noise affects less than half the examples on average. In this setting we are able to estimate the rate of noise using only the knowledge that the rate is less than one half. The basic ideas extend to other types of random noise as well. We also show that the search problem associated with this strategy is intractable in general. However, for particular classes of rules the target rule may be efficiently identified if we use techniques specific to that class. For an important class of formulas - the k-CNF formulas studied by Valiant - we present a polynomial-time algorithm that identifies concepts in this form when the rate of classification errors is less than one half.","1988-04","2024-08-29 02:50:06","2024-08-29 02:50:06","2023-02-20 20:38:48","343-370","","4","2","","Mach Learn","","","","","","","","en","","","","","DOI.org (Crossref)","","Number: 4","","C:\Users\frank\Zotero\storage\WQ9NUN9G\Angluin e Laird - 1988 - Learning from noisy examples.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AXZ74RP5","conferencePaper","2019","Ménard, Pierre André; Mougeot, Antoine","Turning silver into gold: error-focused corpus reannotation with active learning","Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)","","","10.26615/978-954-452-056-4_088","https://aclanthology.org/R19-1088","While high quality gold standard annotated corpora are crucial for most tasks in natural language processing, many annotated corpora published in recent years, created by annotators or tools, contains noisy annotations. These corpora can be viewed as more silver than gold standards, even if they are used in evaluation campaigns or to compare systems' performances. As upgrading a silver corpus to gold level is still a challenge, we explore the application of active learning techniques to detect errors using four datasets designed for document classification and part-of-speech tagging. Our results show that the proposed method for the seeding step improves the chance of finding incorrect annotations by a factor of 2.73 when compared to random selection, a 14.71% increase from the baseline methods. Our query method provides an increase in the error detection precision on average by a factor of 1.78 against random selection, an increase of 61.82% compared to other query approaches.","2019-09","2024-08-29 02:50:08","2024-08-29 02:50:08","2023-02-21 20:35:22","758–767","","","","","","Turning silver into gold","","","","","INCOMA Ltd.","Varna, Bulgaria","","","","","","ACLWeb","","","","C:\Users\frank\Zotero\storage\ERD2E45N\Ménard e Mougeot - 2019 - Turning silver into gold error-focused corpus rea.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","RANLP 2019","","","","","","","","","","","","","","",""
"TBU35FV7","conferencePaper","2014","Rehbein, Ines","POS error detection in automatically annotated corpora","Proceedings of LAW VIII - The 8th Linguistic Annotation Workshop","","","10.3115/v1/W14-4903","https://aclanthology.org/W14-4903","","2014-08","2024-08-29 02:50:09","2024-08-29 02:50:09","2023-02-21 20:35:56","20–28","","","","","","","","","","","Association for Computational Linguistics and Dublin City University","Dublin, Ireland","","","","","","ACLWeb","","","","C:\Users\frank\Zotero\storage\2Q4KP395\Rehbein - 2014 - POS error detection in automatically annotated cor.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","LAW 2014","","","","","","","","","","","","","","",""
"RTH8URBK","conferencePaper","2000","van Halteren, Hans","The Detection of Inconsistency in Manually Tagged Text","Proceedings of the COLING-2000 Workshop on Linguistically Interpreted Corpora","","","","https://aclanthology.org/W00-1907","","2000-08","2024-08-29 02:50:09","2024-08-29 02:50:09","2023-02-22 19:13:20","48–55","","","","","","","","","","","International Committee on Computational Linguistics","Centre Universitaire, Luxembourg","","","","","","ACLWeb","","","","C:\Users\frank\Zotero\storage\YEPQANNL\van Halteren - 2000 - The Detection of Inconsistency in Manually Tagged .pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MVDYD5XZ","preprint","2023","Goh, Hui Wen; Tkachenko, Ulyana; Mueller, Jonas","CROWDLAB: Supervised learning to infer consensus labels and quality scores for data with multiple annotators","","","","10.48550/arXiv.2210.06812","http://arxiv.org/abs/2210.06812","Real-world data for classification is often labeled by multiple annotators. For analyzing such data, we introduce CROWDLAB, a straightforward approach to utilize any trained classifier to estimate: (1) A consensus label for each example that aggregates the available annotations; (2) A confidence score for how likely each consensus label is correct; (3) A rating for each annotator quantifying the overall correctness of their labels. Existing algorithms to estimate related quantities in crowdsourcing often rely on sophisticated generative models with iterative inference. CROWDLAB instead uses a straightforward weighted ensemble. Existing algorithms often rely solely on annotator statistics, ignoring the features of the examples from which the annotations derive. CROWDLAB utilizes any classifier model trained on these features, and can thus better generalize between examples with similar features. On real-world multi-annotator image data, our proposed method provides superior estimates for (1)-(3) than existing algorithms like Dawid-Skene/GLAD.","2023-01-27","2024-08-29 02:50:11","2024-08-29 02:50:11","2023-02-23 19:11:28","","","","","","","CROWDLAB","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2210.06812 arXiv:2210.06812 [cs, stat]","","C:\Users\frank\Zotero\storage\K88T6BY2\Goh et al. - 2023 - CROWDLAB Supervised learning to infer consensus l.pdf; C:\Users\frank\Zotero\storage\CKZ99N9E\2210.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Human-Computer Interaction","","","","","","","","","","","","","","","","","","","arXiv:2210.06812","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BEPRBUPN","preprint","2022","Wang, Jiaxi; Wu, Ji; Huang, Lei","Understanding the Failure of Batch Normalization for Transformers in NLP","","","","10.48550/arXiv.2210.05153","http://arxiv.org/abs/2210.05153","Batch Normalization (BN) is a core and prevalent technique in accelerating the training of deep neural networks and improving the generalization on Computer Vision (CV) tasks. However, it fails to defend its position in Natural Language Processing (NLP), which is dominated by Layer Normalization (LN). In this paper, we are trying to answer why BN usually performs worse than LN in NLP tasks with Transformer models. We find that the inconsistency between training and inference of BN is the leading cause that results in the failure of BN in NLP. We define Training Inference Discrepancy (TID) to quantitatively measure this inconsistency and reveal that TID can indicate BN's performance, supported by extensive experiments, including image classification, neural machine translation, language modeling, sequence labeling, and text classification tasks. We find that BN can obtain much better test performance than LN when TID keeps small through training. To suppress the explosion of TID, we propose Regularized BN (RBN) that adds a simple regularization term to narrow the gap between batch statistics and population statistics of BN. RBN improves the performance of BN consistently and outperforms or is on par with LN on 17 out of 20 settings, involving ten datasets and two common variants of Transformer Our code is available at https://github.com/wjxts/RegularizedBN.","2022-10-11","2024-08-29 02:50:13","2024-08-29 02:50:13","2023-02-26 13:30:12","","","","","","","","","","","","","","","","","","","arXiv.org","","Issue: arXiv:2210.05153 arXiv:2210.05153 [cs]","","C:\Users\frank\Zotero\storage\G8X63PNT\Wang et al. - 2022 - Understanding the Failure of Batch Normalization for Transformers in NLP.pdf","","notion","Computer Science - Machine Learning; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2210.05153","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W9DUB2CN","preprint","2022","Roberts, Nicholas; Li, Xintong; Huang, Tzu-Heng; Adila, Dyah; Schoenberg, Spencer; Liu, Cheng-Yu; Pick, Lauren; Ma, Haotian; Albarghouthi, Aws; Sala, Frederic","AutoWS-Bench-101: Benchmarking Automated Weak Supervision with 100 Labels","","","","10.48550/arXiv.2208.14362","http://arxiv.org/abs/2208.14362","Weak supervision (WS) is a powerful method to build labeled datasets for training supervised models in the face of little-to-no labeled data. It replaces hand-labeling data with aggregating multiple noisy-but-cheap label estimates expressed by labeling functions (LFs). While it has been used successfully in many domains, weak supervision's application scope is limited by the difficulty of constructing labeling functions for domains with complex or high-dimensional features. To address this, a handful of methods have proposed automating the LF design process using a small set of ground truth labels. In this work, we introduce AutoWS-Bench-101: a framework for evaluating automated WS (AutoWS) techniques in challenging WS settings -- a set of diverse application domains on which it has been previously difficult or impossible to apply traditional WS techniques. While AutoWS is a promising direction toward expanding the application-scope of WS, the emergence of powerful methods such as zero-shot foundation models reveals the need to understand how AutoWS techniques compare or cooperate with modern zero-shot or few-shot learners. This informs the central question of AutoWS-Bench-101: given an initial set of 100 labels for each task, we ask whether a practitioner should use an AutoWS method to generate additional labels or use some simpler baseline, such as zero-shot predictions from a foundation model or supervised learning. We observe that in many settings, it is necessary for AutoWS methods to incorporate signal from foundation models if they are to outperform simple few-shot baselines, and AutoWS-Bench-101 promotes future research in this direction. We conclude with a thorough ablation study of AutoWS methods.","2022-08-30","2024-08-29 02:50:15","2024-08-29 02:50:15","2023-02-27 11:47:04","","","","","","","AutoWS-Bench-101","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2208.14362 arXiv:2208.14362 [cs, stat]","","C:\Users\frank\Zotero\storage\3629WCF9\Roberts et al. - 2022 - AutoWS-Bench-101 Benchmarking Automated Weak Supe.pdf; C:\Users\frank\Zotero\storage\QXWFQ6QU\2208.html","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2208.14362","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LEV8BIZ6","conferencePaper","2022","Kuang, Zhaobin; Arachie, Chidubem G.; Liang, Bangyong; Narayana, Pradyumna; Desalvo, Giulia; Quinn, Michael S.; Huang, Bert; Downs, Geoffrey; Yang, Yang","Firebolt: Weak Supervision Under Weaker Assumptions","Proceedings of The 25th International Conference on Artificial Intelligence and Statistics","","","","https://proceedings.mlr.press/v151/kuang22a.html","Modern machine learning demands a large amount of training data. Weak supervision is a promising approach to meet this demand. It aggregates multiple labeling functions (LFs)–noisy, user-provided labeling heuristics—to rapidly and cheaply curate probabilistic labels for large-scale unlabeled data. However, standard assumptions in weak supervision—such as user-specified class balance, similar accuracy of an LF in classifying different classes, and full knowledge of LF dependency at inference time—might be undesirable in practice. In response, we present Firebolt, a new weak supervision framework that seeks to operate under weaker assumptions. In particular, Firebolt learns the class balance and class-specific accuracy of LFs jointly from unlabeled data. It carries out inference in an efficient and interpretable manner. We analyze the parameter estimation error of Firebolt and characterize its impact on downstream model performance. Furthermore, we show that on five publicly available datasets, Firebolt outperforms a state-of-the-art weak supervision method by up to 5.8 points in AUC. We also provide a case study in the production setting of a tech company, where a Firebolt-supervised model outperforms the existing weakly-supervised production model by 1.3 points in AUC and speedup label model training and inference from one hour to three minutes.","2022-05-03","2024-08-29 02:50:17","2024-08-29 02:50:17","2023-02-27 11:47:27","8214-8259","","","","","","Firebolt","","","","","PMLR","","en","","","","","proceedings.mlr.press","","ISSN: 2640-3498","","C:\Users\frank\Zotero\storage\SZBMCLZI\Kuang et al. - 2022 - Firebolt Weak Supervision Under Weaker Assumption.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Artificial Intelligence and Statistics","","","","","","","","","","","","","","",""
"2HYA76XH","preprint","2022","Abhishek, Guttu Sai; Ingole, Harshad; Laturia, Parth; Dorna, Vineeth; Maheshwari, Ayush; Iyer, Rishabh; Ramakrishnan, Ganesh","SPEAR : Semi-supervised Data Programming in Python","","","","10.48550/arXiv.2108.00373","http://arxiv.org/abs/2108.00373","We present SPEAR, an open-source python library for data programming with semi supervision. The package implements several recent data programming approaches including facility to programmatically label and build training data. SPEAR facilitates weak supervision in the form of heuristics (or rules) and association of noisy labels to the training dataset. These noisy labels are aggregated to assign labels to the unlabeled data for downstream tasks. We have implemented several label aggregation approaches that aggregate the noisy labels and then train using the noisily labeled set in a cascaded manner. Our implementation also includes other approaches that jointly aggregate and train the model for text classification tasks. Thus, in our python package, we integrate several cascade and joint data-programming approaches while also providing the facility of data programming by letting the user define labeling functions or rules. The code and tutorial notebooks are available at https://github.com/decile-team/spear. Further, extensive documentation can be found at https://spear-decile.readthedocs.io/. Video tutorials demonstrating the usage of our package are available here. We also present some real-world use cases of SPEAR.","2022-10-05","2024-08-29 02:50:21","2024-08-29 02:50:21","2023-02-27 11:49:09","","","","","","","SPEAR","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2108.00373 arXiv:2108.00373 [cs]","","C:\Users\frank\Zotero\storage\TT39US5L\Abhishek et al. - 2022 - SPEAR  Semi-supervised Data Programming in Python.pdf; C:\Users\frank\Zotero\storage\ZS9Q6NER\2108.html","","notion","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2108.00373","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"39CJFHAN","preprint","2022","Zhang, Jieyu; Song, Linxin; Ratner, Alexander","Leveraging Instance Features for Label Aggregation in Programmatic Weak Supervision","","","","10.48550/arXiv.2210.02724","http://arxiv.org/abs/2210.02724","Programmatic Weak Supervision (PWS) has emerged as a widespread paradigm to synthesize training labels efficiently. The core component of PWS is the label model, which infers true labels by aggregating the outputs of multiple noisy supervision sources abstracted as labeling functions (LFs). Existing statistical label models typically rely only on the outputs of LF, ignoring the instance features when modeling the underlying generative process. In this paper, we attempt to incorporate the instance features into a statistical label model via the proposed FABLE. In particular, it is built on a mixture of Bayesian label models, each corresponding to a global pattern of correlation, and the coefficients of the mixture components are predicted by a Gaussian Process classifier based on instance features. We adopt an auxiliary variable-based variational inference algorithm to tackle the non-conjugate issue between the Gaussian Process and Bayesian label models. Extensive empirical comparison on eleven benchmark datasets sees FABLE achieving the highest averaged performance across nine baselines.","2022-10-09","2024-08-29 02:50:22","2024-08-29 02:50:22","2023-02-27 11:49:54","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2210.02724 arXiv:2210.02724 [cs, stat]","","C:\Users\frank\Zotero\storage\IQ2JARKX\Zhang et al. - 2022 - Leveraging Instance Features for Label Aggregation.pdf; C:\Users\frank\Zotero\storage\MJL3RRBL\2210.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning; Statistics - Methodology","","","","","","","","","","","","","","","","","","","arXiv:2210.02724","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P6BTUWS8","preprint","2019","Chen, Pengfei; Liao, Benben; Chen, Guangyong; Zhang, Shengyu","Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels","","","","10.48550/arXiv.1905.05040","http://arxiv.org/abs/1905.05040","Noisy labels are ubiquitous in real-world datasets, which poses a challenge for robustly training deep neural networks (DNNs) as DNNs usually have the high capacity to memorize the noisy labels. In this paper, we find that the test accuracy can be quantitatively characterized in terms of the noise ratio in datasets. In particular, the test accuracy is a quadratic function of the noise ratio in the case of symmetric noise, which explains the experimental findings previously published. Based on our analysis, we apply cross-validation to randomly split noisy datasets, which identifies most samples that have correct labels. Then we adopt the Co-teaching strategy which takes full advantage of the identified samples to train DNNs robustly against noisy labels. Compared with extensive state-of-the-art methods, our strategy consistently improves the generalization performance of DNNs under both synthetic and real-world training noise.","2019-05-13","2024-08-29 02:50:23","2024-08-29 02:50:23","2023-03-01 01:08:10","","","","","","","","","","","","","","","","","","","arXiv.org","","Issue: arXiv:1905.05040 arXiv:1905.05040 [cs, stat]","","C:\Users\frank\Zotero\storage\H6D4PYZ9\Chen et al. - 2019 - Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels.pdf","","notion","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1905.05040","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"38TF88DE","preprint","2023","Goh, Hui Wen; Mueller, Jonas","ActiveLab: Active Learning with Re-Labeling by Multiple Annotators","","","","10.48550/arXiv.2301.11856","http://arxiv.org/abs/2301.11856","In real-world data labeling applications, annotators often provide imperfect labels. It is thus common to employ multiple annotators to label data with some overlap between their examples. We study active learning in such settings, aiming to train an accurate classifier by collecting a dataset with the fewest total annotations. Here we propose ActiveLab, a practical method to decide what to label next that works with any classifier model and can be used in pool-based batch active learning with one or multiple annotators. ActiveLab automatically estimates when it is more informative to re-label examples vs. labeling entirely new ones. This is a key aspect of producing high quality labels and trained models within a limited annotation budget. In experiments on image and tabular data, ActiveLab reliably trains more accurate classifiers with far fewer annotations than a wide variety of popular active learning methods.","2023-01-27","2024-08-29 02:50:25","2024-08-29 02:50:25","2023-03-02 17:50:55","","","","","","","ActiveLab","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2301.11856 arXiv:2301.11856 [cs, stat]","","C:\Users\frank\Zotero\storage\LHXSJNNG\Goh e Mueller - 2023 - ActiveLab Active Learning with Re-Labeling by Mul.pdf; C:\Users\frank\Zotero\storage\X89M6B3G\2301.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2301.11856","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"37KKX8V4","preprint","2022","Zhan, Xueying; Wang, Qingzhong; Huang, Kuan-hao; Xiong, Haoyi; Dou, Dejing; Chan, Antoni B.","A Comparative Survey of Deep Active Learning","","","","10.48550/arXiv.2203.13450","http://arxiv.org/abs/2203.13450","While deep learning (DL) is data-hungry and usually relies on extensive labeled data to deliver good performance, Active Learning (AL) reduces labeling costs by selecting a small proportion of samples from unlabeled data for labeling and training. Therefore, Deep Active Learning (DAL) has risen as a feasible solution for maximizing model performance under a limited labeling cost/budget in recent years. Although abundant methods of DAL have been developed and various literature reviews conducted, the performance evaluation of DAL methods under fair comparison settings is not yet available. Our work intends to fill this gap. In this work, We construct a DAL toolkit, DeepAL+, by re-implementing 19 highly-cited DAL methods. We survey and categorize DAL-related works and construct comparative experiments across frequently used datasets and DAL algorithms. Additionally, we explore some factors (e.g., batch size, number of epochs in the training process) that influence the efficacy of DAL, which provides better references for researchers to design their DAL experiments or carry out DAL-related applications.","2022-07-19","2024-08-29 02:50:27","2024-08-29 02:50:27","2023-03-04 19:18:55","","","","","","","","","","","","","","","","","","","arXiv.org","","Issue: arXiv:2203.13450 arXiv:2203.13450 [cs]","","C:\Users\frank\Zotero\storage\G5K9N3WL\Zhan et al. - 2022 - A Comparative Survey of Deep Active Learning.pdf","","notion","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2203.13450","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XDZCA7GV","preprint","2018","Ratner, Alexander; Hancock, Braden; Dunnmon, Jared; Sala, Frederic; Pandey, Shreyash; Ré, Christopher","Training Complex Models with Multi-Task Weak Supervision","","","","","http://arxiv.org/abs/1810.02840","As machine learning models continue to increase in complexity, collecting large hand-labeled training sets has become one of the biggest roadblocks in practice. Instead, weaker forms of supervision that provide noisier but cheaper labels are often used. However, these weak supervision sources have diverse and unknown accuracies, may output correlated labels, and may label different tasks or apply at different levels of granularity. We propose a framework for integrating and modeling such weak supervision sources by viewing them as labeling different related sub-tasks of a problem, which we refer to as the multi-task weak supervision setting. We show that by solving a matrix completion-style problem, we can recover the accuracies of these multi-task sources given their dependency structure, but without any labeled data, leading to higher-quality supervision for training an end model. Theoretically, we show that the generalization error of models trained with this approach improves with the number of unlabeled data points, and characterize the scaling with respect to the task and dependency structures. On three fine-grained classification problems, we show that our approach leads to average gains of 20.2 points in accuracy over a traditional supervised approach, 6.8 points over a majority vote baseline, and 4.1 points over a previously proposed weak supervision method that models tasks separately.","2018-12-07","2024-08-29 02:50:30","2024-08-29 02:50:30","2023-03-05 20:06:37","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:1810.02840 arXiv:1810.02840 [cs, stat]","","C:\Users\frank\Zotero\storage\UQPZX7KP\Ratner et al. - 2018 - Training Complex Models with Multi-Task Weak Super.pdf; C:\Users\frank\Zotero\storage\TBAKJSGS\1810.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1810.02840","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z6IGENDD","preprint","2022","Maheshwari, Ayush; Killamsetty, Krishnateja; Ramakrishnan, Ganesh; Iyer, Rishabh; Danilevsky, Marina; Popa, Lucian","Learning to Robustly Aggregate Labeling Functions for Semi-supervised Data Programming","","","","10.48550/arXiv.2109.11410","http://arxiv.org/abs/2109.11410","A critical bottleneck in supervised machine learning is the need for large amounts of labeled data which is expensive and time consuming to obtain. However, it has been shown that a small amount of labeled data, while insufficient to re-train a model, can be effectively used to generate human-interpretable labeling functions (LFs). These LFs, in turn, have been used to generate a large amount of additional noisy labeled data, in a paradigm that is now commonly referred to as data programming. However, previous approaches to automatically generate LFs make no attempt to further use the given labeled data for model training, thus giving up opportunities for improved performance. Moreover, since the LFs are generated from a relatively small labeled dataset, they are prone to being noisy, and naively aggregating these LFs can lead to very poor performance in practice. In this work, we propose an LF based reweighting framework \ouralgo{} to solve these two critical limitations. Our algorithm learns a joint model on the (same) labeled dataset used for LF induction along with any unlabeled data in a semi-supervised manner, and more critically, reweighs each LF according to its goodness, influencing its contribution to the semi-supervised loss using a robust bi-level optimization algorithm. We show that our algorithm significantly outperforms prior approaches on several text classification datasets.","2022-03-10","2024-08-29 02:50:32","2024-08-29 02:50:32","2023-03-09 12:32:27","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2109.11410 arXiv:2109.11410 [cs]","","C:\Users\frank\Zotero\storage\V72R9EJS\Maheshwari et al. - 2022 - Learning to Robustly Aggregate Labeling Functions .pdf; C:\Users\frank\Zotero\storage\CTLIMG26\2109.html","","notion","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2109.11410","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ELUP4URQ","journalArticle","2022","Hsieh, Cheng-Yu; Zhang, Jieyu; Ratner, Alexander","Nemo: Guiding and Contextualizing Weak Supervision for Interactive Data Programming","Proceedings of the VLDB Endowment","","2150-8097","10.14778/3565838.3565859","https://dl.acm.org/doi/10.14778/3565838.3565859","Weak Supervision (WS) techniques allow users to e ciently create large training datasets by programmatically labeling data with heuristic sources of supervision. While the success of WS relies heavily on the provided labeling heuristics, the process of how these heuristics are created in practice has remained under-explored. In this work, we formalize the development process of labeling heuristics as an interactive procedure, built around the existing work ow where users draw ideas from a selected set of development data for designing the heuristic sources. With the formalism, shown in Figure 1, we study two core problems of (1) how to strategically select the development data to guide users in e ciently creating informative heuristics, and (2) how to exploit the information within the development process to contextualize and better learn from the resultant heuristics. Building upon two novel methodologies that e ectively tackle the respective problems considered, we present Nemo, an end-to-end interactive system that improves the overall productivity of WS learning pipeline by an average 20% (and up to 47% in one task) compared to the prevailing WS approach.","2022-09","2024-08-29 02:50:34","2024-08-29 02:50:34","2023-03-31 19:43:45","4093-4105","","13","15","","Proc. VLDB Endow.","Nemo","","","","","","","en","","","","","DOI.org (Crossref)","","Number: 13","","C:\Users\frank\Zotero\storage\ZHX74MNF\Hsieh et al. - 2022 - Nemo Guiding and Contextualizing Weak Supervision.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZU66TCFU","preprint","2022","Oyen, Diane; Kucer, Michal; Hengartner, Nick; Singh, Har Simrat","Robustness to Label Noise Depends on the Shape of the Noise Distribution in Feature Space","","","","10.48550/arXiv.2206.01106","http://arxiv.org/abs/2206.01106","Machine learning classifiers have been demonstrated, both empirically and theoretically, to be robust to label noise under certain conditions -- notably the typical assumption is that label noise is independent of the features given the class label. We provide a theoretical framework that generalizes beyond this typical assumption by modeling label noise as a distribution over feature space. We show that both the scale and the shape of the noise distribution influence the posterior likelihood; and the shape of the noise distribution has a stronger impact on classification performance if the noise is concentrated in feature space where the decision boundary can be moved. For the special case of uniform label noise (independent of features and the class label), we show that the Bayes optimal classifier for $c$ classes is robust to label noise until the ratio of noisy samples goes above $\frac{c-1}{c}$ (e.g. 90% for 10 classes), which we call the tipping point. However, for the special case of class-dependent label noise (independent of features given the class label), the tipping point can be as low as 50%. Most importantly, we show that when the noise distribution targets decision boundaries (label noise is directly dependent on feature space), classification robustness can drop off even at a small scale of noise. Even when evaluating recent label-noise mitigation methods we see reduced accuracy when label noise is dependent on features. These findings explain why machine learning often handles label noise well if the noise distribution is uniform in feature-space; yet it also points to the difficulty of overcoming label noise when it is concentrated in a region of feature space where a decision boundary can move.","2022-06-02","2024-08-29 02:50:36","2024-08-29 02:50:36","2023-04-03 10:09:40","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2206.01106 arXiv:2206.01106 [cs]","","C:\Users\frank\Zotero\storage\SBEPMCZL\Oyen et al. - 2022 - Robustness to Label Noise Depends on the Shape of .pdf; C:\Users\frank\Zotero\storage\F8I6FUQP\2206.html","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2206.01106","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IU8J2WNK","preprint","2022","Zhang, Jieyu; Wang, Yujing; Yang, Yaming; Luo, Yang; Ratner, Alexander","Binary Classification with Positive Labeling Sources","","","","10.48550/arXiv.2208.01704","http://arxiv.org/abs/2208.01704","To create a large amount of training labels for machine learning models effectively and efficiently, researchers have turned to Weak Supervision (WS), which uses programmatic labeling sources rather than manual annotation. Existing works of WS for binary classification typically assume the presence of labeling sources that are able to assign both positive and negative labels to data in roughly balanced proportions. However, for many tasks of interest where there is a minority positive class, negative examples could be too diverse for developers to generate indicative labeling sources. Thus, in this work, we study the application of WS on binary classification tasks with positive labeling sources only. We propose WEAPO, a simple yet competitive WS method for producing training labels without negative labeling sources. On 10 benchmark datasets, we show WEAPO achieves the highest averaged performance in terms of both the quality of synthesized labels and the performance of the final classifier supervised with these labels. We incorporated the implementation of \method into WRENCH, an existing benchmarking platform.","2022-08-02","2024-08-29 02:50:38","2024-08-29 02:50:38","2023-04-08 20:05:37","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2208.01704 arXiv:2208.01704 [cs]","","C:\Users\frank\Zotero\storage\3ZEDPB5I\Zhang et al. - 2022 - Binary Classification with Positive Labeling Sourc.pdf; C:\Users\frank\Zotero\storage\PLW9F2AE\2208.html","","notion","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2208.01704","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZLJND4US","preprint","2023","Boecking, Benedikt; Roberts, Nicholas; Neiswanger, Willie; Ermon, Stefano; Sala, Frederic; Dubrawski, Artur","Generative Modeling Helps Weak Supervision (and Vice Versa)","","","","10.48550/arXiv.2203.12023","http://arxiv.org/abs/2203.12023","Many promising applications of supervised machine learning face hurdles in the acquisition of labeled data in sufficient quantity and quality, creating an expensive bottleneck. To overcome such limitations, techniques that do not depend on ground truth labels have been studied, including weak supervision and generative modeling. While these techniques would seem to be usable in concert, improving one another, how to build an interface between them is not well-understood. In this work, we propose a model fusing programmatic weak supervision and generative adversarial networks and provide theoretical justification motivating this fusion. The proposed approach captures discrete latent variables in the data alongside the weak supervision derived label estimate. Alignment of the two allows for better modeling of sample-dependent accuracies of the weak supervision sources, improving the estimate of unobserved labels. It is the first approach to enable data augmentation through weakly supervised synthetic images and pseudolabels. Additionally, its learned latent variables can be inspected qualitatively. The model outperforms baseline weak supervision label models on a number of multiclass image classification datasets, improves the quality of generated images, and further improves end-model performance through data augmentation with synthetic samples.","2023-03-11","2024-08-29 02:50:39","2024-08-29 02:50:39","2023-04-08 20:06:10","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2203.12023 arXiv:2203.12023 [cs, stat]","","C:\Users\frank\Zotero\storage\ICXX6DNH\Boecking et al. - 2023 - Generative Modeling Helps Weak Supervision (and Vi.pdf; C:\Users\frank\Zotero\storage\RRT8T2NA\2203.html","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition; I.2.0; I.4.m","","","","","","","","","","","","","","","","","","","arXiv:2203.12023","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XJJZT6WS","preprint","2023","Wu, Renzhi; Chen, Shen-En; Zhang, Jieyu; Chu, Xu","Learning Hyper Label Model for Programmatic Weak Supervision","","","","10.48550/arXiv.2207.13545","http://arxiv.org/abs/2207.13545","To reduce the human annotation efforts, the programmatic weak supervision (PWS) paradigm abstracts weak supervision sources as labeling functions (LFs) and involves a label model to aggregate the output of multiple LFs to produce training labels. Most existing label models require a parameter learning step for each dataset. In this work, we present a hyper label model that (once learned) infers the ground-truth labels for each dataset in a single forward pass without dataset-specific parameter learning. The hyper label model approximates an optimal analytical (yet computationally intractable) solution of the ground-truth labels. We train the model on synthetic data generated in the way that ensures the model approximates the analytical optimal solution, and build the model upon Graph Neural Network (GNN) to ensure the model prediction being invariant (or equivariant) to the permutation of LFs (or data points). On 14 real-world datasets, our hyper label model outperforms the best existing methods in both accuracy (by 1.4 points on average) and efficiency (by six times on average). Our code is available at https://github.com/wurenzhi/hyper_label_model","2023-03-08","2024-08-29 02:50:42","2024-08-29 02:50:42","2023-04-08 20:07:57","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2207.13545 arXiv:2207.13545 [cs]","","C:\Users\frank\Zotero\storage\PMHFWL6T\Wu et al. - 2023 - Learning Hyper Label Model for Programmatic Weak S.pdf; C:\Users\frank\Zotero\storage\YVUVR45R\2207.html; C:\Users\frank\Zotero\storage\EZTLHJW3\Wu et al. - 2023 - Learning Hyper Label Model for Programmatic Weak Supervision.pdf","","notion","Computer Science - Machine Learning; Computer Science - Databases","","","","","","","","","","","","","","","","","","","arXiv:2207.13545","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B2TYL6IY","preprint","2022","Stephan, Andreas; Kougia, Vasiliki; Roth, Benjamin","SepLL: Separating Latent Class Labels from Weak Supervision Noise","","","","10.48550/arXiv.2210.13898","http://arxiv.org/abs/2210.13898","In the weakly supervised learning paradigm, labeling functions automatically assign heuristic, often noisy, labels to data samples. In this work, we provide a method for learning from weak labels by separating two types of complementary information associated with the labeling functions: information related to the target label and information specific to one labeling function only. Both types of information are reflected to different degrees by all labeled instances. In contrast to previous works that aimed at correcting or removing wrongly labeled instances, we learn a branched deep model that uses all data as-is, but splits the labeling function information in the latent space. Specifically, we propose the end-to-end model SepLL which extends a transformer classifier by introducing a latent space for labeling function specific and task-specific information. The learning signal is only given by the labeling functions matches, no pre-processing or label model is required for our method. Notably, the task prediction is made from the latent layer without any direct task signal. Experiments on Wrench text classification tasks show that our model is competitive with the state-of-the-art, and yields a new best average performance.","2022-10-25","2024-08-29 02:50:45","2024-08-29 02:50:45","2023-04-10 16:38:02","","","","","","","SepLL","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2210.13898 arXiv:2210.13898 [cs]","","C:\Users\frank\Zotero\storage\MV3CEPQG\Stephan et al. - 2022 - SepLL Separating Latent Class Labels from Weak Su.pdf; C:\Users\frank\Zotero\storage\9VQ2BXZ6\2210.html","","notion","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2210.13898","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I85Q7N65","webpage","2023","","On the relative value of weak information of supervision for learning generative models: An empirical study | Elsevier Enhanced Reader","","","","","https://reader.elsevier.com/reader/sd/pii/S0888613X22001244?token=1D7996D01A31ADEE02BD86237F984A1F2117C6C3BF133FB30175D4B4719D99BF9F69E0E233509ACF3243062B7E85E782&originRegion=us-east-1&originCreation=20230415123828","","2023-04-15","2024-08-29 02:50:47","2024-08-29 02:50:47","2023-04-15 12:40:25","","","","","","","On the relative value of weak information of supervision for learning generative models","","","","","","","en","","","","","","","DOI: 10.1016/j.ijar.2022.08.012","","C:\Users\frank\Zotero\storage\EEGN9L2S\On the relative value of weak information of super.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WHYGSBWB","conferencePaper","2022","Yu, Peilin; Ding, Tiffany; Bach, Stephen H.","Learning from Multiple Noisy Partial Labelers","Proceedings of The 25th International Conference on Artificial Intelligence and Statistics","","","","https://proceedings.mlr.press/v151/yu22c.html","Programmatic weak supervision creates models without hand-labeled training data by combining the outputs of heuristic labelers. Existing frameworks make the restrictive assumption that labelers output a single class label. Enabling users to create partial labelers that output subsets of possible class labels would greatly expand the expressivity of programmatic weak supervision. We introduce this capability by defining a probabilistic generative model that can estimate the underlying accuracies of multiple noisy partial labelers without ground truth labels. We show how to scale up learning, for example learning on 100k examples in one minute, a 300××\times speed up compared to a naive implementation. We also prove that this class of models is generically identifiable up to label swapping under mild conditions. We evaluate our framework on three text classification and six object classification tasks. On text tasks, adding partial labels increases average accuracy by 8.6 percentage points. On image tasks, we show that partial labels allow us to approach some zero-shot object classification problems with programmatic weak supervision by using class attributes as partial labelers. On these tasks, our framework has accuracy comparable to recent embedding-based zero-shot learning methods, while using only pre-trained attribute detectors.","2022-05-03","2024-08-29 02:50:49","2024-08-29 02:50:49","2023-04-16 17:20:38","11072-11095","","","","","","","","","","","PMLR","","en","","","","","proceedings.mlr.press","","","","C:\Users\frank\Zotero\storage\MLIJLFQD\Yu et al. - 2022 - Learning from Multiple Noisy Partial Labelers.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Artificial Intelligence and Statistics","","","","","","","","","","","","","","",""
"NCK2V7YF","preprint","2019","Boecking, Benedikt; Dubrawski, Artur","Pairwise Feedback for Data Programming","","","","10.48550/arXiv.1912.07685","http://arxiv.org/abs/1912.07685","The scalability of the labeling process and the attainable quality of labels have become limiting factors for many applications of machine learning. The programmatic creation of labeled datasets via the synthesis of noisy heuristics provides a promising avenue to address this problem. We propose to improve modeling of latent class variables in the programmatic creation of labeled datasets by incorporating pairwise feedback into the process. We discuss the ease with which such pairwise feedback can be obtained or generated in many application domains. Our experiments show that even a small number of sources of pairwise feedback can substantially improve the quality of the posterior estimate of the latent class variable.","2019-12-16","2024-08-29 02:50:51","2024-08-29 02:50:51","2023-04-20 13:00:12","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:1912.07685 arXiv:1912.07685 [cs, stat]","","C:\Users\frank\Zotero\storage\M9ZGHGN5\Boecking e Dubrawski - 2019 - Pairwise Feedback for Data Programming.pdf; C:\Users\frank\Zotero\storage\JM9MFSSI\1912.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1912.07685","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"82N56P3M","preprint","2022","Zhang, Jieyu; Wang, Bohan; Song, Xiangchen; Wang, Yujing; Yang, Yaming; Bai, Jing; Ratner, Alexander","Creating Training Sets via Weak Indirect Supervision","","","","","http://arxiv.org/abs/2110.03484","Creating labeled training sets has become one of the major roadblocks in machine learning. To address this, recent \emph{Weak Supervision (WS)} frameworks synthesize training labels from multiple potentially noisy supervision sources. However, existing frameworks are restricted to supervision sources that share the same output space as the target task. To extend the scope of usable sources, we formulate Weak Indirect Supervision (WIS), a new research problem for automatically synthesizing training labels based on indirect supervision sources that have different output label spaces. To overcome the challenge of mismatched output spaces, we develop a probabilistic modeling approach, PLRM, which uses user-provided label relations to model and leverage indirect supervision sources. Moreover, we provide a theoretically-principled test of the distinguishability of PLRM for unseen labels, along with a generalization bound. On both image and text classification tasks as well as an industrial advertising application, we demonstrate the advantages of PLRM by outperforming baselines by a margin of 2%-9%.","2022-03-14","2024-08-29 02:50:52","2024-08-29 02:50:52","2023-04-20 17:06:33","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2110.03484 arXiv:2110.03484 [cs, stat]","","C:\Users\frank\Zotero\storage\A6EP2YEF\2110.html; C:\Users\frank\Zotero\storage\S3UGPAAG\Zhang et al. - 2022 - Creating Training Sets via Weak Indirect Supervisi.pdf","","notion","Computer Science - Machine Learning; Statistics - Machine Learning; Statistics - Applications","","","","","","","","","","","","","","","","","","","arXiv:2110.03484","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HCF46KYT","preprint","2023","Pukdee, Rattana; Sam, Dylan; Balcan, Maria-Florina; Ravikumar, Pradeep","Label Propagation with Weak Supervision","","","","10.48550/arXiv.2210.03594","http://arxiv.org/abs/2210.03594","Semi-supervised learning and weakly supervised learning are important paradigms that aim to reduce the growing demand for labeled data in current machine learning applications. In this paper, we introduce a novel analysis of the classical label propagation algorithm (LPA) (Zhu & Ghahramani, 2002) that moreover takes advantage of useful prior information, specifically probabilistic hypothesized labels on the unlabeled data. We provide an error bound that exploits both the local geometric properties of the underlying graph and the quality of the prior information. We also propose a framework to incorporate multiple sources of noisy information. In particular, we consider the setting of weak supervision, where our sources of information are weak labelers. We demonstrate the ability of our approach on multiple benchmark weakly supervised classification tasks, showing improvements upon existing semi-supervised and weakly supervised methods.","2023-04-09","2024-08-29 02:50:56","2024-08-29 02:50:56","2023-04-20 17:09:48","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2210.03594 arXiv:2210.03594 [cs, stat]","","C:\Users\frank\Zotero\storage\ETMA3UBC\Pukdee et al. - 2023 - Label Propagation with Weak Supervision.pdf; C:\Users\frank\Zotero\storage\SFPMIJCF\2210.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2210.03594","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2YSLRA6Z","preprint","2022","Chen, Mayee F.; Fu, Daniel Y.; Adila, Dyah; Zhang, Michael; Sala, Frederic; Fatahalian, Kayvon; Ré, Christopher","Shoring Up the Foundations: Fusing Model Embeddings and Weak Supervision","","","","10.48550/arXiv.2203.13270","http://arxiv.org/abs/2203.13270","Foundation models offer an exciting new paradigm for constructing models with out-of-the-box embeddings and a few labeled examples. However, it is not clear how to best apply foundation models without labeled data. A potential approach is to fuse foundation models with weak supervision frameworks, which use weak label sources -- pre-trained models, heuristics, crowd-workers -- to construct pseudolabels. The challenge is building a combination that best exploits the signal available in both foundation models and weak sources. We propose Liger, a combination that uses foundation model embeddings to improve two crucial elements of existing weak supervision techniques. First, we produce finer estimates of weak source quality by partitioning the embedding space and learning per-part source accuracies. Second, we improve source coverage by extending source votes in embedding space. Despite the black-box nature of foundation models, we prove results characterizing how our approach improves performance and show that lift scales with the smoothness of label distributions in embedding space. On six benchmark NLP and video tasks, Liger outperforms vanilla weak supervision by 14.1 points, weakly-supervised kNN and adapters by 11.8 points, and kNN and adapters supervised by traditional hand labels by 7.2 points.","2022-08-01","2024-08-29 02:50:58","2024-08-29 02:50:58","2023-04-24 14:38:52","","","","","","","Shoring Up the Foundations","","","","","","","","","","","","arXiv.org","","Issue: arXiv:2203.13270 arXiv:2203.13270 [cs, stat]","","C:\Users\frank\Zotero\storage\XM8Q4SEF\Chen et al. - 2022 - Shoring Up the Foundations Fusing Model Embeddings and Weak Supervision.pdf","","notion","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2203.13270","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FLH3CAJI","preprint","2022","Kuan, Johnson; Mueller, Jonas","Back to the Basics: Revisiting Out-of-Distribution Detection Baselines","","","","10.48550/arXiv.2207.03061","http://arxiv.org/abs/2207.03061","We study simple methods for out-of-distribution (OOD) image detection that are compatible with any already trained classifier, relying on only its predictions or learned representations. Evaluating the OOD detection performance of various methods when utilized with ResNet-50 and Swin Transformer models, we find methods that solely consider the model's predictions can be easily outperformed by also considering the learned representations. Based on our analysis, we advocate for a dead-simple approach that has been neglected in other studies: simply flag as OOD images whose average distance to their K nearest neighbors is large (in the representation space of an image classifier trained on the in-distribution data).","2022-07-06","2024-08-29 02:51:00","2024-08-29 02:51:00","2023-05-11 17:17:36","","","","","","","Back to the Basics","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2207.03061 arXiv:2207.03061 [cs, stat]","","C:\Users\frank\Zotero\storage\IH3F46GR\Kuan e Mueller - 2022 - Back to the Basics Revisiting Out-of-Distribution.pdf; C:\Users\frank\Zotero\storage\8T54QVE5\2207.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2207.03061","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JAFS2QVH","journalArticle","2022","Bernhardt, Mélanie; Castro, Daniel C.; Tanno, Ryutaro; Schwaighofer, Anton; Tezcan, Kerem C.; Monteiro, Miguel; Bannur, Shruthi; Lungren, Matthew P.; Nori, Aditya; Glocker, Ben; Alvarez-Valle, Javier; Oktay, Ozan","Active label cleaning for improved dataset quality under resource constraints","Nature Communications","","2041-1723","10.1038/s41467-022-28818-3","https://www.nature.com/articles/s41467-022-28818-3","Imperfections in data annotation, known as label noise, are detrimental to the training of machine learning models and have a confounding effect on the assessment of model performance. Nevertheless, employing experts to remove label noise by fully re-annotating large datasets is infeasible in resource-constrained settings, such as healthcare. This work advocates for a data-driven approach to prioritising samples for re-annotation—which we term “active label cleaning"". We propose to rank instances according to estimated label correctness and labelling difficulty of each sample, and introduce a simulation framework to evaluate relabelling efficacy. Our experiments on natural images and on a specifically-devised medical imaging benchmark show that cleaning noisy labels mitigates their negative impact on model training, evaluation, and selection. Crucially, the proposed approach enables correcting labels up to 4 × more effectively than typical random selection in realistic conditions, making better use of experts’ valuable time for improving dataset quality.","2022-03-04","2024-08-29 02:51:01","2024-08-29 02:51:01","2023-05-20 19:58:32","1161","","1","13","","Nat Commun","","","","","","","","en","2022 The Author(s)","","","","www.nature.com","","Number: 1 Publisher: Nature Publishing Group","","C:\Users\frank\Zotero\storage\YXU77SKX\Bernhardt et al. - 2022 - Active label cleaning for improved dataset quality.pdf","","notion","Diagnosis; Machine learning; Radiography","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SFM554TH","thesis","2022","Tekumalla, Ramya","When Silver Is As Good As Gold:  Using Weak Supervision to Train Machine Learning Models on Social Media Data","","","","","https://scholarworks.gsu.edu/cs_diss/187","<p>Over the last decade, advances in machine learning have led to an exponential growth in artificial intelligence i.e., machine learning models capable of learning from vast amounts of data to perform several tasks such as text classification, regression, machine translation, speech recognition, and many others. While massive volumes of data are available, due to the manual curation process involved in the generation of training datasets, only a percentage of the data is used to train machine learning models. The process of labeling data with a ground-truth value is extremely tedious, expensive, and is the major bottleneck of supervised learning. To curtail this, the theory of noisy learning can be employed where data labeled through heuristics, knowledge bases and weak classifiers can be utilized for training, instead of data obtained through manual annotation. The assumption here is that a large volume of training data, which contains noise and acquired through an automated process, can compensate for the lack of manual labels. In this study, we utilize heuristic based approaches to create noisy silver standard datasets. We extensively tested the theory of noisy learning on four different applications by training several machine learning models using the silver standard dataset with several sample sizes and class imbalances and tested the performance using a gold standard dataset. Our evaluations on the four applications indicate the success of silver standard datasets in identifying a gold standard dataset. We conclude the study with evidence that noisy social media data can be utilized for weak supervision</p>","2022","2024-08-29 02:51:03","2024-08-29 02:51:03","2023-06-12 18:34:14","","","","","","","When Silver Is As Good As Gold","","","","","Georgia State University","","en","","","","","DOI.org (Datacite)","","DOI: 10.57709/30434727","","C:\Users\frank\Zotero\storage\XVAI94JK\Tekumalla - 2022 - When Silver Is As Good As Gold  Using Weak Superv.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E6CR2LQI","preprint","2022","Mazumder, Mark; Banbury, Colby; Yao, Xiaozhe; Karlaš, Bojan; Rojas, William Gaviria; Diamos, Sudnya; Diamos, Greg; He, Lynn; Kiela, Douwe; Jurado, David; Kanter, David; Mosquera, Rafael; Ciro, Juan; Aroyo, Lora; Acun, Bilge; Eyuboglu, Sabri; Ghorbani, Amirata; Goodman, Emmett; Kane, Tariq; Kirkpatrick, Christine R.; Kuo, Tzu-Sheng; Mueller, Jonas; Thrush, Tristan; Vanschoren, Joaquin; Warren, Margaret; Williams, Adina; Yeung, Serena; Ardalani, Newsha; Paritosh, Praveen; Zhang, Ce; Zou, James; Wu, Carole-Jean; Coleman, Cody; Ng, Andrew; Mattson, Peter; Reddi, Vijay Janapa","DataPerf: Benchmarks for Data-Centric AI Development","","","","10.48550/arXiv.2207.10062","http://arxiv.org/abs/2207.10062","Machine learning (ML) research has generally focused on models, while the most prominent datasets have been employed for everyday ML tasks without regard for the breadth, difficulty, and faithfulness of these datasets to the underlying problem. Neglecting the fundamental importance of datasets has caused major problems involving data cascades in real-world applications and saturation of dataset-driven criteria for model quality, hindering research growth. To solve this problem, we present DataPerf, a benchmark package for evaluating ML datasets and dataset-working algorithms. We intend it to enable the ""data ratchet,"" in which training sets will aid in evaluating test sets on the same problems, and vice versa. Such a feedback-driven strategy will generate a virtuous loop that will accelerate development of data-centric AI. The MLCommons Association will maintain DataPerf.","2022-07-20","2024-08-29 02:51:07","2024-08-29 02:51:07","2023-06-16 20:33:41","","","","","","","DataPerf","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2207.10062 arXiv:2207.10062 [cs]","","C:\Users\frank\Zotero\storage\FRVBR7RY\Mazumder et al. - 2022 - DataPerf Benchmarks for Data-Centric AI Developme.pdf; C:\Users\frank\Zotero\storage\98DU7II5\2207.html","","notion","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2207.10062","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6LFJHKYH","journalArticle","","Kuan, Johnson; Mueller, Jonas","Model-Agnostic Label Quality Scoring to Detect Real-World Label Errors","","","","","","We consider algorithms to find wrongly labeled data, which lurks in many real-world applications and hampers training/evaluation of ML models. We present the first empirical study of various scoring methods for this task on real datasets with naturally-occurring label errors (as opposed to synthetically introduced label errors). The label quality scores considered here can be utilized with arbitrary classification models. We examine five popular image recognition models (and ensembles thereof) to comprehensively characterize how well different scores detect label errors in practice.","","2024-08-29 02:51:09","2024-08-29 02:51:09","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\frank\Zotero\storage\SLMK5AKK\Kuan e Mueller - Model-Agnostic Label Quality Scoring to Detect Rea.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HSV6BU87","preprint","2017","Northcutt, Curtis G.; Wu, Tailin; Chuang, Isaac L.","Learning with Confident Examples: Rank Pruning for Robust Classification with Noisy Labels","","","","10.48550/arXiv.1705.01936","http://arxiv.org/abs/1705.01936","Noisy PN learning is the problem of binary classification when training examples may be mislabeled (flipped) uniformly with noise rate rho1 for positive examples and rho0 for negative examples. We propose Rank Pruning (RP) to solve noisy PN learning and the open problem of estimating the noise rates, i.e. the fraction of wrong positive and negative labels. Unlike prior solutions, RP is time-efficient and general, requiring O(T) for any unrestricted choice of probabilistic classifier with T fitting time. We prove RP has consistent noise estimation and equivalent expected risk as learning with uncorrupted labels in ideal conditions, and derive closed-form solutions when conditions are non-ideal. RP achieves state-of-the-art noise estimation and F1, error, and AUC-PR for both MNIST and CIFAR datasets, regardless of the amount of noise and performs similarly impressively when a large portion of training examples are noise drawn from a third distribution. To highlight, RP with a CNN classifier can predict if an MNIST digit is a ""one""or ""not"" with only 0.25% error, and 0.46 error across all digits, even when 50% of positive examples are mislabeled and 50% of observed positive labels are mislabeled negative examples.","2017-08-09","2024-08-29 02:51:10","2024-08-29 02:51:10","2023-06-16 20:37:26","","","","","","","Learning with Confident Examples","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:1705.01936 arXiv:1705.01936 [cs, stat]","","C:\Users\frank\Zotero\storage\HSDLE6SD\Northcutt et al. - 2017 - Learning with Confident Examples Rank Pruning for.pdf; C:\Users\frank\Zotero\storage\L3LQY7TV\1705.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1705.01936","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P9A2TMLX","preprint","2022","Song, Hwanjun; Kim, Minseok; Park, Dongmin; Shin, Yooju; Lee, Jae-Gil","Learning from Noisy Labels with Deep Neural Networks: A Survey","","","","10.48550/arXiv.2007.08199","http://arxiv.org/abs/2007.08199","Deep learning has achieved remarkable success in numerous domains with help from large amounts of big data. However, the quality of data labels is a concern because of the lack of high-quality labels in many real-world scenarios. As noisy labels severely degrade the generalization performance of deep neural networks, learning from noisy labels (robust training) is becoming an important task in modern deep learning applications. In this survey, we first describe the problem of learning with label noise from a supervised learning perspective. Next, we provide a comprehensive review of 62 state-of-the-art robust training methods, all of which are categorized into five groups according to their methodological difference, followed by a systematic comparison of six properties used to evaluate their superiority. Subsequently, we perform an in-depth analysis of noise rate estimation and summarize the typically used evaluation methodology, including public noisy datasets and evaluation metrics. Finally, we present several promising research directions that can serve as a guideline for future studies. All the contents will be available at https://github.com/songhwanjun/Awesome-Noisy-Labels.","2022-03-09","2024-08-29 02:51:13","2024-08-29 02:51:13","2023-06-26 10:52:13","","","","","","","Learning from Noisy Labels with Deep Neural Networks","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2007.08199 arXiv:2007.08199 [cs, stat]","","C:\Users\frank\Zotero\storage\FQGUWUAQ\Song et al. - 2022 - Learning from Noisy Labels with Deep Neural Networ.pdf; C:\Users\frank\Zotero\storage\WVNQ42W6\2007.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2007.08199","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YR9IVCRD","preprint","2020","Jiang, Lu; Huang, Di; Liu, Mason; Yang, Weilong","Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels","","","","10.48550/arXiv.1911.09781","http://arxiv.org/abs/1911.09781","Performing controlled experiments on noisy data is essential in understanding deep learning across noise levels. Due to the lack of suitable datasets, previous research has only examined deep learning on controlled synthetic label noise, and real-world label noise has never been studied in a controlled setting. This paper makes three contributions. First, we establish the first benchmark of controlled real-world label noise from the web. This new benchmark enables us to study the web label noise in a controlled setting for the first time. The second contribution is a simple but effective method to overcome both synthetic and real noisy labels. We show that our method achieves the best result on our dataset as well as on two public benchmarks (CIFAR and WebVision). Third, we conduct the largest study by far into understanding deep neural networks trained on noisy labels across different noise levels, noise types, network architectures, and training settings. The data and code are released at the following link: http://www.lujiang.info/cnlw.html","2020-08-27","2024-08-29 02:51:15","2024-08-29 02:51:15","2023-06-26 11:00:54","","","","","","","Beyond Synthetic Noise","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:1911.09781 arXiv:1911.09781 [cs, stat]","","C:\Users\frank\Zotero\storage\WJBJEPTA\Jiang et al. - 2020 - Beyond Synthetic Noise Deep Learning on Controlle.pdf; C:\Users\frank\Zotero\storage\3B7QQ5TD\1911.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:1911.09781","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6PWSV8QE","preprint","2023","Yu, Peilin; Bach, Stephen H.","Alfred: A System for Prompted Weak Supervision","","","","10.48550/arXiv.2305.18623","http://arxiv.org/abs/2305.18623","Alfred is the first system for programmatic weak supervision (PWS) that creates training data for machine learning by prompting. In contrast to typical PWS systems where weak supervision sources are programs coded by experts, Alfred enables users to encode their subject matter expertise via natural language prompts for language and vision-language models. Alfred provides a simple Python interface for the key steps of this emerging paradigm, with a high-throughput backend for large-scale data labeling. Users can quickly create, evaluate, and refine their prompt-based weak supervision sources; map the results to weak labels; and resolve their disagreements with a label model. Alfred enables a seamless local development experience backed by models served from self-managed computing clusters. It automatically optimizes the execution of prompts with optimized batching mechanisms. We find that this optimization improves query throughput by 2.9x versus a naive approach. We present two example use cases demonstrating Alfred on YouTube comment spam detection and pet breeds classification. Alfred is open source, available at https://github.com/BatsResearch/alfred.","2023-05-29","2024-08-29 02:51:18","2024-08-29 02:51:18","2023-07-02 17:05:00","","","","","","","Alfred","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2305.18623 arXiv:2305.18623 [cs]","","C:\Users\frank\Zotero\storage\JL8ZGSD3\Yu e Bach - 2023 - Alfred A System for Prompted Weak Supervision.pdf; C:\Users\frank\Zotero\storage\5SXR2F8L\2305.html","","notion","Computer Science - Machine Learning; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2305.18623","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AWIST87V","preprint","2022","Wang, Wei-Chen; Mueller, Jonas","Detecting Label Errors in Token Classification Data","","","","10.48550/arXiv.2210.03920","http://arxiv.org/abs/2210.03920","Mislabeled examples are a common issue in real-world data, particularly for tasks like token classification where many labels must be chosen on a fine-grained basis. Here we consider the task of finding sentences that contain label errors in token classification datasets. We study 11 different straightforward methods that score tokens/sentences based on the predicted class probabilities output by a (any) token classification model (trained via any procedure). In precision-recall evaluations based on real-world label errors in entity recognition data from CoNLL-2003, we identify a simple and effective method that consistently detects those sentences containing label errors when applied with different token classification models.","2022-10-08","2024-08-29 02:51:19","2024-08-29 02:51:19","2023-07-21 16:20:17","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2210.03920 arXiv:2210.03920 [cs]","","C:\Users\frank\Zotero\storage\3XVCCVVH\Wang e Mueller - 2022 - Detecting Label Errors in Token Classification Dat.pdf; C:\Users\frank\Zotero\storage\AN325F2B\2210.html","","notion","Computer Science - Machine Learning; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2210.03920","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"33F2G8LS","conferencePaper","2019","Geva, Mor; Goldberg, Yoav; Berant, Jonathan","Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets","Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)","","","10.18653/v1/D19-1107","https://www.aclweb.org/anthology/D19-1107","Crowdsourcing has been the prevalent paradigm for creating natural language understanding datasets in recent years. A common crowdsourcing practice is to recruit a small number of high-quality workers, and have them massively generate examples. Having only a few workers generate the majority of examples raises concerns about data diversity, especially when workers freely generate sentences. In this paper, we perform a series of experiments showing these concerns are evident in three recent NLP datasets. We show that model performance improves when training with annotator identiﬁers as features, and that models are able to recognize the most productive annotators. Moreover, we show that often models do not generalize well to examples from annotators that did not contribute to the training set. Our ﬁndings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators.","2019","2024-08-29 02:51:21","2024-08-29 02:51:21","2023-07-22 19:05:49","1161-1166","","","","","","Are We Modeling the Task or the Annotator?","","","","","Association for Computational Linguistics","Hong Kong, China","en","","","","","DOI.org (Crossref)","","","","C:\Users\frank\Zotero\storage\JV3ZM2PQ\Geva et al. - 2019 - Are We Modeling the Task or the Annotator An Inve.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)","","","","","","","","","","","","","","",""
"TWC3HEVG","journalArticle","2015","Aroyo, Lora; Welty, Chris","Truth Is a Lie: Crowd Truth and the Seven Myths of Human Annotation","AI Magazine","","0738-4602, 2371-9621","10.1609/aimag.v36i1.2564","https://onlinelibrary.wiley.com/doi/10.1609/aimag.v36i1.2564","","2015-03","2024-08-29 02:51:21","2024-08-29 02:51:21","2023-07-22 19:06:01","15-24","","1","36","","AI Magazine","Truth Is a Lie","","","","","","","en","","","","","DOI.org (Crossref)","","Number: 1","","C:\Users\frank\Zotero\storage\UCTGG53Z\Aroyo e Welty - 2015 - Truth Is a Lie Crowd Truth and the Seven Myths of.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZIQ7ZIWH","journalArticle","","Konyushkova, Ksenia; Sznitman, Raphael; Fua, Pascal","Learning Active Learning from Data","","","","","","In this paper, we suggest a novel data-driven approach to active learning (AL). The key idea is to train a regressor that predicts the expected error reduction for a candidate sample in a particular learning state. By formulating the query selection procedure as a regression problem we are not restricted to working with existing AL heuristics; instead, we learn strategies based on experience from previous AL outcomes. We show that a strategy can be learnt either from simple synthetic 2D datasets or from a subset of domain-speciﬁc data. Our method yields strategies that work well on real data from a wide range of domains.","","2024-08-29 02:51:23","2024-08-29 02:51:23","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\frank\Zotero\storage\675FGFFS\Konyushkova et al. - Learning Active Learning from Data.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F3KQJY2K","journalArticle","2014","Frenay, Benoit; Verleysen, Michel","Classification in the Presence of Label Noise: A Survey","IEEE Transactions on Neural Networks and Learning Systems","","2162-237X, 2162-2388","10.1109/TNNLS.2013.2292894","http://ieeexplore.ieee.org/document/6685834/","Label noise is an important issue in classiﬁcation, with many potential negative consequences. For example, the accuracy of predictions may decrease, whereas the complexity of inferred models and the number of necessary training samples may increase. Many works in the literature have been devoted to the study of label noise and the development of techniques to deal with label noise. However, the ﬁeld lacks a comprehensive survey on the different types of label noise, their consequences and the algorithms that consider label noise. This paper proposes to ﬁll this gap. First, the deﬁnitions and sources of label noise are considered and a taxonomy of the types of label noise is proposed. Second, the potential consequences of label noise are discussed. Third, label noise-robust, label noise cleansing, and label noise-tolerant algorithms are reviewed. For each category of approaches, a short discussion is proposed to help the practitioner to choose the most suitable technique in its own particular ﬁeld of application. Eventually, the design of experiments is also discussed, what may interest the researchers who would like to test their own algorithms. In this paper, label noise consists of mislabeled instances: no additional information is assumed to be available like e.g. conﬁdences on labels.","2014-05","2024-08-29 02:51:24","2024-08-29 02:51:24","2023-07-22 19:06:20","845-869","","5","25","","IEEE Trans. Neural Netw. Learning Syst.","Classification in the Presence of Label Noise","","","","","","","en","","","","","DOI.org (Crossref)","","Number: 5","","C:\Users\frank\Zotero\storage\E6IDHNNN\Frenay e Verleysen - 2014 - Classification in the Presence of Label Noise A S.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QEL72NQB","journalArticle","2017","Prelec, Dražen; Seung, H. Sebastian; McCoy, John","A solution to the single-question crowd wisdom problem","Nature","","0028-0836, 1476-4687","10.1038/nature21054","https://www.nature.com/articles/nature21054","","2017-01","2024-08-29 02:51:27","2024-08-29 02:51:27","2023-07-23 21:17:52","532-535","","7638","541","","Nature","","","","","","","","en","","","","","DOI.org (Crossref)","","Number: 7638","","C:\Users\frank\Zotero\storage\ZRASEIIR\Prelec et al. - 2017 - A solution to the single-question crowd wisdom pro.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TQA6EM6L","conferencePaper","2022","Zhang, Jieyu; Wang, Haonan; Hsieh, Cheng-Yu; Ratner, Alexander J","Understanding Programmatic Weak Supervision via Source-aware Influence Function","Advances in Neural Information Processing Systems","","","","https://proceedings.neurips.cc/paper_files/paper/2022/file/1343edb2739a61a6e20bd8764e814b50-Paper-Conference.pdf","","2022","2024-08-29 02:51:30","2024-08-29 02:51:30","","2862–2875","","","35","","","","","","","","Curran Associates, Inc.","","","","","","","","","https://github.com/JieyuZ2/wrench/blob/main/examples/run_explainer.py http://arxiv.org/abs/2205.12879","","C:\Users\frank\Zotero\storage\XZ2A983S\Zhang et al. - Understanding Programmatic Weak Supervision via So.pdf","","notion","","Koyejo, S.; Mohamed, S.; Agarwal, A.; Belgrave, D.; Cho, K.; Oh, A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IIA64MTQ","journalArticle","2020","Nashaat, Mona; Ghosh, Aindrila; Miller, James; Quader, Shaikh","Asterisk: Generating Large Training Datasets with Automatic Active Supervision","ACM/IMS Transactions on Data Science","","2691-1922","10.1145/3385188","https://dl.acm.org/doi/10.1145/3385188","Labeling datasets is one of the most expensive bottlenecks in data preprocessing tasks in machine learning. Therefore, organizations, in many domains, are applying weak supervision to produce noisy labels. However, since weak supervision relies on cheaper sources, the quality of the generated labels is problematic. Therefore, in this article, we present Asterisk, an end-to-end framework to generate high-quality, large-scale labeled datasets. The system, first, automatically generates heuristics to assign initial labels. Then, the framework applies a novel data-driven active learning process to enhance the labeling quality. We present an algorithm that learns the selection policy by accommodating the modeled accuracies of the heuristics, along with the outcome of the generative model. Finally, the system employs the output of the active learning process to enhance the quality of the labels. To evaluate the proposed system, we report its performance against four state-of-the-art techniques. In collaboration with our industrial partner, IBM, we test the framework within a wide range of real-world applications. The experiments include 10 datasets of varying sizes with a maximum size of 11 million records. The results illustrate the effectiveness of the framework in producing high-quality labels and achieving high classification accuracy with minimal annotation efforts.","2020-05-30","2024-08-29 02:51:32","2024-08-29 02:51:32","2023-07-29 19:49:27","13:1–13:25","","2","1","","ACM/IMS Trans. Data Sci.","Asterisk","","","","","","","","","","","","ACM Digital Library","","Number: 2","","C:\Users\frank\Zotero\storage\YXVKU5L3\Nashaat et al. - 2020 - Asterisk Generating Large Training Datasets with Automatic Active Supervision.pdf","","notion","Machine learning; Active learning; Data labeling; Heuristics design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SVHBQ2EA","preprint","2020","Fu, Daniel Y.; Chen, Mayee F.; Sala, Frederic; Hooper, Sarah M.; Fatahalian, Kayvon; Ré, Christopher","Fast and Three-rious: Speeding Up Weak Supervision with Triplet Methods","","","","10.48550/arXiv.2002.11955","http://arxiv.org/abs/2002.11955","Weak supervision is a popular method for building machine learning models without relying on ground truth annotations. Instead, it generates probabilistic training labels by estimating the accuracies of multiple noisy labeling sources (e.g., heuristics, crowd workers). Existing approaches use latent variable estimation to model the noisy sources, but these methods can be computationally expensive, scaling superlinearly in the data. In this work, we show that, for a class of latent variable models highly applicable to weak supervision, we can find a closed-form solution to model parameters, obviating the need for iterative solutions like stochastic gradient descent (SGD). We use this insight to build FlyingSquid, a weak supervision framework that runs orders of magnitude faster than previous weak supervision approaches and requires fewer assumptions. In particular, we prove bounds on generalization error without assuming that the latent variable model can exactly parameterize the underlying data distribution. Empirically, we validate FlyingSquid on benchmark weak supervision datasets and find that it achieves the same or higher quality compared to previous approaches without the need to tune an SGD procedure, recovers model parameters 170 times faster on average, and enables new video analysis and online learning applications.","2020-07-15","2024-08-29 02:51:34","2024-08-29 02:51:34","2023-08-01 18:45:05","","","","","","","Fast and Three-rious","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2002.11955 arXiv:2002.11955 [cs, stat]","","C:\Users\frank\Zotero\storage\YWYBXHBN\Fu et al. - 2020 - Fast and Three-rious Speeding Up Weak Supervision.pdf; C:\Users\frank\Zotero\storage\B8DQ7XXY\2002.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2002.11955","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EGBIA5S6","preprint","2019","Dunnmon, Jared; Ratner, Alexander; Khandwala, Nishith; Saab, Khaled; Markert, Matthew; Sagreiya, Hersh; Goldman, Roger; Lee-Messer, Christopher; Lungren, Matthew; Rubin, Daniel; Ré, Christopher","Cross-Modal Data Programming Enables Rapid Medical Machine Learning","","","","10.48550/arXiv.1903.11101","http://arxiv.org/abs/1903.11101","Labeling training datasets has become a key barrier to building medical machine learning models. One strategy is to generate training labels programmatically, for example by applying natural language processing pipelines to text reports associated with imaging studies. We propose cross-modal data programming, which generalizes this intuitive strategy in a theoretically-grounded way that enables simpler, clinician-driven input, reduces required labeling time, and improves with additional unlabeled data. In this approach, clinicians generate training labels for models defined over a target modality (e.g. images or time series) by writing rules over an auxiliary modality (e.g. text reports). The resulting technical challenge consists of estimating the accuracies and correlations of these rules; we extend a recent unsupervised generative modeling technique to handle this cross-modal setting in a provably consistent way. Across four applications in radiography, computed tomography, and electroencephalography, and using only several hours of clinician time, our approach matches or exceeds the efficacy of physician-months of hand-labeling with statistical significance, demonstrating a fundamentally faster and more flexible way of building machine learning models in medicine.","2019-03-26","2024-08-29 02:51:37","2024-08-29 02:51:37","2023-08-01 19:07:06","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:1903.11101 arXiv:1903.11101 [cs, eess, stat]","","C:\Users\frank\Zotero\storage\RRQDVABM\Dunnmon et al. - 2019 - Cross-Modal Data Programming Enables Rapid Medical.pdf; C:\Users\frank\Zotero\storage\5PRXI9JG\1903.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning; Electrical Engineering and Systems Science - Image and Video Processing","","","","","","","","","","","","","","","","","","","arXiv:1903.11101","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CMJ7N875","journalArticle","","Li, Jeffrey; Zhang, Jieyu; Schmidt, Ludwig; Ratner, Alexander","Characterizing the Impacts of Semi-supervised Learning for Weak Supervision","","","","","https://dmlr.ai/assets/accepted-papers/107/CameraReady/SSL4WS_ICML_Workshop_DMLR.pdf","Labeling training data is a critical and expensive step in producing high accuracy ML models, whether training from scratch or fine-tuning. To make labeling more efficient, two major approaches are programmatic weak supervision (WS) and semi-supervised learning (SSL). More recent works have either explicitly or implicitly used techniques at their intersection, but in various complex and ad hoc ways. In this work, we define a simple, modular design space to study the use of SSL techniques for WS more systematically. Surprisingly, we find that fairly simple methods from our design space match the performance of more complex state-of-the-art methods, averaging a 3 p.p. increase in accuracy/F1-score across 8 standard WS benchmarks. Further, we provide practical guidance on when different components are worth their added complexity and training costs. Contrary to current understanding, we find using SSL is not necessary to obtain the best performance on most WS benchmarks but is more effective when: (1) end models are smaller, and (2) WS provides labels for only a small portion of training examples.","","2024-08-29 02:51:39","2024-08-29 02:51:39","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\frank\Zotero\storage\PA4V55M3\Li et al. - Characterizing the Impacts of Semi-supervised Lear.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"39HEVSWV","journalArticle","","Haim, Niv; Vardi, Gal; Yehudai, Gilad; Shamir, Ohad; Irani, Michal","Reconstructing Training Data from Trained Neural Networks","","","","","","Understanding to what extent neural networks memorize training data is an intriguing question with practical and theoretical implications. In this paper we show that in some cases a significant fraction of the training data can in fact be reconstructed from the parameters of a trained neural network classifier. We propose a novel reconstruction scheme that stems from recent theoretical results about the implicit bias in training neural networks with gradient-based methods. To the best of our knowledge, our results are the first to show that reconstructing a large portion of the actual training samples from a trained neural network classifier is generally possible. This has negative implications on privacy, as it can be used as an attack for revealing sensitive training data. We demonstrate our method for binary MLP classifiers on a few standard computer vision datasets.","","2024-08-29 02:51:41","2024-08-29 02:51:41","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\frank\Zotero\storage\5ZN2GB9H\Haim et al. - Reconstructing Training Data from Trained Neural N.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UEPIBA7R","journalArticle","2022","Kong, Shuming; Shen, Yanyan; Huang, Linpeng","RESOLVING TRAINING BIASES VIA INFLUENCE- BASED DATA RELABELING","","","","","","The performance of supervised learning methods easily suffers from the training bias issue caused by train-test distribution mismatch or label noise. Inﬂuence function is a technique that estimates the impacts of a training sample on the model’s predictions. Recent studies on data resampling have employed inﬂuence functions to identify harmful training samples that will degrade model’s test performance. They have shown that discarding or downweighting the identiﬁed harmful training samples is an effective way to resolve training biases. In this work, we move one step forward and propose an inﬂuence-based relabeling framework named RDIA for reusing harmful training samples toward better model performance. To achieve this, we use inﬂuence functions to estimate how relabeling a training sample would affect model’s test performance and further develop a novel relabeling function R. We theoretically prove that applying R to relabel harmful training samples allows the model to achieve lower test loss than simply discarding them for any classiﬁcation tasks using cross-entropy loss. Extensive experiments on ten real-world datasets demonstrate RDIA outperforms the state-of-the-art data resampling methods and improves model’s robustness against label noise.","2022","2024-08-29 02:51:42","2024-08-29 02:51:42","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\frank\Zotero\storage\ITI4NBHW\Kong et al. - 2022 - RESOLVING TRAINING BIASES VIA INFLUENCE- BASED DAT.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"66AM3T9E","conferencePaper","2019","Koh, Pang Wei W; Ang, Kai-Siang; Teo, Hubert; Liang, Percy S","On the Accuracy of Influence Functions for Measuring Group Effects","Advances in Neural Information Processing Systems","","","","https://proceedings.neurips.cc/paper_files/paper/2019/hash/a78482ce76496fcf49085f2190e675b4-Abstract.html","Influence functions estimate the effect of removing a training point on a model without the need to retrain. They are based on a first-order Taylor approximation that is guaranteed to be accurate for sufficiently small changes to the model, and so are commonly used to study the effect of individual points in large datasets. However, we often want to study the effects of large groups of training points, e.g., to diagnose batch effects or apportion credit between different data sources. Removing such large groups can result in significant changes to the model. Are influence functions still accurate in this setting? In this paper, we find that across many different types of groups and for a range of real-world datasets, the predicted effect (using influence functions) of a group correlates surprisingly well with its actual effect, even if the absolute and relative errors are large. Our theoretical analysis shows that such strong correlation arises only under certain settings and need not hold in general, indicating that real-world datasets have particular properties that allow the influence approximation to be accurate.","2019","2024-08-29 02:51:44","2024-08-29 02:51:44","2023-08-08 21:07:55","","","","32","","","","","","","","Curran Associates, Inc.","","","","","","","Neural Information Processing Systems","","","","C:\Users\frank\Zotero\storage\R6GW4LD7\Koh et al. - 2019 - On the Accuracy of Influence Functions for Measuri.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N3Y3LDM5","preprint","2023","Zhu, Yiming; Zhang, Peixian; Haq, Ehsan-Ul; Hui, Pan; Tyson, Gareth","Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing Tasks","","","","10.48550/arXiv.2304.10145","http://arxiv.org/abs/2304.10145","The release of ChatGPT has uncovered a range of possibilities whereby large language models (LLMs) can substitute human intelligence. In this paper, we seek to understand whether ChatGPT has the potential to reproduce human-generated label annotations in social computing tasks. Such an achievement could significantly reduce the cost and complexity of social computing research. As such, we use ChatGPT to relabel five seminal datasets covering stance detection (2x), sentiment analysis, hate speech, and bot detection. Our results highlight that ChatGPT does have the potential to handle these data annotation tasks, although a number of challenges remain. ChatGPT obtains an average accuracy 0.609. Performance is highest for the sentiment analysis dataset, with ChatGPT correctly annotating 64.9% of tweets. Yet, we show that performance varies substantially across individual labels. We believe this work can open up new lines of analysis and act as a basis for future research into the exploitation of ChatGPT for human annotation tasks.","2023-04-22","2024-08-29 02:51:46","2024-08-29 02:51:46","2023-08-09 18:28:07","","","","","","","Can ChatGPT Reproduce Human-Generated Labels?","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2304.10145 arXiv:2304.10145 [cs]","","C:\Users\frank\Zotero\storage\WI3WTMTN\Zhu et al. - 2023 - Can ChatGPT Reproduce Human-Generated Labels A St.pdf; C:\Users\frank\Zotero\storage\Q6U4I9CB\2304.html","","notion","Computer Science - Artificial Intelligence; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2304.10145","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RWWAJ4UN","preprint","2023","Zhou, Hang; Mueller, Jonas; Kumar, Mayank; Wang, Jane-Ling; Lei, Jing","Detecting Errors in Numerical Data via any Regression Model","","","","10.48550/arXiv.2305.16583","http://arxiv.org/abs/2305.16583","Noise plagues many numerical datasets, where the recorded values in the data may fail to match the true underlying values due to reasons including: erroneous sensors, data entry/processing mistakes, or imperfect human estimates. Here we consider estimating which data values are incorrect along a numerical column. We present a model-agnostic approach that can utilize any regressor (i.e. statistical or machine learning model) which was fit to predict values in this column based on the other variables in the dataset. By accounting for various uncertainties, our approach distinguishes between genuine anomalies and natural data fluctuations, conditioned on the available information in the dataset. We establish theoretical guarantees for our method and show that other approaches like conformal inference struggle to detect errors. We also contribute a new error detection benchmark involving 5 regression datasets with real-world numerical errors (for which the true values are also known). In this benchmark and additional simulation studies, our method identifies incorrect values with better precision/recall than other approaches.","2023-06-02","2024-08-29 02:51:47","2024-08-29 02:51:47","2023-08-20 12:48:27","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2305.16583 arXiv:2305.16583 [cs, stat]","","C:\Users\frank\Zotero\storage\H5K37D5J\Zhou et al. - 2023 - Detecting Errors in Numerical Data via any Regress.pdf; C:\Users\frank\Zotero\storage\JDIAQZV4\2305.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2305.16583","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QNWAYE4N","webpage","2022","Hammoudeh, Zayd; Lowd, Daniel","Training Data Influence Analysis and Estimation: A Survey","arXiv.org","","","","https://arxiv.org/abs/2212.04612v2","Good models require good training data. For overparameterized deep models, the causal relationship between training data and model predictions is increasingly opaque and poorly understood. Influence analysis partially demystifies training's underlying interactions by quantifying the amount each training instance alters the final model. Measuring the training data's influence exactly can be provably hard in the worst case; this has led to the development and use of influence estimators, which only approximate the true influence. This paper provides the first comprehensive survey of training data influence analysis and estimation. We begin by formalizing the various, and in places orthogonal, definitions of training data influence. We then organize state-of-the-art influence analysis methods into a taxonomy; we describe each of these methods in detail and compare their underlying assumptions, asymptotic complexities, and overall strengths and weaknesses. Finally, we propose future research directions to make influence analysis more useful in practice as well as more theoretically and empirically sound. A curated, up-to-date list of resources related to influence analysis is available at https://github.com/ZaydH/influence_analysis_papers.","2022-12-09","2024-08-29 02:51:48","2024-08-29 02:51:48","2023-08-21 00:29:45","","","","","","","Training Data Influence Analysis and Estimation","","","","","","","en","","","","","","","","","C:\Users\frank\Zotero\storage\WUNIZ8W5\Hammoudeh e Lowd - 2022 - Training Data Influence Analysis and Estimation A.pdf","","notion; influence function","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9GZEWMZG","journalArticle","","Yao, Yu","On the Importance of Transition Matrix for Learning with Noisy Labels","","","","","","","","2024-08-29 02:51:52","2024-08-29 02:51:52","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\frank\Zotero\storage\BM933EHV\Yao - On the Importance of Transition Matrix for Learnin.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5WNUNHLX","conferencePaper","2023","Nouretdinov, Ilia; Gammerman, James","Conformal Association Rule Mining (CARM): A novel technique for data error detection and probabilistic correction","Proceedings of the Twelfth Symposium on Conformal  and Probabilistic Prediction with Applications","","","","https://proceedings.mlr.press/v204/nouretdinov23a.html","Conformal prediction (CP) is a modern framework for  reliable machine learning. It is most commonly used  in the context of supervised learning, where in  combination with an underlying algorithm it  generates predicted labels for new, unlabelled  examples and complements each of them with an  individual measure of confidence. Conversely,  association rule mining (ARM) is an unsupervised  learning technique for discovering interesting  relationships in large datasets in the form of  rules. In this work, we integrate CP and ARM to  develop a novel technique termed Conformal  Association Rule Mining (CARM). The technique  enables the identification of probable errors within  a set of binary labels. Subsequently, these probable  errors are analysed using another modern framework  called Venn-ABERS prediction to correct the value in  a probabilistic way.","2023-08-17","2024-08-29 02:51:57","2024-08-29 02:51:57","2023-08-31 11:54:37","267-286","","","","","","Conformal Association Rule Mining (CARM)","","","","","PMLR","","en","","","","","proceedings.mlr.press","","ISSN: 2640-3498","","C:\Users\frank\Zotero\storage\Y4MPCTK2\Nouretdinov e Gammerman - 2023 - Conformal Association Rule Mining (CARM) A novel .pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Conformal and Probabilistic Prediction with Applications","","","","","","","","","","","","","","",""
"27D5LDGY","preprint","2023","Loukas, Lefteris; Stogiannidis, Ilias; Malakasiotis, Prodromos; Vassos, Stavros","Breaking the Bank with ChatGPT: Few-Shot Text Classification for Finance","","","","","http://arxiv.org/abs/2308.14634","We propose the use of conversational GPT models for easy and quick few-shot text classification in the financial domain using the Banking77 dataset. Our approach involves in-context learning with GPT-3.5 and GPT-4, which minimizes the technical expertise required and eliminates the need for expensive GPU computing while yielding quick and accurate results. Additionally, we fine-tune other pre-trained, masked language models with SetFit, a recent contrastive learning technique, to achieve state-of-the-art results both in full-data and few-shot settings. Our findings show that querying GPT-3.5 and GPT-4 can outperform fine-tuned, non-generative models even with fewer examples. However, subscription fees associated with these solutions may be considered costly for small organizations. Lastly, we find that generative models perform better on the given task when shown representative samples selected by a human expert rather than when shown random ones. We conclude that a) our proposed methods offer a practical solution for few-shot tasks in datasets with limited label availability, and b) our state-of-the-art results can inspire future work in the area.","2023-08-28","2024-08-29 02:51:59","2024-08-29 02:51:59","2023-08-31 11:55:26","","","","","","","Breaking the Bank with ChatGPT","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2308.14634 arXiv:2308.14634 [cs, q-fin]","","C:\Users\frank\Zotero\storage\8C8EDB5V\2308.html; C:\Users\frank\Zotero\storage\9IVAUNZR\Loukas et al. - 2023 - Breaking the Bank with ChatGPT Few-Shot Text Clas.pdf","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Computation and Language; Quantitative Finance - Computational Finance","","","","","","","","","","","","","","","","","","","arXiv:2308.14634","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HN2RIWQG","conferencePaper","2023","Sun, Wei; Ji, Shaoxiong; Denti, Tuulia; Moen, Hans; Kerro, Oleg; Rannikko, Antti; Marttinen, Pekka; Koskinen, Miika","Weak Supervision and Clustering-Based Sample Selection for Clinical Named Entity Recognition","Machine Learning and Knowledge Discovery in Databases: Applied Data Science and Demo Track","978-3-031-43427-3","","10.1007/978-3-031-43427-3_27","","One of the central tasks of medical text analysis is to extract and structure meaningful information from plain-text clinical documents. Named Entity Recognition (NER) is a sub-task of information extraction that involves identifying predefined entities from unstructured free text. Notably, NER models require large amounts of human-labeled data to train, but human annotation is costly and laborious and often requires medical training. Here, we aim to overcome the shortage of manually annotated data by introducing a training scheme for NER models that uses an existing medical ontology to assign weak labels to entities and provides enhanced domain-specific model adaptation with in-domain continual pretraining. Due to limited human annotation resources, we develop a specific module to collect a more representative test dataset from the data lake than a random selection. To validate our framework, we invite clinicians to annotate the test set. In this way, we construct two Finnish medical NER datasets based on clinical records retrieved from a hospital’s data lake and evaluate the effectiveness of the proposed methods. The code is available at https://github.com/VRCMF/HAM-net.git.","2023","2024-08-29 02:52:00","2024-08-29 02:52:00","","444-459","","","","","","","Lecture Notes in Computer Science","","","","Springer Nature Switzerland","Cham","en","","","","","Springer Link","","","","C:\Users\frank\Zotero\storage\9SRPQ93A\Sun et al. - 2023 - Weak Supervision and Clustering-Based Sample Selec.pdf","","notion","Clinical Reports; Distant Supervision; Named Entity Recognition; Sample Selection","De Francisci Morales, Gianmarco; Perlich, Claudia; Ruchansky, Natali; Kourtellis, Nicolas; Baralis, Elena; Bonchi, Francesco","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JB7U9XBN","journalArticle","2023","Yin, Yining; Feng, Yang; Weng, Shihao; Liu, Zixi; Yao, Yuan; Zhang, Yichi; Zhao, Zhihong; Chen, Zhenyu","Dynamic Data Fault Localization for Deep Neural Networks","","","","","https://cs.nju.edu.cn/yuanyao/static/fse2023.pdf","Rich datasets have empowered various deep learning (DL) applications, leading to remarkable success in many fields. However, data faults hidden in the datasets could result in DL applications behaving unpredictably and even cause massive monetary and life losses. To alleviate this problem, in this paper, we propose a dynamic data fault localization approach, namely DFauLo, to locate the mislabeled and noisy data in the deep learning datasets. DFauLo is inspired by the conventional mutation-based code fault localization, but utilizes the differences between DNN mutants to amplify and identify the potential data faults. Specifically, it first generates multiple DNN model mutants of the original trained model. Then it extracts features from these mutants and maps them into a suspiciousness score indicating the probability of the given data being a data fault. Moreover, DFauLo is the first dynamic data fault localization technique, prioritizing the suspected data based on user feedback, and providing the generalizability to unseen data faults during training. To validate DFauLo, we extensively evaluate it on 26 cases with various fault types, data types, and model structures. We also evaluate DFauLo on three widely-used benchmark datasets.","2023","2024-08-29 02:52:01","2024-08-29 02:52:01","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\frank\Zotero\storage\GHAE2NR8\Yin et al. - 2023 - Dynamic Data Fault Localization for Deep Neural Ne.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YSC4W236","preprint","2023","Papadopoulos, Georgios; Silavong, Fran; Moran, Sean","A Benchmark Generative Probabilistic Model for Weak Supervised Learning","","","","10.48550/arXiv.2303.17841","http://arxiv.org/abs/2303.17841","Finding relevant and high-quality datasets to train machine learning models is a major bottleneck for practitioners. Furthermore, to address ambitious real-world use-cases there is usually the requirement that the data come labelled with high-quality annotations that can facilitate the training of a supervised model. Manually labelling data with high-quality labels is generally a time-consuming and challenging task and often this turns out to be the bottleneck in a machine learning project. Weak Supervised Learning (WSL) approaches have been developed to alleviate the annotation burden by offering an automatic way of assigning approximate labels (pseudo-labels) to unlabelled data based on heuristics, distant supervision and knowledge bases. We apply probabilistic generative latent variable models (PLVMs), trained on heuristic labelling representations of the original dataset, as an accurate, fast and cost-effective way to generate pseudo-labels. We show that the PLVMs achieve state-of-the-art performance across four datasets. For example, they achieve 22% points higher F1 score than Snorkel in the class-imbalanced Spouse dataset. PLVMs are plug-and-playable and are a drop-in replacement to existing WSL frameworks (e.g. Snorkel) or they can be used as benchmark models for more complicated algorithms, giving practitioners a compelling accuracy boost.","2023-03-31","2024-08-29 02:52:03","2024-08-29 02:52:03","2023-09-22 19:23:56","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2303.17841 arXiv:2303.17841 [cs]","","C:\Users\frank\Zotero\storage\BRZP5JG8\Papadopoulos et al. - 2023 - A Benchmark Generative Probabilistic Model for Wea.pdf; C:\Users\frank\Zotero\storage\XXCUBMET\2303.html","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2303.17841","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5WAZ7R4C","journalArticle","","Miyaji, Renato O; de Almeida, Felipe V; Corrêa, Pedro L P","Aplicação de Técnicas de Confident Learning para Limpeza de Dados e Melhoria de Desempenho de Classificadores de Aprendizado de Máquina: um Estudo de Caso","","","","","","Model-Centric techniques, such as hyperparameter selection and regularization, are commonly used in the literature to improve the performance of Machine Learning Classifiers. However, when a dataset with uncertain data is used, Data-Centric approaches have a good potential. These methods aim to systematically engineer data to improve model performance. Thus, Confident Learning (CL) techniques were applied for a study case of Species Distribution Modeling in the Amazon Basin using Machine Learning Classifiers, which aimed to predict the probability of occurrence of a species, given environmental conditions. In comparison with Model-Centric methods, CL techniques presented a 23% improvement of ROC-AUC for Logistic Regression.","","2024-08-29 02:52:04","2024-08-29 02:52:04","","","","","","","","","","","","","","","pt","","","","","Zotero","","","","C:\Users\frank\Zotero\storage\UALK92RJ\Miyaji et al. - Aplicação de Técnicas de Confident Learning para L.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A9EC8AMG","conferencePaper","2023","Papadopoulos, Georgios; Silavong, Fran; Moran, Sean","A Baseline Generative Probabilistic Model for Weakly Supervised Learning","Machine Learning and Knowledge Discovery in Databases: Applied Data Science and Demo Track","978-3-031-43427-3","","","","Finding relevant and high-quality datasets to train machine learning models is a major bottleneck for practitioners. Furthermore, to address ambitious real-world use-cases there is usually the requirement that the data come labelled with high-quality annotations that can facilitate the training of a supervised model. Manually labelling data with high-quality labels is generally a time-consuming and challenging task and often this turns out to be the bottleneck in a machine learning project. Weakly Supervised Learning (WSL) approaches have been developed to alleviate the annotation burden by offering an automatic way of assigning approximate labels (pseudo-labels) to unlabelled data based on heuristics, distant supervision and knowledge bases. We apply probabilistic generative latent variable models (PLVMs), trained on heuristic labelling representations of the original dataset, as an accurate, fast and cost-effective way to generate pseudo-labels. We show that the PLVMs achieve state-of-the-art performance across four datasets. For example, they achieve 22% points higher F1 score than Snorkel in the class-imbalanced Spouse dataset. PLVMs are plug-and-playable and are a drop-in replacement to existing WSL frameworks (e.g. Snorkel) or they can be used as baseline high-performance models for more complicated algorithms, giving practitioners a compelling accuracy boost.","2023","2024-08-29 02:52:05","2024-08-29 02:52:05","","36–50","","","","","","","","","","","Springer Nature Switzerland","Cham","","","","","","","","","","","","","","De Francisci Morales, Gianmarco; Perlich, Claudia; Ruchansky, Natali; Kourtellis, Nicolas; Baralis, Elena; Bonchi, Francesco","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D25F9D3Y","journalArticle","","Schmarje, Lars; Grossmann, Vasco; Michels, Tim; Nazarenus, Jakob; Santarossa, Monty; Zelenka, Claudius; Koch, Reinhard","Label Smarter, Not Harder: CleverLabel for Faster Annotation of Ambiguous Image Classiﬁcation with Higher Quality","","","","","","High-quality data is crucial for the success of machine learning, but labeling large datasets is often a time-consuming and costly process. While semi-supervised learning can help mitigate the need for labeled data, label quality remains an open issue due to ambiguity and disagreement among annotators. Thus, we use proposal-guided annotations as one option which leads to more consistency between annotators. However, proposing a label increases the probability of the annotators deciding in favor of this speciﬁc label. This introduces a bias which we can simulate and remove. We propose a new method CleverLabel for Cost-eﬀective LabEling using Validated proposal-guidEd annotations and Repaired LABELs. CleverLabel can reduce labeling costs by up to 30.0%, while achieving a relative improvement in Kullback-Leibler divergence of up to 29.8% compared to the previous state-of-the-art on a multi-domain real-world image classiﬁcation benchmark. CleverLabel offers a novel solution to the challenge of eﬃciently labeling large datasets while also improving the label quality.","","2024-08-29 02:52:05","2024-08-29 02:52:05","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\frank\Zotero\storage\FZ6I967J\Schmarje et al. - Label Smarter, Not Harder CleverLabel for Faster .pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YLIHTVLC","preprint","2023","Chiang, Chao-Kai; Sugiyama, Masashi","Unified Risk Analysis for Weakly Supervised Learning","","","","","http://arxiv.org/abs/2309.08216","Among the flourishing research of weakly supervised learning (WSL), we recognize the lack of a unified interpretation of the mechanism behind the weakly supervised scenarios, let alone a systematic treatment of the risk rewrite problem, a crucial step in the empirical risk minimization approach. In this paper, we introduce a framework providing a comprehensive understanding and a unified methodology for WSL. The formulation component of the framework, leveraging a contamination perspective, provides a unified interpretation of how weak supervision is formed and subsumes fifteen existing WSL settings. The induced reduction graphs offer comprehensive connections over WSLs. The analysis component of the framework, viewed as a decontamination process, provides a systematic method of conducting risk rewrite. In addition to the conventional inverse matrix approach, we devise a novel strategy called marginal chain aiming to decontaminate distributions. We justify the feasibility of the proposed framework by recovering existing rewrites reported in the literature.","2023-09-15","2024-08-29 02:52:06","2024-08-29 02:52:06","2023-09-22 19:32:37","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2309.08216 arXiv:2309.08216 [cs]","","C:\Users\frank\Zotero\storage\W35X6XVY\2309.html; C:\Users\frank\Zotero\storage\MIRVPELD\Chiang e Sugiyama - 2023 - Unified Risk Analysis for Weakly Supervised Learni.pdf","","notion","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2309.08216","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R3PJJBI4","preprint","2021","Zhang, Yikai; Zheng, Songzhu; Wu, Pengxiang; Goswami, Mayank; Chen, Chao","Learning with Feature-Dependent Label Noise: A Progressive Approach","","","","10.48550/arXiv.2103.07756","http://arxiv.org/abs/2103.07756","Label noise is frequently observed in real-world large-scale datasets. The noise is introduced due to a variety of reasons; it is heterogeneous and feature-dependent. Most existing approaches to handling noisy labels fall into two categories: they either assume an ideal feature-independent noise, or remain heuristic without theoretical guarantees. In this paper, we propose to target a new family of feature-dependent label noise, which is much more general than commonly used i.i.d. label noise and encompasses a broad spectrum of noise patterns. Focusing on this general noise family, we propose a progressive label correction algorithm that iteratively corrects labels and refines the model. We provide theoretical guarantees showing that for a wide variety of (unknown) noise patterns, a classifier trained with this strategy converges to be consistent with the Bayes classifier. In experiments, our method outperforms SOTA baselines and is robust to various noise types and levels.","2021-03-27","2024-08-29 02:52:10","2024-08-29 02:52:10","2023-09-22 19:38:47","","","","","","","Learning with Feature-Dependent Label Noise","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2103.07756 arXiv:2103.07756 [cs, stat]","","C:\Users\frank\Zotero\storage\6BRL5W57\Zhang et al. - 2021 - Learning with Feature-Dependent Label Noise A Pro.pdf; C:\Users\frank\Zotero\storage\W32TY834\2103.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning; Statistics - Applications; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2103.07756","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YXZU2LHL","conferencePaper","2023","Zhang, Yongqi; Zhang, Hui; Yao, Quanming; Wan, Jun","Combining Self-Supervised and Supervised Learning with Noisy Labels","2023 IEEE International Conference on Image Processing (ICIP)","","","10.1109/ICIP49359.2023.10221957","","","2023","2024-08-29 02:52:12","2024-08-29 02:52:12","","605-609","","","","","","","","","","","","","","","","","","","","","","C:\Users\frank\Zotero\storage\X252TE9T\Zhang et al. - 2023 - Combining Self-Supervised and Supervised Learning .pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MIJ3C2VE","journalArticle","2023","Wang, Xingyu; Chi, Xurong; Song, Yanzhi; Yang, Zhouwang","Active learning with label quality control","PeerJ Computer Science","","2376-5992","10.7717/peerj-cs.1480","https://doi.org/10.7717/peerj-cs.1480","Training deep neural networks requires a large number of labeled samples, which are typically provided by crowdsourced workers or professionals at a high cost. To obtain qualified labels, samples need to be relabeled for inspection to control the quality of the labels, which further increases the cost. Active learning methods aim to select the most valuable samples for labeling to reduce labeling costs. We designed a practical active learning method that adaptively allocates labeling resources to the most valuable unlabeled samples and the most likely mislabeled labeled samples, thus significantly reducing the overall labeling cost. We prove that the probability of our proposed method labeling more than one sample from any redundant sample set in the same batch is less than 1/k, where k is the number of the k-fold experiment used in the method, thus significantly reducing the labeling resources wasted on redundant samples. Our proposed method achieves the best level of results on benchmark datasets, and it performs well in an industrial application of automatic optical inspection.","2023-09","2024-08-29 02:52:13","2024-08-29 02:52:13","","e1480","","","9","","","","","","","","","","","","","","","","","","","C:\Users\frank\Zotero\storage\Q9PPRVS3\Wang et al. - 2023 - Active learning with label quality control.pdf","","notion","Active learning; Automated optical inspection; Label quality control","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AGX3I837","preprint","2022","Zhu, Zhaowei; Dong, Zihao; Liu, Yang","Detecting Corrupted Labels Without Training a Model to Predict","","","","10.48550/arXiv.2110.06283","http://arxiv.org/abs/2110.06283","Label noise in real-world datasets encodes wrong correlation patterns and impairs the generalization of deep neural networks (DNNs). It is critical to find efficient ways to detect corrupted patterns. Current methods primarily focus on designing robust training techniques to prevent DNNs from memorizing corrupted patterns. These approaches often require customized training processes and may overfit corrupted patterns, leading to a performance drop in detection. In this paper, from a more data-centric perspective, we propose a training-free solution to detect corrupted labels. Intuitively, ``closer'' instances are more likely to share the same clean label. Based on the neighborhood information, we propose two methods: the first one uses ``local voting"" via checking the noisy label consensuses of nearby features. The second one is a ranking-based approach that scores each instance and filters out a guaranteed number of instances that are likely to be corrupted. We theoretically analyze how the quality of features affects the local voting and provide guidelines for tuning neighborhood size. We also prove the worst-case error bound for the ranking-based method. Experiments with both synthetic and real-world label noise demonstrate our training-free solutions consistently and significantly improve most of the training-based baselines. Code is available at github.com/UCSC-REAL/SimiFeat.","2022-06-17","2024-08-29 02:52:14","2024-08-29 02:52:14","2023-09-23 19:05:21","","","","","","","","","","","","","","","","","","","arXiv.org","","Issue: arXiv:2110.06283 arXiv:2110.06283 [cs]","","C:\Users\frank\Zotero\storage\P78TZZDY\Zhu et al. - 2022 - Detecting Corrupted Labels Without Training a Model to Predict.pdf","","notion","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2110.06283","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IWBNVZLF","preprint","2023","Zha, Daochen; Bhat, Zaid Pervaiz; Lai, Kwei-Herng; Yang, Fan; Jiang, Zhimeng; Zhong, Shaochen; Hu, Xia","Data-centric Artificial Intelligence: A Survey","","","","10.48550/arXiv.2303.10158","http://arxiv.org/abs/2303.10158","Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enabler of its great success is the availability of abundant and high-quality data for building machine learning models. Recently, the role of data in AI has been significantly magnified, giving rise to the emerging concept of data-centric AI. The attention of researchers and practitioners has gradually shifted from advancing model design to enhancing the quality and quantity of the data. In this survey, we discuss the necessity of data-centric AI, followed by a holistic view of three general data-centric goals (training data development, inference data development, and data maintenance) and the representative methods. We also organize the existing literature from automation and collaboration perspectives, discuss the challenges, and tabulate the benchmarks for various tasks. We believe this is the first comprehensive survey that provides a global view of a spectrum of tasks across various stages of the data lifecycle. We hope it can help the readers efficiently grasp a broad picture of this field, and equip them with the techniques and further research ideas to systematically engineer data for building AI systems. A companion list of data-centric AI resources will be regularly updated on https://github.com/daochenzha/data-centric-AI","2023-06-11","2024-08-29 02:52:16","2024-08-29 02:52:16","2023-09-25 16:34:50","","","","","","","Data-centric Artificial Intelligence","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2303.10158 arXiv:2303.10158 [cs]","","C:\Users\frank\Zotero\storage\B4W9MAQL\Zha et al. - 2023 - Data-centric Artificial Intelligence A Survey.pdf; C:\Users\frank\Zotero\storage\THPNDVIT\2303.html","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Databases","","","","","","","","","","","","","","","","","","","arXiv:2303.10158","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J3NAH8WK","preprint","2022","Gao, Zhengqi; Sun, Fan-Keng; Yang, Mingran; Ren, Sucheng; Xiong, Zikai; Engeler, Marc; Burazer, Antonio; Wildling, Linda; Daniel, Luca; Boning, Duane S.","Learning from Multiple Annotator Noisy Labels via Sample-wise Label Fusion","","","","10.48550/arXiv.2207.11327","http://arxiv.org/abs/2207.11327","Data lies at the core of modern deep learning. The impressive performance of supervised learning is built upon a base of massive accurately labeled data. However, in some real-world applications, accurate labeling might not be viable; instead, multiple noisy labels (instead of one accurate label) are provided by several annotators for each data sample. Learning a classifier on such a noisy training dataset is a challenging task. Previous approaches usually assume that all data samples share the same set of parameters related to annotator errors, while we demonstrate that label error learning should be both annotator and data sample dependent. Motivated by this observation, we propose a novel learning algorithm. The proposed method displays superiority compared with several state-of-the-art baseline methods on MNIST, CIFAR-100, and ImageNet-100. Our code is available at: https://github.com/zhengqigao/Learning-from-Multiple-Annotator-Noisy-Labels.","2022-07-22","2024-08-29 02:52:19","2024-08-29 02:52:19","2023-10-13 20:55:54","","","","","","","","","","","","","","","","","","","arXiv.org","","Issue: arXiv:2207.11327 arXiv:2207.11327 [cs]","","C:\Users\frank\Zotero\storage\6M4I9GY3\Gao et al. - 2022 - Learning from Multiple Annotator Noisy Labels via Sample-wise Label Fusion.pdf","","notion","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2207.11327","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CDRU8AXJ","preprint","2023","Shin, Changho; Cromp, Sonia; Adila, Dyah; Sala, Frederic","Mitigating Source Bias for Fairer Weak Supervision","","","","10.48550/arXiv.2303.17713","http://arxiv.org/abs/2303.17713","Weak supervision overcomes the label bottleneck, enabling efficient development of training sets. Millions of models trained on such datasets have been deployed in the real world and interact with users on a daily basis. However, the techniques that make weak supervision attractive -- such as integrating any source of signal to estimate unknown labels -- also ensure that the pseudolabels it produces are highly biased. Surprisingly, given everyday use and the potential for increased bias, weak supervision has not been studied from the point of view of fairness. This work begins such a study. Our departure point is the observation that even when a fair model can be built from a dataset with access to ground-truth labels, the corresponding dataset labeled via weak supervision can be arbitrarily unfair. Fortunately, not all is lost: we propose and empirically validate a model for source unfairness in weak supervision, then introduce a simple counterfactual fairness-based technique that can mitigate these biases. Theoretically, we show that it is possible for our approach to simultaneously improve both accuracy and fairness metrics -- in contrast to standard fairness approaches that suffer from tradeoffs. Empirically, we show that our technique improves accuracy on weak supervision baselines by as much as 32% while reducing demographic parity gap by 82.5%.","2023-03-30","2024-08-29 02:52:21","2024-08-29 02:52:21","2023-10-14 20:59:44","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2303.17713 arXiv:2303.17713 [cs, stat]","","C:\Users\frank\Zotero\storage\9Q8SFQC2\Shin et al. - 2023 - Mitigating Source Bias for Fairer Weak Supervision.pdf; C:\Users\frank\Zotero\storage\XIEFEHMD\2303.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","arXiv:2303.17713","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2A39CV6F","preprint","2023","Guu, Kelvin; Webson, Albert; Pavlick, Ellie; Dixon, Lucas; Tenney, Ian; Bolukbasi, Tolga","Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs","","","","10.48550/arXiv.2303.08114","http://arxiv.org/abs/2303.08114","Training data attribution (TDA) methods offer to trace a model's prediction on any given example back to specific influential training examples. Existing approaches do so by assigning a scalar influence score to each training example, under a simplifying assumption that influence is additive. But in reality, we observe that training examples interact in highly non-additive ways due to factors such as inter-example redundancy, training order, and curriculum learning effects. To study such interactions, we propose Simfluence, a new paradigm for TDA where the goal is not to produce a single influence score per example, but instead a training run simulator: the user asks, ``If my model had trained on example $z_1$, then $z_2$, ..., then $z_n$, how would it behave on $z_{test}$?''; the simulator should then output a simulated training run, which is a time series predicting the loss on $z_{test}$ at every step of the simulated run. This enables users to answer counterfactual questions about what their model would have learned under different training curricula, and to directly see where in training that learning would occur. We present a simulator, Simfluence-Linear, that captures non-additive interactions and is often able to predict the spiky trajectory of individual example losses with surprising fidelity. Furthermore, we show that existing TDA methods such as TracIn and influence functions can be viewed as special cases of Simfluence-Linear. This enables us to directly compare methods in terms of their simulation accuracy, subsuming several prior TDA approaches to evaluation. In experiments on large language model (LLM) fine-tuning, we show that our method predicts loss trajectories with much higher accuracy than existing TDA methods (doubling Spearman's correlation and reducing mean-squared error by 75%) across several tasks, models, and training methods.","2023-03-14","2024-08-29 02:52:24","2024-08-29 02:52:24","2023-10-21 11:58:06","","","","","","","Simfluence","","","","","","","","","","","","arXiv.org","","Issue: arXiv:2303.08114 arXiv:2303.08114 [cs]","","C:\Users\frank\Zotero\storage\6RQ5Q7WU\Guu et al. - 2023 - Simfluence Modeling the Influence of Individual Training Examples by Simulating Training Runs.pdf","","notion","Computer Science - Machine Learning; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2303.08114","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LWTUJ5EJ","preprint","2022","Stephan, Andreas; Roth, Benjamin","WeaNF: Weak Supervision with Normalizing Flows","","","","10.48550/arXiv.2204.13409","http://arxiv.org/abs/2204.13409","A popular approach to decrease the need for costly manual annotation of large data sets is weak supervision, which introduces problems of noisy labels, coverage and bias. Methods for overcoming these problems have either relied on discriminative models, trained with cost functions specific to weak supervision, and more recently, generative models, trying to model the output of the automatic annotation process. In this work, we explore a novel direction of generative modeling for weak supervision: Instead of modeling the output of the annotation process (the labeling function matches), we generatively model the input-side data distributions (the feature space) covered by labeling functions. Specifically, we estimate a density for each weak labeling source, or labeling function, by using normalizing flows. An integral part of our method is the flow-based modeling of multiple simultaneously matching labeling functions, and therefore phenomena such as labeling function overlap and correlations are captured. We analyze the effectiveness and modeling capabilities on various commonly used weak supervision data sets, and show that weakly supervised normalizing flows compare favorably to standard weak supervision baselines.","2022-05-02","2024-08-29 02:52:27","2024-08-29 02:52:27","2023-10-21 15:46:56","","","","","","","WeaNF","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2204.13409 arXiv:2204.13409 [cs]","","C:\Users\frank\Zotero\storage\3BSCRKJ2\Stephan e Roth - 2022 - WeaNF Weak Supervision with Normalizing Flows.pdf; C:\Users\frank\Zotero\storage\BTRHWZKN\2204.html","","notion","Computer Science - Machine Learning; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2204.13409","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W6M2989A","conferencePaper","2019","Amershi, Saleema; Weld, Dan; Vorvoreanu, Mihaela; Fourney, Adam; Nushi, Besmira; Collisson, Penny; Suh, Jina; Iqbal, Shamsi; Bennett, Paul N.; Inkpen, Kori; Teevan, Jaime; Kikin-Gil, Ruth; Horvitz, Eric","Guidelines for Human-AI Interaction","Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","978-1-4503-5970-2","","10.1145/3290605.3300233","https://dl.acm.org/doi/10.1145/3290605.3300233","Advances in artifcial intelligence (AI) frame opportunities and challenges for user interface design. Principles for humanAI interaction have been discussed in the human-computer interaction community for over two decades, but more study and innovation are needed in light of advances in AI and the growing uses of AI technologies in human-facing applications. We propose 18 generally applicable design guidelines for human-AI interaction. These guidelines are validated through multiple rounds of evaluation including a user study with 49 design practitioners who tested the guidelines against 20 popular AI-infused products. The results verify the relevance of the guidelines over a spectrum of interaction scenarios and reveal gaps in our knowledge, highlighting opportunities for further research. Based on the evaluations, we believe the set of design guidelines can serve as a resource to practitioners working on the design of applications and features that harness AI technologies, and to researchers interested in the further development of guidelines for human-AI interaction design.","2019-05-02","2024-08-29 02:52:29","2024-08-29 02:52:29","2023-10-24 13:58:50","1-13","","","","","","","","","","","ACM","Glasgow Scotland Uk","en","","","","","DOI.org (Crossref)","","","","C:\Users\frank\Zotero\storage\K7VHKYSA\Amershi et al. - 2019 - Guidelines for Human-AI Interaction.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","CHI '19: CHI Conference on Human Factors in Computing Systems","","","","","","","","","","","","","","",""
"PQQPM9BU","conferencePaper","2019","Geva, Mor; Goldberg, Yoav; Berant, Jonathan","Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets","Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)","","","10.18653/v1/D19-1107","https://www.aclweb.org/anthology/D19-1107","Crowdsourcing has been the prevalent paradigm for creating natural language understanding datasets in recent years. A common crowdsourcing practice is to recruit a small number of high-quality workers, and have them massively generate examples. Having only a few workers generate the majority of examples raises concerns about data diversity, especially when workers freely generate sentences. In this paper, we perform a series of experiments showing these concerns are evident in three recent NLP datasets. We show that model performance improves when training with annotator identiﬁers as features, and that models are able to recognize the most productive annotators. Moreover, we show that often models do not generalize well to examples from annotators that did not contribute to the training set. Our ﬁndings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators.","2019","2024-08-29 02:52:33","2024-08-29 02:52:33","2023-10-24 13:58:21","1161-1166","","","","","","Are We Modeling the Task or the Annotator?","","","","","Association for Computational Linguistics","Hong Kong, China","en","","","","","DOI.org (Crossref)","","","","C:\Users\frank\Zotero\storage\YH2ZXEUQ\Geva et al. - 2019 - Are We Modeling the Task or the Annotator An Inve.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)","","","","","","","","","","","","","","",""
"UNAGEFZ4","journalArticle","2019","Bansal, Gagan; Nushi, Besmira; Kamar, Ece; Lasecki, Walter S.; Weld, Daniel S.; Horvitz, Eric","Beyond Accuracy: The Role of Mental Models in Human-AI Team Performance","Proceedings of the AAAI Conference on Human Computation and Crowdsourcing","","2769-1349, 2769-1330","10.1609/hcomp.v7i1.5285","https://ojs.aaai.org/index.php/HCOMP/article/view/5285","Decisions made by human-AI teams (e.g., AI-advised humans) are increasingly common in high-stakes domains such as healthcare, criminal justice, and ﬁnance. Achieving high team performance depends on more than just the accuracy of the AI system: Since the human and the AI may have different expertise, the highest team performance is often reached when they both know how and when to complement one another. We focus on a factor that is crucial to supporting such complementary: the human’s mental model of the AI capabilities, speciﬁcally the AI system’s error boundary (i.e. knowing “When does the AI err?”). Awareness of this lets the human decide when to accept or override the AI’s recommendation. We highlight two key properties of an AI’s error boundary, parsimony and stochasticity, and a property of the task, dimensionality. We show experimentally how these properties affect humans’ mental models of AI capabilities and the resulting team performance. We connect our evaluations to related work and propose goals, beyond accuracy, that merit consideration during model selection and optimization to improve overall human-AI team performance.","2019-10-28","2024-08-29 02:52:34","2024-08-29 02:52:34","2023-10-24 13:58:02","2-11","","","7","","HCOMP","Beyond Accuracy","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\frank\Zotero\storage\J6GJVVKJ\Bansal et al. - 2019 - Beyond Accuracy The Role of Mental Models in Huma.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N7BFT6AQ","conferencePaper","2020","Barshan, Elnaz; Brunet, Marc-Etienne; Dziugaite, Gintare Karolina","RelatIF: Identifying Explanatory Training Samples via Relative Influence","Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics","","","","https://proceedings.mlr.press/v108/barshan20a.html","In this work, we focus on the use of influence functions to identify relevant training examples that one might hope “explain” the predictions of a machine learning model. One shortcoming of influence functions is that the training examples deemed most “influential” are often outliers or mislabelled, making them poor choices for explanation. In order to address this shortcoming, we separate the role of global versus local influence. We introduce RelatIF, a new class of criteria for choosing relevant training examples by way of an optimization objective that places a constraint on global influence. RelatIF considers the local influence that an explanatory example has on a prediction relative to its global effects on the model. In empirical evaluations, we find that the examples returned by RelatIF are more intuitive when compared to those found using influence functions.","2020-06-03","2024-08-29 02:52:35","2024-08-29 02:52:35","2023-10-28 19:41:48","1899-1909","","","","","","RelatIF","","","","","PMLR","","en","","","","","proceedings.mlr.press","","ISSN: 2640-3498","","C:\Users\frank\Zotero\storage\KGGZFMKH\Barshan et al. - 2020 - RelatIF Identifying Explanatory Training Samples .pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Artificial Intelligence and Statistics","","","","","","","","","","","","","","",""
"7RJBRIEX","preprint","2022","Chen, Yuanyuan; Li, Boyang; Yu, Han; Wu, Pengcheng; Miao, Chunyan","HYDRA: Hypergradient Data Relevance Analysis for Interpreting Deep Neural Networks","","","","10.48550/arXiv.2102.02515","http://arxiv.org/abs/2102.02515","The behaviors of deep neural networks (DNNs) are notoriously resistant to human interpretations. In this paper, we propose Hypergradient Data Relevance Analysis, or HYDRA, which interprets the predictions made by DNNs as effects of their training data. Existing approaches generally estimate data contributions around the final model parameters and ignore how the training data shape the optimization trajectory. By unrolling the hypergradient of test loss w.r.t. the weights of training data, HYDRA assesses the contribution of training data toward test data points throughout the training trajectory. In order to accelerate computation, we remove the Hessian from the calculation and prove that, under moderate conditions, the approximation error is bounded. Corroborating this theoretical claim, empirical results indicate the error is indeed small. In addition, we quantitatively demonstrate that HYDRA outperforms influence functions in accurately estimating data contribution and detecting noisy data labels. The source code is available at https://github.com/cyyever/aaai_hydra_8686.","2022-12-29","2024-08-29 02:52:36","2024-08-29 02:52:36","2023-10-28 19:49:19","","","","","","","HYDRA","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2102.02515 arXiv:2102.02515 [cs]","","C:\Users\frank\Zotero\storage\9CIC4TJM\Chen et al. - 2022 - HYDRA Hypergradient Data Relevance Analysis for I.pdf; C:\Users\frank\Zotero\storage\J2JN4FTN\2102.html","","notion","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2102.02515","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"86K94CLB","journalArticle","2018","Varma, Paroma; Ré, Christopher","Snuba: automating weak supervision to label training data","Proceedings of the VLDB Endowment","","2150-8097","10.14778/3291264.3291268","https://dl.acm.org/doi/10.14778/3291264.3291268","As deep learning models are applied to increasingly diverse problems, a key bottleneck is gathering enough high-quality training labels tailored to each task. Users therefore turn to weak supervision, relying on imperfect sources of labels like pattern matching and user-deﬁned heuristics. Unfortunately, users have to design these sources for each task. This process can be time consuming and expensive: domain experts often perform repetitive steps like guessing optimal numerical thresholds and developing informative text patterns. To address these challenges, we present Snuba, a system to automatically generate heuristics using a small labeled dataset to assign training labels to a large, unlabeled dataset in the weak supervision setting. Snuba generates heuristics that each labels the subset of the data it is accurate for, and iteratively repeats this process until the heuristics together label a large portion of the unlabeled data. We develop a statistical measure that guarantees the iterative process will automatically terminate before it degrades training label quality. Snuba automatically generates heuristics in under ﬁve minutes and performs up to 9.74 F1 points better than the best known user-deﬁned heuristics developed over many days. In collaborations with users at research labs, Stanford Hospital, and on open source datasets, Snuba outperforms other automated approaches like semisupervised learning by up to 14.35 F1 points.","2018-11","2024-08-29 02:52:38","2024-08-29 02:52:38","2023-10-30 18:58:36","223-236","","3","12","","Proc. VLDB Endow.","Snuba","","","","","","","en","","","","","DOI.org (Crossref)","","Number: 3","","C:\Users\frank\Zotero\storage\YAHITVSE\Varma e Ré - 2018 - Snuba automating weak supervision to label traini.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PCPJI5PC","journalArticle","2022","Denham, Benjamin; Lai, Edmund M-K.; Sinha, Roopak; Naeem, M. Asif","Witan: unsupervised labelling function generation for assisted data programming","Proceedings of the VLDB Endowment","","2150-8097","10.14778/3551793.3551797","https://dl.acm.org/doi/10.14778/3551793.3551797","Effective supervised training of modern machine learning models often requires large labelled training datasets, which could be prohibitively costly to acquire for many practical applications. Research addressing this problem has sought ways to leverage               weak supervision               sources, such as the user-defined heuristic labelling functions used in the               data programming               paradigm, which are cheaper and easier to acquire. Automatic generation of these functions can make data programming even more efficient and effective. However, existing approaches rely on initial supervision in the form of small labelled datasets or interactive user feedback. In this paper, we propose Witan, an algorithm for generating labelling functions without any initial supervision. This flexibility affords many interaction modes, including unsupervised dataset exploration before the user even defines a set of classes. Experiments in binary and multi-class classification demonstrate the efficiency and classification accuracy of Witan compared to alternative labelling approaches.","2022-07","2024-08-29 02:52:40","2024-08-29 02:52:40","2023-10-30 22:38:26","2334-2347","","11","15","","Proc. VLDB Endow.","Witan","","","","","","","en","","","","","DOI.org (Crossref)","","Number: 11","","C:\Users\frank\Zotero\storage\MXWJ94F4\p2334-denham.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R6J8GBGX","preprint","2022","Seedat, Nabeel; Crabbé, Jonathan; Bica, Ioana; van der Schaar, Mihaela","Data-IQ: Characterizing subgroups with heterogeneous outcomes in tabular data","","","","10.48550/arXiv.2210.13043","http://arxiv.org/abs/2210.13043","High model performance, on average, can hide that models may systematically underperform on subgroups of the data. We consider the tabular setting, which surfaces the unique issue of outcome heterogeneity - this is prevalent in areas such as healthcare, where patients with similar features can have different outcomes, thus making reliable predictions challenging. To tackle this, we propose Data-IQ, a framework to systematically stratify examples into subgroups with respect to their outcomes. We do this by analyzing the behavior of individual examples during training, based on their predictive confidence and, importantly, the aleatoric (data) uncertainty. Capturing the aleatoric uncertainty permits a principled characterization and then subsequent stratification of data examples into three distinct subgroups (Easy, Ambiguous, Hard). We experimentally demonstrate the benefits of Data-IQ on four real-world medical datasets. We show that Data-IQ's characterization of examples is most robust to variation across similarly performant (yet different) models, compared to baselines. Since Data-IQ can be used with any ML model (including neural networks, gradient boosting etc.), this property ensures consistency of data characterization, while allowing flexible model selection. Taking this a step further, we demonstrate that the subgroups enable us to construct new approaches to both feature acquisition and dataset selection. Furthermore, we highlight how the subgroups can inform reliable model usage, noting the significant impact of the Ambiguous subgroup on model generalization.","2022-10-24","2024-08-29 02:52:43","2024-08-29 02:52:43","2023-11-13 17:46:14","","","","","","","Data-IQ","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2210.13043 arXiv:2210.13043 [cs]","","C:\Users\frank\Zotero\storage\BKIVU69F\Seedat et al. - 2022 - Data-IQ Characterizing subgroups with heterogeneo.pdf; C:\Users\frank\Zotero\storage\ZHVHFUMS\2210.html","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2210.13043","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FH6LGZ8X","journalArticle","2020","Ratner, Alexander; Bach, Stephen H.; Ehrenberg, Henry; Fries, Jason; Wu, Sen; Ré, Christopher","Snorkel: rapid training data creation with weak supervision","The VLDB Journal","","1066-8888, 0949-877X","10.1007/s00778-019-00552-1","http://link.springer.com/10.1007/s00778-019-00552-1","Labeling training data is increasingly the largest bottleneck in deploying machine learning systems. We present Snorkel, a ﬁrst-of-its-kind system that enables users to train state-of-the-art models without hand labeling any training data. Instead, users write labeling functions that express arbitrary heuristics, which can have unknown accuracies and correlations. Snorkel denoises their outputs without access to ground truth by incorporating the ﬁrst end-to-end implementation of our recently proposed machine learning paradigm, data programming. We present a ﬂexible interface layer for writing labeling functions based on our experience over the past year collaborating with companies, agencies, and research laboratories. In a user study, subject matter experts build models 2.8× faster and increase predictive performance an average 45.5% versus seven hours of hand labeling. We study the modeling trade-offs in this new setting and propose an optimizer for automating trade-off decisions that gives up to 1.8× speedup per pipeline execution. In two collaborations, with the US Department of Veterans Affairs and the US Food and Drug Administration, and on four open-source text and image data sets representative of other deployments, Snorkel provides 132% average improvements to predictive performance over prior heuristic approaches and comes within an average 3.60% of the predictive performance of large hand-curated training sets.","2020-05","2024-08-29 02:52:50","2024-08-29 02:52:50","2023-11-13 17:49:57","709-730","","2-3","29","","The VLDB Journal","Snorkel","","","","","","","en","","","","","DOI.org (Crossref)","","Number: 2-3","","C:\Users\frank\Zotero\storage\8UP4LTCB\Ratner et al. - 2020 - Snorkel rapid training data creation with weak su.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BJ5D97ZM","preprint","2022","Thyagarajan, Aditya; Snorrason, Elías; Northcutt, Curtis; Mueller, Jonas","Identifying Incorrect Annotations in Multi-Label Classification Data","","","","10.48550/arXiv.2211.13895","http://arxiv.org/abs/2211.13895","In multi-label classification, each example in a dataset may be annotated as belonging to one or more classes (or none of the classes). Example applications include image (or document) tagging where each possible tag either applies to a particular image (or document) or not. With many possible classes to consider, data annotators are likely to make errors when labeling such data in practice. Here we consider algorithms for finding mislabeled examples in multi-label classification datasets. We propose an extension of the Confident Learning framework to this setting, as well as a label quality score that ranks examples with label errors much higher than those which are correctly labeled. Both approaches can utilize any trained classifier. After demonstrating that our methodology empirically outperforms other algorithms for label error detection, we apply our approach to discover many label errors in the CelebA image tagging dataset.","2022-11-25","2024-08-29 02:52:54","2024-08-29 02:52:54","2023-11-21 16:47:52","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2211.13895 arXiv:2211.13895 [cs]","","C:\Users\frank\Zotero\storage\4AQE83HA\Thyagarajan et al. - 2022 - Identifying Incorrect Annotations in Multi-Label C.pdf; C:\Users\frank\Zotero\storage\6H5IU2AM\2211.html","","notion","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2211.13895","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R3NHATFK","conferencePaper","2022","Zeng, Ziqian; Ni, Weimin; Fang, Tianqing; Li, Xiang; Zhao, Xinran; Song, Yangqiu","Weakly Supervised Text Classification using Supervision Signals from a Language Model","","","","10.18653/v1/2022.findings-naacl.176","https://aclanthology.org/2022.findings-naacl.176","Solving text classification in a weakly supervised manner is important for real-world applications where human annotations are scarce. In this paper, we propose to query a masked language model with cloze style prompts to obtain supervision signals. We design a prompt which combines the document itself and “this article is talking about [MASK].” A masked language model can generate words for the [MASK] token. The generated words which summarize the content of a document can be utilized as supervision signals. We propose a latent variable model to learn a word distribution learner which associates generated words to pre-defined categories and a document classifier simultaneously without using any annotated data. Evaluation on three datasets, AGNews, 20Newsgroups, and UCINews, shows that our method can outperform baselines by 2%, 4%, and 3%.","2022-07","2024-08-29 02:52:56","2024-08-29 02:52:56","2023-11-21 20:15:50","2295–2305","","","","","","","","","","","Association for Computational Linguistics","Seattle, United States","","","","","","ACLWeb","","","","C:\Users\frank\Zotero\storage\I8I52QAU\Zeng et al. - 2022 - Weakly Supervised Text Classification using Supervision Signals from a Language Model.pdf","","notion","","Carpuat, Marine; de Marneffe, Marie-Catherine; Meza Ruiz, Ivan Vladimir","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Findings 2022","","","","","","","","","","","","","","",""
"UR2KKBCV","preprint","2023","Sedova, Anastasiia; Roth, Benjamin","ULF: Unsupervised Labeling Function Correction using Cross-Validation for Weak Supervision","","","","10.48550/arXiv.2204.06863","http://arxiv.org/abs/2204.06863","A cost-effective alternative to manual data labeling is weak supervision (WS), where data samples are automatically annotated using a predefined set of labeling functions (LFs), rule-based mechanisms that generate artificial labels for the associated classes. In this work, we investigate noise reduction techniques for WS based on the principle of k-fold cross-validation. We introduce a new algorithm ULF for Unsupervised Labeling Function correction, which denoises WS data by leveraging models trained on all but some LFs to identify and correct biases specific to the held-out LFs. Specifically, ULF refines the allocation of LFs to classes by re-estimating this assignment on highly reliable cross-validated samples. Evaluation on multiple datasets confirms ULF's effectiveness in enhancing WS learning without the need for manual labeling.","2023-10-24","2024-08-29 02:52:57","2024-08-29 02:52:57","2023-11-22 19:25:06","","","","","","","ULF","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2204.06863 arXiv:2204.06863 [cs]","","C:\Users\frank\Zotero\storage\KIHGAQUS\Sedova e Roth - 2023 - ULF Unsupervised Labeling Function Correction usi.pdf; C:\Users\frank\Zotero\storage\HB2SIE4N\2204.html","","notion","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2204.06863","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EUFIU6XB","book","2023","Sedova, Anastasiia; Zellinger, Lena; Roth, Benjamin","Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal","","","","","http://arxiv.org/abs/2306.04502","An accurate and substantial dataset is essential for training a reliable and well-performing model. However, even manually annotated datasets contain label errors, not to mention automatically labeled ones. Previous methods for label denoising have primarily focused on detecting outliers and their permanent removal - a process that is likely to over- or underfilter the dataset. In this work, we propose AGRA: a new method for learning with noisy labels by using Adaptive GRAdient-based outlier removal. Instead of cleaning the dataset prior to model training, the dataset is dynamically adjusted during the training process. By comparing the aggregated gradient of a batch of samples and an individual example gradient, our method dynamically decides whether a corresponding example is helpful for the model at this point or is counter-productive and should be left out for the current update. Extensive evaluation on several datasets demonstrates AGRA's effectiveness, while a comprehensive results analysis supports our initial hypothesis: permanent hard outlier removal is not always what model benefits the most from.","2023","2024-08-29 02:52:59","2024-08-29 02:52:59","2023-11-23 13:58:07","","","","14169","","","","","","","","","","","","","","","arXiv.org","","arXiv:2306.04502 [cs]","","C:\Users\frank\Zotero\storage\YC6YBLUS\Sedova et al. - 2023 - Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal.pdf","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q3LYZWMP","conferencePaper","2019","Awasthi, Abhijeet; Ghosh, Sabyasachi; Goyal, Rasna; Sarawagi, Sunita","Learning from Rules Generalizing Labeled Exemplars","","","","","https://openreview.net/forum?id=SkeuexBtDr","In many applications labeled data is not readily available, and needs to be collected via pain-staking human supervision. We propose a rule-exemplar method for collecting human supervision to combine the efficiency of rules with the quality of instance labels. The supervision is coupled such that it is both natural for humans and synergistic for learning. We propose a training algorithm that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference. Empirical evaluation on five different tasks shows that (1) our algorithm is more accurate than several existing methods of learning from a mix of clean and noisy supervision, and (2) the coupled rule-exemplar supervision is effective in denoising rules.","2019-09-25","2024-08-29 02:53:00","2024-08-29 02:53:00","2023-11-24 18:22:17","","","","","","","","","","","","","","en","","","","","openreview.net","","","","C:\Users\frank\Zotero\storage\WMITNFCT\Awasthi et al. - 2019 - Learning from Rules Generalizing Labeled Exemplars.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Learning Representations","","","","","","","","","","","","","","",""
"56VSSQTV","preprint","2019","Wang, Zihan; Shang, Jingbo; Liu, Liyuan; Lu, Lihao; Liu, Jiacheng; Han, Jiawei","CrossWeigh: Training Named Entity Tagger from Imperfect Annotations","","","","10.48550/arXiv.1909.01441","http://arxiv.org/abs/1909.01441","Everyone makes mistakes. So do human annotators when curating labels for named entity recognition (NER). Such label mistakes might hurt model training and interfere model comparison. In this study, we dive deep into one of the widely-adopted NER benchmark datasets, CoNLL03 NER. We are able to identify label mistakes in about 5.38% test sentences, which is a significant ratio considering that the state-of-the-art test F1 score is already around 93%. Therefore, we manually correct these label mistakes and form a cleaner test set. Our re-evaluation of popular models on this corrected test set leads to more accurate assessments, compared to those on the original test set. More importantly, we propose a simple yet effective framework, CrossWeigh, to handle label mistakes during NER model training. Specifically, it partitions the training data into several folds and train independent NER models to identify potential mistakes in each fold. Then it adjusts the weights of training data accordingly to train the final NER model. Extensive experiments demonstrate significant improvements of plugging various NER models into our proposed framework on three datasets. All implementations and corrected test set are available at our Github repo: https://github.com/ZihanWangKi/CrossWeigh.","2019-09-03","2024-08-29 02:53:02","2024-08-29 02:53:02","2023-12-02 17:54:43","","","","","","","CrossWeigh","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:1909.01441 arXiv:1909.01441 [cs]","","C:\Users\frank\Zotero\storage\HA6IEX7T\Wang et al. - 2019 - CrossWeigh Training Named Entity Tagger from Impe.pdf; C:\Users\frank\Zotero\storage\EWQL5CMA\1909.html","","notion","Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:1909.01441","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2IB6NS2I","preprint","2021","Cheng, Hao; Zhu, Zhaowei; Li, Xingyu; Gong, Yifei; Sun, Xing; Liu, Yang","Learning with Instance-Dependent Label Noise: A Sample Sieve Approach","","","","10.48550/arXiv.2010.02347","http://arxiv.org/abs/2010.02347","Human-annotated labels are often prone to noise, and the presence of such noise will degrade the performance of the resulting deep neural network (DNN) models. Much of the literature (with several recent exceptions) of learning with noisy labels focuses on the case when the label noise is independent of features. Practically, annotations errors tend to be instance-dependent and often depend on the difficulty levels of recognizing a certain task. Applying existing results from instance-independent settings would require a significant amount of estimation of noise rates. Therefore, providing theoretically rigorous solutions for learning with instance-dependent label noise remains a challenge. In this paper, we propose CORES$^{2}$ (COnfidence REgularized Sample Sieve), which progressively sieves out corrupted examples. The implementation of CORES$^{2}$ does not require specifying noise rates and yet we are able to provide theoretical guarantees of CORES$^{2}$ in filtering out the corrupted examples. This high-quality sample sieve allows us to treat clean examples and the corrupted ones separately in training a DNN solution, and such a separation is shown to be advantageous in the instance-dependent noise setting. We demonstrate the performance of CORES$^{2}$ on CIFAR10 and CIFAR100 datasets with synthetic instance-dependent label noise and Clothing1M with real-world human noise. As of independent interests, our sample sieve provides a generic machinery for anatomizing noisy datasets and provides a flexible interface for various robust training techniques to further improve the performance. Code is available at https://github.com/UCSC-REAL/cores.","2021-03-22","2024-08-29 02:53:04","2024-08-29 02:53:04","2023-12-06 17:25:12","","","","","","","Learning with Instance-Dependent Label Noise","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2010.02347 arXiv:2010.02347 [cs, stat]","","C:\Users\frank\Zotero\storage\JN9U4W46\Cheng et al. - 2021 - Learning with Instance-Dependent Label Noise A Sa.pdf; C:\Users\frank\Zotero\storage\INEAGSPT\2010.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2010.02347","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N9UICERA","preprint","2022","Zhu, Zhaowei; Wang, Jialu; Liu, Yang","Beyond Images: Label Noise Transition Matrix Estimation for Tasks with Lower-Quality Features","","","","10.48550/arXiv.2202.01273","http://arxiv.org/abs/2202.01273","The label noise transition matrix, denoting the transition probabilities from clean labels to noisy labels, is crucial for designing statistically robust solutions. Existing estimators for noise transition matrices, e.g., using either anchor points or clusterability, focus on computer vision tasks that are relatively easier to obtain high-quality representations. We observe that tasks with lower-quality features fail to meet the anchor-point or clusterability condition, due to the coexistence of both uninformative and informative representations. To handle this issue, we propose a generic and practical information-theoretic approach to down-weight the less informative parts of the lower-quality features. This improvement is crucial to identifying and estimating the label noise transition matrix. The salient technical challenge is to compute the relevant information-theoretical metrics using only noisy labels instead of clean ones. We prove that the celebrated $f$-mutual information measure can often preserve the order when calculated using noisy labels. We then build our transition matrix estimator using this distilled version of features. The necessity and effectiveness of the proposed method are also demonstrated by evaluating the estimation error on a varied set of tabular data and text classification tasks with lower-quality features. Code is available at github.com/UCSC-REAL/BeyondImages.","2022-06-17","2024-08-29 02:53:06","2024-08-29 02:53:06","2023-12-09 20:33:55","","","","","","","Beyond Images","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2202.01273 arXiv:2202.01273 [cs]","","C:\Users\frank\Zotero\storage\LYKXIX5H\Zhu et al. - 2022 - Beyond Images Label Noise Transition Matrix Estim.pdf; C:\Users\frank\Zotero\storage\QLPVINVL\2202.html","","notion","Computer Science - Machine Learning; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2202.01273","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JH4PNKBX","preprint","2022","Liu, Yang; Cheng, Hao; Zhang, Kun","Identifiability of Label Noise Transition Matrix","","","","10.48550/arXiv.2202.02016","http://arxiv.org/abs/2202.02016","The noise transition matrix plays a central role in the problem of learning with noisy labels. Among many other reasons, a large number of existing solutions rely on access to it. Identifying and estimating the transition matrix without ground truth labels is a critical and challenging task. When label noise transition depends on each instance, the problem of identifying the instance-dependent noise transition matrix becomes substantially more challenging. Despite recent works proposing solutions for learning from instance-dependent noisy labels, the field lacks a unified understanding of when such a problem remains identifiable. The goal of this paper is to characterize the identifiability of the label noise transition matrix. Building on Kruskal's identifiability results, we are able to show the necessity of multiple noisy labels in identifying the noise transition matrix for the generic case at the instance level. We further instantiate the results to explain the successes of the state-of-the-art solutions and how additional assumptions alleviated the requirement of multiple noisy labels. Our result also reveals that disentangled features are helpful in the above identification task and we provide empirical evidence.","2022-07-04","2024-08-29 02:53:09","2024-08-29 02:53:09","2023-12-09 20:34:15","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2202.02016 arXiv:2202.02016 [cs, stat]","","C:\Users\frank\Zotero\storage\L6TZ7IRT\Liu et al. - 2022 - Identifiability of Label Noise Transition Matrix.pdf; C:\Users\frank\Zotero\storage\FSCRJ6JM\2202.html","","notion","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2202.02016","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R2U8DTBJ","preprint","2021","Zhu, Zhaowei; Song, Yiwen; Liu, Yang","Clusterability as an Alternative to Anchor Points When Learning with Noisy Labels","","","","10.48550/arXiv.2102.05291","http://arxiv.org/abs/2102.05291","The label noise transition matrix, characterizing the probabilities of a training instance being wrongly annotated, is crucial to designing popular solutions to learning with noisy labels. Existing works heavily rely on finding ""anchor points"" or their approximates, defined as instances belonging to a particular class almost surely. Nonetheless, finding anchor points remains a non-trivial task, and the estimation accuracy is also often throttled by the number of available anchor points. In this paper, we propose an alternative option to the above task. Our main contribution is the discovery of an efficient estimation procedure based on a clusterability condition. We prove that with clusterable representations of features, using up to third-order consensuses of noisy labels among neighbor representations is sufficient to estimate a unique transition matrix. Compared with methods using anchor points, our approach uses substantially more instances and benefits from a much better sample complexity. We demonstrate the estimation accuracy and advantages of our estimates using both synthetic noisy labels (on CIFAR-10/100) and real human-level noisy labels (on Clothing1M and our self-collected human-annotated CIFAR-10). Our code and human-level noisy CIFAR-10 labels are available at https://github.com/UCSC-REAL/HOC.","2021-07-13","2024-08-29 02:53:10","2024-08-29 02:53:10","2023-12-09 20:39:29","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2102.05291 arXiv:2102.05291 [cs, stat]","","C:\Users\frank\Zotero\storage\G99IQRNR\Zhu et al. - 2021 - Clusterability as an Alternative to Anchor Points .pdf; C:\Users\frank\Zotero\storage\5LWFQSX8\2102.html","","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2102.05291","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DEN9ZCH2","journalArticle","2015","Stowell, Dan; Giannoulis, Dimitrios; Benetos, Emmanouil; Lagrange, Mathieu; Plumbley, Mark D.","Detection and Classification of Acoustic Scenes and Events","IEEE Transactions on Multimedia","","1520-9210, 1941-0077","10.1109/TMM.2015.2428998","http://ieeexplore.ieee.org/document/7100934/","While there is the saying of two heads are better than one, having multiple opinions brings the problem of finding a common ground. For data, multiple annotator opinions are usually aggregated into a single set of labels, regarded as the ground truth. With this ground truth, classification models can be trained in a supervised way to learn the annotated data categories. Finding a suitable aggregation for multiple annotator opinions is the topic of research in many domains. In this work we investigate the use of raw data obtained from multiple annotators with various levels of reliability, to train a model for audio classification. The model sees all the individual annotator opinions and learns the categories without the need of aggregating the information. The results show that using a fullyconnected layer that models individual annotators, it is possible to leverage the data distribution and learn to classify sounds without the need for aggregation of labels.","2015-10","2024-08-29 02:53:13","2024-08-29 02:53:13","2023-12-14 10:13:11","1733-1746","","10","17","","IEEE Trans. Multimedia","","","","","","","","en","","","","","DOI.org (Crossref)","","Number: 10","","C:\Users\frank\Zotero\storage\X2CUVMDL\Stowell et al. - 2015 - Detection and Classification of Acoustic Scenes an.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5AKW8MED","conferencePaper","2023","Zhang, Yunyi; Jiang, Minhao; Meng, Yu; Zhang, Yu; Han, Jiawei","PIEClass: Weakly-Supervised Text Classification with Prompting and Noise-Robust Iterative Ensemble Training","Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing","","","","https://aclanthology.org/2023.emnlp-main.780","Weakly-supervised text classification trains a classifier using the label name of each target class as the only supervision, which largely reduces human annotation efforts. Most existing methods first use the label names as static keyword-based features to generate pseudo labels, which are then used for final classifier training. While reasonable, such a commonly adopted framework suffers from two limitations: (1) keywords can have different meanings in different contexts and some text may not have any keyword, so keyword matching can induce noisy and inadequate pseudo labels; (2) the errors made in the pseudo label generation stage will directly propagate to the classifier training stage without a chance of being corrected. In this paper, we propose a new method, PIEClass, consisting of two modules: (1) a pseudo label acquisition module that uses zero-shot prompting of pre-trained language models (PLM) to get pseudo labels based on contextualized text understanding beyond static keyword matching, and (2) a noise-robust iterative ensemble training module that iteratively trains classifiers and updates pseudo labels by utilizing two PLM fine-tuning methods that regularize each other. Extensive experiments show that PIEClass achieves overall better performance than existing strong baselines on seven benchmark datasets and even achieves similar performance to fully-supervised classifiers on sentiment classification tasks.","2023-12","2024-08-29 02:53:13","2024-08-29 02:53:13","2023-12-14 10:13:11","12655–12670","","","","","","PIEClass","","","","","Association for Computational Linguistics","Singapore","","","","","","ACLWeb","","","","C:\Users\frank\Zotero\storage\AFH5L2X2\Zhang et al. - 2023 - PIEClass Weakly-Supervised Text Classification wi.pdf","","notion","","Bouamor, Houda; Pino, Juan; Bali, Kalika","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","EMNLP 2023","","","","","","","","","","","","","","",""
"LPGR3F8P","journalArticle","","Zhang, Mengtian; Jiang, Bo; Ling, Yuye; Wang, Xinbing","LEARNING WITH NON-UNIFORM LABEL NOISE: A CLUSTER-DEPENDENT WEAKLY SUPERVISED APPROACH","","","","","","Learning with noisy labels is a challenging task in machine learning. Furthermore in reality, label noise can be highly non-uniform in feature space, e.g. with higher error rate for more difficult samples. Some recent works consider instance-dependent label noise but they require additional information such as some cleanly labeled data and confidence scores, which are usually unavailable or costly to obtain. In this paper, we consider learning with non-uniform label noise that requires no such additional information. Inspired by stratified sampling, we propose a cluster-dependent sample selection algorithm followed by a contrastive training mechanism based on the cluster-dependent label noise. Despite its simplicity, the proposed method can distinguish clean data from the corrupt ones more precisely and achieve state-of-the-art performance on most image classification benchmarks, especially when the number of training samples is small and the noise rate is high. The code is released at https://github.com/MattZ-99/ClusterCL.","","2024-08-29 02:53:16","2024-08-29 02:53:16","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\frank\Zotero\storage\JJAUR3PW\Zhang et al. - LEARNING WITH NON-UNIFORM LABEL NOISE A CLUSTER-D.pdf","","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GVJRTUDW","journalArticle","2024","Ding, Jiaman; Zhang, Yihang; Jia, Lianyin; Fu, Xiaodong; Jiang, Ying","Noisy feature decomposition-based multi-label learning with missing labels","Information Sciences","","0020-0255","10.1016/j.ins.2024.120228","https://www.sciencedirect.com/science/article/pii/S0020025524001415","In recent years, multi-label learning with missing labels (MLML) has become a popular topic. The major challenge for MLML is enhancing the performance of classifiers in the presence of missing labels. Most existing algorithms focus on recovering missing labels using label correlations. However, incomplete label correlations in the early stages of recovery may adversely affect the results. To address this problem, we focus on the original task of finding the mapping between labels and features and propose a Noisy Feature Decomposition-based Multi-label learning with Missing Labels (NFDMML) method. Specifically, the label information is assumed to be integral, and the features corresponding to missing labels are defined as noisy features. Not recovering the missing labels, we reduce the interference of noisy features in the classifications. Accordingly, the MLML problem is converted into a feature decomposition problem. Based on label correlation, a low-rank relationship is used to eliminate the features caused by missing labels, and reverse mapping is employed to preserve the features corresponding to the relevant labels. We conduct detailed experiments on multiple datasets, and the results clearly demonstrate that the proposed method achieves competitive performance over other algorithms.","2024-03-01","2024-08-29 02:53:16","2024-08-29 02:53:16","2024-02-01 16:17:19","120228","","","662","","Information Sciences","","","","","","","","","","","","","ScienceDirect","","","","C:\Users\frank\Zotero\storage\AXJ767FY\Ding et al. - 2024 - Noisy feature decomposition-based multi-label lear.pdf; C:\Users\frank\Zotero\storage\N5N4AS45\S0020025524001415.html","","notion","Feature decomposition; Label correlation; Missing labels; Multi-label learning; Noisy feature","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XDZTC7WS","preprint","2023","Chen, Jiuhai; Mueller, Jonas","Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness","","","","10.48550/arXiv.2308.16175","http://arxiv.org/abs/2308.16175","We introduce BSDetector, a method for detecting bad and speculative answers from a pretrained Large Language Model by estimating a numeric confidence score for any output it generated. Our uncertainty quantification technique works for any LLM accessible only via a black-box API, whose training data remains unknown. By expending a bit of extra computation, users of any LLM API can now get the same response as they would ordinarily, as well as a confidence estimate that cautions when not to trust this response. Experiments on both closed and open-form Question-Answer benchmarks reveal that BSDetector more accurately identifies incorrect LLM responses than alternative uncertainty estimation procedures (for both GPT-3 and ChatGPT). By sampling multiple responses from the LLM and considering the one with the highest confidence score, we can additionally obtain more accurate responses from the same LLM, without any extra training steps. In applications involving automated evaluation with LLMs, accounting for our confidence scores leads to more reliable evaluation in both human-in-the-loop and fully-automated settings (across both GPT 3.5 and 4).","2023-10-04","2024-08-29 02:53:19","2024-08-29 02:53:19","2024-02-08 16:29:20","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Issue: arXiv:2308.16175 arXiv:2308.16175 [cs]","","C:\Users\frank\Zotero\storage\8YC9RVUQ\Chen e Mueller - 2023 - Quantifying Uncertainty in Answers from any Langua.pdf; C:\Users\frank\Zotero\storage\E88M7NVX\2308.html","","notion","Computer Science - Artificial Intelligence; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2308.16175","","","","","","","","","","","","","","","","","","","","","","","","","","",""